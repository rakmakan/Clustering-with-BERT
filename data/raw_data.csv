BibliographyType,Identifier,Author,Title,Journal,Month,Year,URL,Abstract,Custom2,Custom3,Custom4,Custom5
7,adhikari_docbert:_2019,"Adhikari, Ashutosh; Ram, Achyudh; Tang, Raphael; Lin, Jimmy",DocBERT: BERT for Document Classification,arXiv:1904.08398 [cs],April,2019,http://arxiv.org/abs/1904.08398,"We present, to our knowledge, the first application of BERT to document classification. A few characteristics of the task might lead one to think that BERT is not the most appropriate model: syntactic structures matter less for content categories, documents can often be longer than typical BERT input, and documents often have multiple labels. Nevertheless, we show that a straightforward classification model using BERT is able to achieve the state of the art across four popular datasets. To address the computational expense associated with BERT inference, we distill knowledge from BERT-large to small bidirectional LSTMs, reaching BERT-base parity on multiple datasets using 30x fewer parameters. The primary contribution of our paper is improved baselines that can provide the foundation for future work.",,Computer Science - Computation and Language,,
7,agirre_studying_2015,"Agirre, Eneko; Barrena, Ander; Soroa, Aitor",Studying the Wikipedia Hyperlink Graph for Relatedness and Disambiguation,arXiv:1503.01655 [cs],March,2015,http://arxiv.org/abs/1503.01655,,,Computer Science - Computation and Language,,
7,agirre_studying_2015,"Agirre, Eneko; Barrena, Ander; Soroa, Aitor",Studying the Wikipedia Hyperlink Graph for Relatedness and Disambiguation,arXiv:1503.01655 [cs],March,2015,http://arxiv.org/abs/1503.01655,"Hyperlinks and other relations in Wikipedia are a extraordinary resource which is still not fully understood. In this paper we study the different types of links in Wikipedia, and contrast the use of the full graph with respect to just direct links. We apply a well-known random walk algorithm on two tasks, word relatedness and named-entity disambiguation. We show that using the full graph is more effective than just direct links by a large margin, that non-reciprocal links harm performance, and that there is no benefit from categories and infoboxes, with coherent results on both tasks. We set new state-of-the-art figures for systems based on Wikipedia links, comparable to systems exploiting several information sources and/or supervised machine learning. Our approach is open source, with instruction to reproduce results, and amenable to be integrated with complementary text-based methods.",,Computer Science - Computation and Language,,
7,alberi_2019_2018,"Alberi, Kirstin; Nardelli, Marco Buongiorno; Zakutayev, Andriy; Mitas, Lubos; Curtarolo, Stefano; Jain, Anubhav; Fornari, Marco; Marzari, Nicola; Takeuchi, Ichiro; Green, Martin L.; Kanatzidis, Mercouri; Toney, Mike F.; Butenko, Sergiy; Meredig, Bryce; Lany, Stephan; Kattner, Ursula; Davydov, Albert; Toberer, Eric S.; Stevanovic, Vladan; Walsh, Aron; Park, Nam-Gyu; Aspuru-Guzik, AlÃ¡n; Tabor, Daniel P.; Nelson, Jenny; Murphy, James; Setlur, Anant; Gregoire, John; Li, Hong; Xiao, Ruijuan; Ludwig, Alfred; Martin, Lane W.; Rappe, Andrew M.; Wei, Su-Huai; Perkins, John",The 2019 materials by design roadmap,Journal of Physics D: Applied Physics,October,2018,https://doi.org/10.1088%2F1361-6463%2Faad926,"Advances in renewable and sustainable energy technologies critically depend on our ability to design and realize materials with optimal properties. Materials discovery and design efforts ideally involve close coupling between materials prediction, synthesis and characterization. The increased use of computational tools, the generation of materials databases, and advances in experimental methods have substantially accelerated these activities. It is therefore an opportune time to consider future prospects for materials by design approaches. The purpose of this Roadmap is to present an overview of the current state of computational materials prediction, synthesis and characterization approaches, materials design needs for various technologies, and future challenges and opportunities that must be addressed. The various perspectives cover topics on computational techniques, validation, materials databases, materials informatics, high-throughput combinatorial methods, advanced characterization approaches, and materials design issues in thermoelectrics, photovoltaics, solid state lighting, catalysts, batteries, metal alloys, complex oxides and transparent conducting materials. It is our hope that this Roadmap will guide researchers and funding agencies in identifying new prospects for materials design.",,,,
7,aljamel_smart_2018,"Aljamel, A.; Osman, T.; Acampora, G.; Vitiello, A.; Zhang, Z.",Smart Information Retrieval: Domain Knowledge Centric Optimization Approach,IEEE Access,December,2018,,"In the age of the Internet of Things, online data have witnessed a significant growth in terms of volume and diversity, and research into information retrieval has become one of the important research themes in the Internet-oriented data science research. This paper introduces a novel domain knowledge centric methodology aimed at improving the accuracy of using machine learning methods for relation extraction from text data, which is critical to the accuracy and efficiency of information retrieval-based applications, including recommender systems and sentiment analysis. The proposed methodology makes a significant contribution to the processes of domain knowledge-based relation extraction including interrogating Linked Open Datasets to generate the relation classification training data, addressing the imbalanced classification in the training datasets, determining the probability threshold of the best learning algorithm, and establishing the optimum parameters for genetic algorithms, which were utilized to optimize the feature selection for the learning algorithms. The experimental evaluation of the proposed methodology reveals that the adopted machine-learning algorithms exhibit higher precision and recall in relation extraction in the reduced feature space optimized by our implementation. The considered machine learning includes support vector machine, perceptron algorithm uneven margin, and K-nearest neighbors. The outcome is verified by comparing against the random mutation hill-climbing optimization algorithm using Wilcoxon signed-rank statistical analysis.",,"Classification algorithms, Feature extraction, Genetic algorithms, IoT, Machine learning, Optimization, Task analysis, Training, genetic algorithms, information extraction, machine learning, optimization, smart system",,
7,allahyari_text_2017,"Allahyari, Mehdi; Pouriyeh, Seyedamin; Assefi, Mehdi; Safaei, Saeid; Trippe, Elizabeth D.; Gutierrez, Juan B.; Kochut, Krys",Text Summarization Techniques: A Brief Survey,arXiv:1707.02268 [cs],July,2017,http://arxiv.org/abs/1707.02268,"In recent years, there has been a explosion in the amount of text data from a variety of sources. This volume of text is an invaluable source of information and knowledge which needs to be eï¬€ectively summarized to be useful. In this review, the main approaches to automatic text summarization are described. We review the diï¬€erent processes for summarization and describe the eï¬€ectiveness and shortcomings of the diï¬€erent methods.",,Computer Science - Computation and Language,,
7,alsentzer_publicly_2019,"Alsentzer, Emily; Murphy, John R.; Boag, Willie; Weng, Wei-Hung; Jin, Di; Naumann, Tristan; McDermott, Matthew B. A.",Publicly Available Clinical BERT Embeddings,arXiv:1904.03323 [cs],June,2019,http://arxiv.org/abs/1904.03323,"Contextual word embedding models such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018) have dramatically improved performance for many natural language processing (NLP) tasks in recent months. However, these models have been minimally explored on specialty corpora, such as clinical text; moreover, in the clinical domain, no publicly-available pre-trained BERT models yet exist. In this work, we address this need by exploring and releasing BERT models for clinical text: one for generic clinical text and another for discharge summaries speciï¬Åcally. We demonstrate that using a domain-speciï¬Åc model yields performance improvements on three common clinical NLP tasks as compared to nonspeciï¬Åc embeddings. These domainspeciï¬Åc models are not as performant on two clinical de-identiï¬Åcation tasks, and argue that this is a natural consequence of the differences between de-identiï¬Åed source text and synthetically non de-identiï¬Åed task text.",,Computer Science - Computation and Language,,
7,alshargi_concept2vec:_2018,"Alshargi, Faisal; Shekarpour, Saeedeh; Soru, Tommaso; Sheth, Amit",Concept2vec: Metrics for Evaluating Quality of Embeddings for Ontological Concepts,arXiv:1803.04488 [cs],July,2018,http://arxiv.org/abs/1803.04488,"Although there is an emerging trend towards generating embeddings for primarily unstructured data, and recently for structured data, there is not yet any systematic suite for measuring the quality of embeddings. This deficiency is further sensed with respect to embeddings generated for structured data because there are no concrete evaluation metrics measuring the quality of encoded structure as well as semantic patterns in the embedding space. In this paper, we introduce a framework containing three distinct tasks concerned with the individual aspects of ontological concepts: (i) the categorization aspect, (ii) the hierarchical aspect, and (iii) the relational aspect. Then, in the scope of each task, a number of intrinsic metrics are proposed for evaluating the quality of the embeddings. Furthermore, w.r.t. this framework multiple experimental studies were run to compare the quality of the available embedding models. Employing this framework in future research can reduce misjudgment and provide greater insight about quality comparisons of embeddings for ontological concepts.",,"Computer Science - Artificial Intelligence, Computer Science - Computation and Language, I.2.4, I.2.6",,
7,al-smadi_deep_2018,"Al-Smadi, Mohammad; Qawasmeh, Omar; Al-Ayyoub, Mahmoud; Jararweh, Yaser; Gupta, Brij",Deep Recurrent neural network vs. support vector machine for aspect-based sentiment analysis of Arabic hotelsâ€™ reviews,Journal of Computational Science,July,2018,http://www.sciencedirect.com/science/article/pii/S1877750317305252,,,"Arabic reviews, Aspect-based sentiment analysis, Deep learning, Supervised machine learning",,
7,andrew_document_nodate,"Andrew, M. Dai",Document Embedding with Paragraph Vectors,,,,https://storage.googleapis.com/pub-tools-public-publication-data/pdf/44894.pdf,,,,,
7,annarumma_automated_2019,"Annarumma, Mauro; Withey, Samuel J.; Bakewell, Robert J.; Pesce, Emanuele; Goh, Vicky; Montana, Giovanni",Automated Triaging of Adult Chest Radiographs with Deep Artificial Neural Networks,Radiology,April,2019,http://pubs.rsna.org/doi/10.1148/radiol.2018180921,"Purpose:â€ƒ To develop and test an artificial intelligence (AI) system, bas ed on deep convolutional neural networks (CNNs), for automated real-time triaging of adult chest radiographs on the basis of the urgency of imaging appearances. Materials and Methods:â€ƒ An AI system was developed by using 470â€‰388 fully anonymized institutional adult chest radiographs acquired from 2007 to 2017. The free-text radiology reports were preprocessed by using an in-house natural language processing (NLP) system modeling radiologic language. The NLP system analyzed the free-text report to prioritize each radiograph as critical, urgent, nonurgent, or normal. An AI system for computer vision using an ensemble of two deep CNNs was then trained by using labeled radiographs to predict the clinical priority from radiologic appearances only. The systemâ€™s performance in radiograph prioritization was tested in a simulation by using an independent set of 15â€‰887 radiographs. Prediction performance was assessed with the area under the receiver operating characteristic curve; sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV) were also determined. Nonparametric testing of the improvement in time to final report was determined at a nominal significance level of 5\%. Results:â€ƒ Normal chest radiographs were detected by our AI system with a sensitivity of 71\%, specificity of 95\%, PPV of 73\%, and NPV of 94\%. The average reporting delay was reduced from 11.2 to 2.7 days for critical imaging findings (P , .001) and from 7.6 to 4.1 days for urgent imaging findings (P , .001) in the simulation compared with historical data. Conclusion:â€ƒ Automated real-time triaging of adult chest radiographs with use of an artificial intelligence system is feasible, with clinically acceptable performance.",,,,
7,arguello_casteleiro_deep_2018,"Arguello Casteleiro, Mercedes; Demetriou, George; Read, Warren; Fernandez Prieto, Maria Jesus; Maroto, Nava; Maseda Fernandez, Diego; Nenadic, Goran; Klein, Julie; Keane, John; Stevens, Robert",Deep learning meets ontologies: experiments to anchor the cardiovascular disease ontology in the biomedical literature,Journal of Biomedical Semantics,April,2018,https://doi.org/10.1186/s13326-018-0181-1,"Automatic identification of term variants or acceptable alternative free-text terms for gene and protein names from the millions of biomedical publications is a challenging task. Ontologies, such as the Cardiovascular Disease Ontology (CVDO), capture domain knowledge in a computational form and can provide context for gene/protein names as written in the literature. This study investigates: 1) if word embeddings from Deep Learning algorithms can provide a list of term variants for a given gene/protein of interest; and 2) if biological knowledge from the CVDO can improve such a list without modifying the word embeddings created.",,,,
7,arras_what_2017,"Arras, Leila; Horn, Franziska; Montavon, GrÃ©goire; MÃ_ller, Klaus-Robert; Samek, Wojciech","What is relevant in a text document?"": An interpretable machine learning approach""",PLOS ONE,August,2017,https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0181142,"Text documents can be described by a number of abstract concepts such as semantic category, writing style, or sentiment. Machine learning (ML) models have been trained to automatically map documents to these abstract concepts, allowing to annotate very large text collections, more than could be processed by a human in a lifetime. Besides predicting the textâ€™s category very accurately, it is also highly desirable to understand how and why the categorization process takes place. In this paper, we demonstrate that such understanding can be achieved by tracing the classification decision back to individual words using layer-wise relevance propagation (LRP), a recently developed technique for explaining predictions of complex non-linear classifiers. We train two word-based ML models, a convolutional neural network (CNN) and a bag-of-words SVM classifier, on a topic categorization task and adapt the LRP method to decompose the predictions of these models onto words. Resulting scores indicate how much individual words contribute to the overall classification decision. This enables one to distill relevant information from text documents without an explicit semantic information extraction step. We further use the word-wise relevance scores for generating novel vector-based document representations which capture semantic information. Based on these document vectors, we introduce a measure of model explanatory power and show that, although the SVM and CNN models perform similarly in terms of classification accuracy, the latter exhibits a higher level of explainability which makes it more comprehensible for humans and potentially more useful for other applications.",,"Imaging techniques, Machine learning, Neural networks, Neurons, Preprocessing, Semantics, Support vector machines, Vector spaces",,
7,ash_deep_2020,"Ash, Jordan T.; Zhang, Chicheng; Krishnamurthy, Akshay; Langford, John; Agarwal, Alekh","Deep Batch Active Learning by Diverse, Uncertain Gradient Lower Bounds",arXiv:1906.03671,February,2020,https://arxiv.org/abs/1906.03671,"We design a new algorithm for batch active learning with deep neural network models. Our algorithm, Batch Active learning by Diverse Gradient Embeddings (BADGE), samples groups of points that are disparate and high magnitude when represented in a hallucinated gradient space, a strategy designed to incorporate both predictive uncertainty and sample diversity into every selected batch. Crucially, BADGE trades off between uncertainty and diversity without requiring any hand-tuned hyperparameters. While other approaches sometimes succeed for particular batch sizes or architectures, BADGE consistently performs as well or better, making it a useful option for real world active learning problems.",,,,
7,atighehchian_bayesian_2020,"Atighehchian, Parmida; Branchaud-Charron, FrÃ©dÃ©ric; Lacoste, Alexandre","Bayesian active learning for production, a systematic study and a reusable library",arXiv:2006.09916 [cs.LG],June,2020,https://arxiv.org/abs/2006.09916,"Active learning is able to reduce the amount of labelling effort by using a machine learning model to query the user for specific inputs. While there are many papers on new active learning techniques, these techniques rarely satisfy the constraints of a real-world project. In this paper, we analyse the main drawbacks of current active learning techniques and we present approaches to alleviate them. We do a systematic study on the effects of the most common issues of real-world datasets on the deep active learning process: model convergence, annotation error, and dataset imbalance. We derive two techniques that can speed up the active learning loop such as partial uncertainty sampling and larger query size. Finally, we present our open-source Bayesian active learning library, BaaL.",,,,
7,atighehchian_bayesian_nodate,"Atighehchian, Parmida; Branchaud-Charron, FrÃ©dÃ©ric; Lacoste, Alexandre","Bayesian active learning for production, a systematic study and a reusable library",arXiv:2006.09916 [cs.LG],,,https://arxiv.org/abs/2006.09916,"Active learning is able to reduce the amount of labelling effort by using a machine learning model to query the user for specific inputs. While there are many papers on new active learning techniques, these techniques rarely satisfy the constraints of a real-world project. In this paper, we analyse the main drawbacks of current active learning techniques and we present approaches to alleviate them. We do a systematic study on the effects of the most common issues of real-world datasets on the deep active learning process: model convergence, annotation error, and dataset imbalance. We derive two techniques that can speed up the active learning loop such as partial uncertainty sampling and larger query size. Finally, we present our open-source Bayesian active learning library, BaaL.",,,,
7,azimaee_national_2018,"Azimaee, Mahmoud; Victor, J. Charles; Vermeulen, Marian; Smith, Mark",A National Concept Dictionary,International Journal of Population Data Science,August,2018,https://ijpds.org/article/view/709,,,,,
7,bahdanau_neural_2014,"Bahdanau, Dzmitry; Cho, Kyunghyun; Bengio, Yoshua",Neural Machine Translation by Jointly Learning to Align and Translate,"arXiv:1409.0473 [cs, stat]",September,2014,http://arxiv.org/abs/1409.0473,"Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoderâ€“decoders and encode a source sentence into a ï¬Åxed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a ï¬Åxed-length vector is a bottleneck in improving the performance of this basic encoderâ€“decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",,"Attention, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning",,
7,banerjee_comparative_2019,"Banerjee, Imon; Ling, Yuan; Chen, Matthew C.; Hasan, Sadid A.; Langlotz, Curtis P.; Moradzadeh, Nathaniel; Chapman, Brian; Amrhein, Timothy; Mong, David; Rubin, Daniel L.; Farri, Oladimeji; Lungren, Matthew P.",Comparative effectiveness of convolutional neural network (CNN) and recurrent neural network (RNN) architectures for radiology text report classification,Artificial Intelligence in Medicine,June,2019,https://linkinghub.elsevier.com/retrieve/pii/S0933365717306255,"This paper explores cutting-edge deep learning methods for information extraction from medical imaging free text reports at a multi-institutional scale and compares them to the state-of-the-art domain-speciï¬Åc rule-based system â€“ PEFinder and traditional machine learning methods â€“ SVM and Adaboost. We proposed two distinct deep learning models â€“ (i) CNN Word â€“ Glove, and (ii) Domain phrase attention-based hierarchical recurrent neural network (DPA-HNN), for synthesizing information on pulmonary emboli (PE) from over 7370 clinical thoracic computed tomography (CT) free-text radiology reports collected from four major healthcare centers. Our proposed DPA-HNN model encodes domain-dependent phrases into an attention mechanism and represents a radiology report through a hierarchical RNN structure composed of word-level, sentence-level and documentlevel representations. Experimental results suggest that the performance of the deep learning models that are trained on a single institutional dataset, are better than rule-based PEFinder on our multi-institutional test sets. The best F1 score for the presence of PE in an adult patient population was 0.99 (DPA-HNN) and for a pediatrics population was 0.99 (HNN) which shows that the deep learning models being trained on adult data, demonstrated generalizability to pediatrics population with comparable accuracy. Our work suggests feasibility of broader usage of neural network models in automated classiï¬Åcation of multi-institutional imaging text reports for a variety of applications including evaluation of imaging utilization, imaging yield, clinical decision support tools, and as part of automated classiï¬Åcation of large corpus for medical imaging deep learning work.",,,,
7,banerjee_radiology_2018,"Banerjee, Imon; Chen, Matthew C.; Lungren, Matthew P.; Rubin, Daniel L.",Radiology Report Annotation using Intelligent Word Embeddings: Applied to Multi-institutional Chest CT Cohort,Journal of biomedical informatics,January,2018,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5771955/,"We proposed an unsupervised hybrid method - Intelligent Word Embedding (IWE) that combines neural embedding method with a semantic dictionary mapping technique for creating a dense vector representation of unstructured radiology reports. We applied IWE to generate embedding of chest CT radiology reports from two healthcare organizations and utilized the vector representations to semi-automate report categorization based on clinically relevant categorization related to the diagnosis of pulmonary embolism (PE). We benchmark the performance against a state-of-the-art rule-based tool, PeFinder and out-of-the-box word2vec. On the Stanford test set, the IWE model achieved average F1 score 0.97, whereas the PeFinder scored 0.9 and the original word2vec scored 0.94. On UPMC dataset, the IWE modelâ€™s average F1 score was 0.94, when the PeFinder scored 0.92 and word2vec scored 0.85. The IWE model had lowest generalization error with highest F1 scores. Of particular interest, the IWE model (trained on the Stanford dataset) outperformed PeFinder on the UPMC dataset which was used originally to tailor the PeFinder model.,",,,,
7,barker_whats_nodate,"Barker, Emma; Paramita, Monica; Funk, Adam; Kurtic, Emina; Aker, Ahmet; Foster, Jonathan; Hepple, Mark; Gaizauskas, Robert",Whatâ€™s the Issue Here?: Task-based Evaluation of Reader Comment Summarization Systems,,,,,"Automatic summarization of reader comments in on-line news is an extremely challenging task and a capability for which there is a clear need. Work to date has focussed on producing extractive summaries using well-known techniques imported from other areas of language processing. But are extractive summaries of comments what users really want? Do they support users in performing the sorts of tasks they are likely to want to perform with reader comments? In this paper we address these questions by doing three things. First, we offer a speciï¬Åcation of one possible summary type for reader comment, based on an analysis of reader comment in terms of issues and viewpoints. Second, we deï¬Åne a task-based evaluation framework for reader comment summarization that allows summarization systems to be assessed in terms of how well they support users in a time-limited task of identifying issues and characterising opinion on issues in comments. Third, we describe a pilot evaluation in which we used the task-based evaluation framework to evaluate a prototype reader comment clustering and summarization system, demonstrating the viability of the evaluation framework and illustrating the sorts of insight such an evaluation affords.",,,,
7,belli_privacy-aware_2020,"Belli, Luca; Ktena, Sofia Ira; Tejani, Alykhan; Lung-Yut-Fon, Alexandre; Portman, Frank; Zhu, Xiao; Xie, Yuanpu; Gupta, Akshay; Bronstein, Michael; DeliÄ‡, Amra; Sottocornola, Gabriele; Anelli, Walter; Andrade, Nazareno; Smith, Jessie; Shi, Wenzhe",Privacy-Aware Recommender Systems Challenge on Twitter's Home Timeline,"arXiv:2004.13715 [cs, stat]",September,2020,http://arxiv.org/abs/2004.13715,"Recommender systems constitute the core engine of most social network platforms nowadays, aiming to maximize user satisfaction along with other key business objectives. Twitter is no exception. Despite the fact that Twitter data has been extensively used to understand socioeconomic and political phenomena and user behaviour, the implicit feedback provided by users on Tweets through their engagements on the Home Timeline has only been explored to a limited extent. At the same time, there is a lack of large-scale public social network datasets that would enable the scientific community to both benchmark and build more powerful and comprehensive models that tailor content to user interests. By releasing an original dataset of 160 million Tweets along with engagement information, Twitter aims to address exactly that. During this release, special attention is drawn on maintaining compliance with existing privacy laws. Apart from user privacy, this paper touches on the key challenges faced by researchers and professionals striving to predict user engagements. It further describes the key aspects of the RecSys 2020 Challenge that was organized by ACM RecSys in partnership with Twitter using this dataset.",,"Computer Science - Machine Learning, Computer Science - Social and Information Networks, Statistics - Machine Learning",,
7,beltagy_scibert:_2019,"Beltagy, Iz; Lo, Kyle; Cohan, Arman",SciBERT: Pretrained Contextualized Embeddings for Scientific Text,arXiv:1903.10676 [cs],March,2019,http://arxiv.org/abs/1903.10676,"Obtaining large-scale annotated data for NLP tasks in the scientific domain is challenging and expensive. We release SciBERT, a pretrained contextualized embedding model based on BERT (Devlin et al., 2018) to address the lack of high-quality, large-scale labeled scientific data. SciBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state-of-the-art results on several of these tasks. The code and pretrained models are available at https://github.com/allenai/scibert/.",,Computer Science - Computation and Language,,
7,bhaskar_query_nodate,"Bhaskar, Pinaki; Bandyopadhyay, Sivaji",A Query Focused Multi Document Automatic Summarization,,,,,"The present paper describes the development of a query focused multi-document automatic summarization. A graph is constructed, where the nodes are sentences of the documents and edge scores reflect the correlation measure between the nodes. The system clusters similar texts having related topical features from the graph using edge scores. Next, query dependent weights for each sentence are added to the edge score of the sentence and accumulated with the corresponding cluster score. Top ranked sentence of each cluster is identified and compressed using a dependency parser. The compressed sentences are included in the output summary. The inter-document cluster is revisited in order until the length of the summary is less than the maximum limit. The summarizer has been tested on the standard TAC 2008 test data sets of the Update Summarization Track. Evaluation of the summarizer yielded accuracy scores of 0.10317 (ROUGE-2) and 0.13998 (ROUGEâ€“SU-4).",,"Graph-based, summarization",,
7,bhatnagar_pal_2020,"Bhatnagar, Shubhang; Tank, Darshan; Goyal, Sachin; Sethi, Amit",PAL : Pretext-based Active Learning,arXiv:2010.15947 [cs],October,2020,http://arxiv.org/abs/2010.15947,"When obtaining labels is expensive, the requirement of a large labeled training data set for deep learning can be mitigated by active learning. Active learning refers to the development of algorithms to judiciously pick limited subsets of unlabeled samples that can be sent for labeling by an oracle. We propose an intuitive active learning technique that, in addition to the task neural network (e.g., for classification), uses an auxiliary self-supervised neural network that assesses the utility of an unlabeled sample for inclusion in the labeled set. Our core idea is that the difficulty of the auxiliary network trained on labeled samples to solve a self-supervision task on an unlabeled sample represents the utility of obtaining the label of that unlabeled sample. Specifically, we assume that an unlabeled image on which the precision of predicting a random applied geometric transform is low must be out of the distribution represented by the current set of labeled images. These images will therefore maximize the relative information gain when labeled by the oracle. We also demonstrate that augmenting the auxiliary network with task specific training further improves the results. We demonstrate strong performance on a range of widely used datasets and establish a new state of the art for active learning. We also make our code publicly available to encourage further research.",,"Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning",,
7,blake_miller_active_nodate,Blake Miller; Fridolin Linder; Walter Mebane,Active Learning Approaches for Labeling Text: Review and Assessment of the Performance of Active Learning Approaches,Political Analysis (conditionally accepted),,,http://www-personal.umich.edu/~wmebane/active-learning-approaches-4-18-2018.pdf,"In the case where concepts to measure in corpora are known in advance, supervised methods are likely to provide better qualitative results, model selection procedures, and model performance measures. In this paper, we illustrate that much of the expense of manual corpus labeling comes from common sampling practices such as random sampling that result in sparse coverage across classes, and duplicated effort of the expert who is labeling texts (it does not help your modelâ€™s performance to label a document that is very similar to a document the expert has already labeled). In this paper we outline several active learning methods for iteratively modeling text and sampling articles based on model uncertainty with respect to unlabeled posts. We show that with particular care in sampling unlabeled data, researchers can train high performance text classification models using a fraction of the labeled documents one would need using random sampling. We illustrate this using several experiments on three corpora that vary in size and domain type (Tweets, Wikipedia talk sections, and Breitbart articles).",,,,
7,blumenschein_evaluating_2020,"Blumenschein, Michael; Zhang, Xuan; Pomerenke, David; Keim, Daniel A.; Fuchs, Johannes",Evaluating Reordering Strategies for Cluster Identification in Parallel Coordinates,Computer Graphics Forum,,2020,https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.14000,"The ability to perceive patterns in parallel coordinates plots (PCPs) is heavily influenced by the ordering of the dimensions. While the community has proposed over 30 automatic ordering strategies, we still lack empirical guidance for choosing an appropriate strategy for a given task. In this paper, we first propose a classification of tasks and patterns and analyze which PCP reordering strategies help in detecting them. Based on our classification, we then conduct an empirical user study with 31 participants to evaluate reordering strategies for cluster identification tasks. We particularly measure time, identification quality, and the usersâ€™ confidence for two different strategies using both synthetic and real-world datasets. Our results show that, somewhat unexpectedly, participants tend to focus on dissimilar rather than similar dimension pairs when detecting clusters, and are more confident in their answers. This is especially true when increasing the amount of clutter in the data. As a result of these findings, we propose a new reordering strategy based on the dissimilarity of neighboring dimension pairs.",,"CCS Concepts, â€¢ Human-centered computing â†’ Empirical studies in visualization",,
7,bojanowski_enriching_2017,"Bojanowski, Piotr; Grave, Edouard; Joulin, Armand; Mikolov, Tomas",Enriching Word Vectors with Subword Information,Transactions of the Association for Computational Linguistics,,2017,https://www.aclweb.org/anthology/Q17-1010,"Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.",,,,
7,borkan_nuanced_2019,"Borkan, Daniel; Dixon, Lucas; Sorensen, Jeffrey; Thain, Nithum; Vasserman, Lucy",Nuanced Metrics for Measuring Unintended Bias with Real Data for Text Classification,"arXiv:1903.04561 [cs, stat]",March,2019,http://arxiv.org/abs/1903.04561,"Unintended bias in Machine Learning can manifest as systemic differences in performance for different demographic groups, potentially compounding existing challenges to fairness in society at large. In this paper, we introduce a suite of threshold-agnostic metrics that provide a nuanced view of this unintended bias, by considering the various ways that a classifier's score distribution can vary across designated groups. We also introduce a large new test set of online comments with crowd-sourced annotations for identity references. We use this to show how our metrics can be used to find new and potentially subtle unintended bias in existing public models.",,"Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning",,
7,bradley_approaching_nodate,"Bradley, Adam J.; Sawal, Victor; Collins, Christopher",Approaching Humanities Questions Using Slow Visual Search Interfaces,,,,,"In this paper we discuss a visual search system that was designed and implemented for humanities scholars to ask questions of large document corpora. The system allows searches to be speciï¬Åed through traditional search, as well as through providing reference documents or visually exploring a semantic ontology for terms of interest. We offer a discussion of using visualization to try to answer humanities questions and take an honest look at the difï¬Åculties of using a methodology of one discipline to solve the problems of another.",,,,
7,brooks_featureinsight:_2015,"Brooks, Michael; Amershi, Saleema; Lee, Bongshin; Drucker, Steven; Kapoor, Ashish; Simard, Patrice",FeatureInsight: Visual Support for Error-Driven Feature Ideation in Text Classification,IEEE VAST,October,2015,https://www.microsoft.com/en-us/research/publication/featureinsight-visual-support-error-driven-feature-ideation-text-classification/,"Machine learning requires an effective combination of data, features, and algorithms. While many tools exist for working with machine learning data and algorithms, support for thinking of new features, or feature ideation, remains poor. In this paper, we investigate two general approaches to support feature ideation: visual summaries and sets of errors. We present FeatureInsight, â€_",,,,
7,brown_language_2020,"Brown, Tom B.; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Gretchen; Henighan, Tom; Child, Rewon; Ramesh, Aditya; Ziegler, Daniel M.; Wu, Jeffrey; Winter, Clemens; Hesse, Christopher; Chen, Mark; Sigler, Eric; Litwin, Mateusz; Gray, Scott; Chess, Benjamin; Clark, Jack; Berner, Christopher; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario",Language Models are Few-Shot Learners,arXiv:2005.14165 [cs],July,2020,http://arxiv.org/abs/2005.14165,"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",,Computer Science - Computation and Language,,
7,budd_survey_2019,"Budd, Samuel; Robinson, Emma C.; Kainz, Bernhard",A Survey on Active Learning and Human-in-the-Loop Deep Learning for Medical Image Analysis,"arXiv:1910.02923 [cs, eess]",October,2019,http://arxiv.org/abs/1910.02923,"Fully automatic deep learning has become the state-of-the-art technique for many tasks including image acquisition, analysis and interpretation, and for the extraction of clinically useful information for computer-aided detection, diagnosis, treatment planning, intervention and therapy. However, the unique challenges posed by medical image analysis suggest that retaining a human end-user in any deep learning enabled system will be beneficial. In this review we investigate the role that humans might play in the development and deployment of deep learning enabled diagnostic applications and focus on techniques that will retain a significant input from a human end user. Human-in-the-Loop computing is an area that we see as increasingly important in future research due to the safety-critical nature of working in the medical domain. We evaluate four key areas that we consider vital for deep learning in the clinical practice: (1) Active Learning - to choose the best data to annotate for optimal model performance; (2) Interpretation and Refinement - using iterative feedback to steer models to optima for a given prediction and offering meaningful ways to interpret and respond to predictions; (3) Practical considerations - developing full scale applications and the key considerations that need to be made before deployment; (4) Related Areas - research fields that will benefit human-in-the-loop computing as they evolve. We offer our opinions on the most promising directions of research and how various aspects of each area might be unified towards common goals.",,"Computer Science - Computer Vision and Pattern Recognition, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing",,
7,bustos_padchest_2019,"Bustos, Aurelia; Pertusa, Antonio; Salinas, Jose-Maria; de la Iglesia-VayÃ¡, Maria",PadChest: A large chest x-ray image dataset with multi-label annotated reports,"arXiv:1901.07441 [cs, eess]",February,2019,http://arxiv.org/abs/1901.07441,"We present a labeled large-scale, high resolution chest x-ray dataset for the automated exploration of medical images along with their associated reports. This dataset includes more than 160,000 images obtained from 67,000 patients that were interpreted and reported by radiologists at Hospital San Juan Hospital (Spain) from 2009 to 2017, covering six different position views and additional information on image acquisition and patient demography. The reports were labeled with 174 different radiographic findings, 19 differential diagnoses and 104 anatomic locations organized as a hierarchical taxonomy and mapped onto standard Unified Medical Language System (UMLS) terminology. Of these reports, 27\% were manually annotated by trained physicians and the remaining set was labeled using a supervised method based on a recurrent neural network with attention mechanisms. The labels generated were then validated in an independent test set achieving a 0.93 Micro-F1 score. To the best of our knowledge, this is one of the largest public chest x-ray database suitable for training supervised models concerning radiographs, and the first to contain radiographic reports in Spanish. The PadChest dataset can be downloaded from http://bimcv.cipf.es/bimcv-projects/padchest/.",,"92B20, 92C50, 68T50, 92B10, Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing",,
7,cai_natural_2016,"Cai, Tianrun; Giannopoulos, Andreas A.; Yu, Sheng; Kelil, Tatiana; Ripley, Beth; Kumamaru, Kanako K.; Rybicki, Frank J.; Mitsouras, Dimitrios",Natural Language Processing Technologies in Radiology Research and Clinical Applications,RadioGraphics,January,2016,http://pubs.rsna.org/doi/10.1148/rg.2016150080,,,,,
7,calanca_responsible_2018,"Calanca, Federica; Sayfullina, Luiza; Minkus, Lara; Wagner, Claudia; Malmi, Eric",Responsible team players wanted: an analysis of soft skill requirements in job advertisements,arXiv:1810.07781 [cs],October,2018,http://arxiv.org/abs/1810.07781,"During the past decades the importance of soft skills for labour market outcomes has grown substantially. This carries implications for labour market inequality, since previous research shows that soft skills are not valued equally across race and gender. This work explores the role of soft skills in job advertisements by drawing on methods from computational science as well as on theoretical and empirical insights from economics, sociology and psychology. We present a semi-automatic approach based on crowdsourcing and text mining for extracting a list of soft skills. We find that soft skills are a crucial component of job ads, especially of low-paid jobs and jobs in female-dominated professions. Our work shows that soft skills can serve as partial predictors of the gender composition in job categories and that not all soft skills receive equal wage returns at the labour market. Especially female"" skills are frequently associated with wage penalties. Our results expand the growing literature on the association of soft skills on wage inequality and highlight their importance for occupational gender segregation at labour markets.""",,"Computer Science - Computation and Language, Computer Science - Computers and Society",,
7,camacho-collados_nasari_2016,"Camacho-Collados, JosÃ©; Pilehvar, Mohammad Taher; Navigli, Roberto",Nasari: Integrating explicit knowledge and corpus statistics for a multilingual representation of concepts and entities,Artificial Intelligence,November,2016,http://www.sciencedirect.com/science/article/pii/S0004370216300820,"Owing to the need for a deep understanding of linguistic items, semantic representation is considered to be one of the fundamental components of several applications in Natural Language Processing and Artificial Intelligence. As a result, semantic representation has been one of the prominent research areas in lexical semantics over the past decades. However, due mainly to the lack of large sense-annotated corpora, most existing representation techniques are limited to the lexical level and thus cannot be effectively applied to individual word senses. In this paper we put forward a novel multilingual vector representation, called Nasari, which not only enables accurate representation of word senses in different languages, but it also provides two main advantages over existing approaches: (1) high coverage, including both concepts and named entities, (2) comparability across languages and linguistic levels (i.e., words, senses and concepts), thanks to the representation of linguistic items in a single unified semantic space and in a joint embedded space, respectively. Moreover, our representations are flexible, can be applied to multiple applications and are freely available at http://lcl.uniroma1.it/nasari/. As evaluation benchmark, we opted for four different tasks, namely, word similarity, sense clustering, domain labeling, and Word Sense Disambiguation, for each of which we report state-of-the-art performance on several standard datasets across different languages.",,"Domain labeling, Lexical semantics, Semantic representation, Semantic similarity, Sense clustering, Word Sense Disambiguation",,
7,camacho-collados_word_2018,"Camacho-Collados, Jose; Pilehvar, Mohammad Taher",From Word to Sense Embeddings: A Survey on Vector Representations of Meaning,J. Artif. Int. Res.,September,2018,https://doi.org/10.1613/jair.1.11259,"Over the past years, distributed semantic representations have proved to be effective and flexible keepers of prior knowledge to be integrated into downstream applications. This survey focuses on the representation of meaning. We start from the theoretical background behind word vector space models and highlight one of their major limitations: the meaning conflation deficiency, which arises from representing a word with all its possible meanings as a single vector. Then, we explain how this deficiency can be addressed through a transition from the word level to the more fine-grained level of word senses (in its broader acceptation) as a method for modelling unambiguous lexical meaning. We present a comprehensive overview of the wide range of techniques in the two main branches of sense representation, i.e., unsupervised and knowledge-based. Finally, this survey covers the main evaluation procedures and applications for this type of representation, and provides an analysis of four of its important aspects: interpretability, sense granularity, adaptability to different domains and compositionality.",,,,
7,camacho-collados_word_2018,"Camacho-Collados, Jose; Pilehvar, Mohammad Taher",From Word to Sense Embeddings: A Survey on Vector Representations of Meaning,arXiv:1805.04032 [cs],May,2018,http://arxiv.org/abs/1805.04032,"Over the past years, distributed semantic representations have proved to be effective and flexible keepers of prior knowledge to be integrated into downstream applications. This survey focuses on the representation of meaning. We start from the theoretical background behind word vector space models and highlight one of their major limitations: the meaning conflation deficiency, which arises from representing a word with all its possible meanings as a single vector. Then, we explain how this deficiency can be addressed through a transition from the word level to the more fine-grained level of word senses (in its broader acceptation) as a method for modelling unambiguous lexical meaning. We present a comprehensive overview of the wide range of techniques in the two main branches of sense representation, i.e., unsupervised and knowledge-based. Finally, this survey covers the main evaluation procedures and applications for this type of representation, and provides an analysis of four of its important aspects: interpretability, sense granularity, adaptability to different domains and compositionality.",,"Computer Science - Artificial Intelligence, Computer Science - Computation and Language",,
7,camacho-collados_word_2018-1,"Camacho-Collados, Jose; Pilehvar, Mohammad Taher",From Word to Sense Embeddings: A Survey on Vector Representations of Meaning,arXiv:1805.04032 [cs],May,2018,http://arxiv.org/abs/1805.04032,"Over the past years, distributed semantic representations have proved to be effective and flexible keepers of prior knowledge to be integrated into downstream applications. This survey focuses on the representation of meaning. We start from the theoretical background behind word vector space models and highlight one of their major limitations: the meaning conflation deficiency, which arises from representing a word with all its possible meanings as a single vector. Then, we explain how this deficiency can be addressed through a transition from the word level to the more fine-grained level of word senses (in its broader acceptation) as a method for modelling unambiguous lexical meaning. We present a comprehensive overview of the wide range of techniques in the two main branches of sense representation, i.e., unsupervised and knowledge-based. Finally, this survey covers the main evaluation procedures and applications for this type of representation, and provides an analysis of four of its important aspects: interpretability, sense granularity, adaptability to different domains and compositionality.",,"Computer Science - Artificial Intelligence, Computer Science - Computation and Language",,
7,caron_pruning_2020,"Caron, Mathilde; Morcos, Ari; Bojanowski, Piotr; Mairal, Julien; Joulin, Armand",Pruning Convolutional Neural Networks with Self-Supervision,arXiv:2001.03554 [cs],January,2020,http://arxiv.org/abs/2001.03554,"Convolutional neural networks trained without supervision come close to matching performance with supervised pre-training, but sometimes at the cost of an even higher number of parameters. Extracting subnetworks from these large unsupervised convnets with preserved performance is of particular interest to make them less computationally intensive. Typical pruning methods operate during training on a task while trying to maintain the performance of the pruned network on the same task. However, in self-supervised feature learning, the training objective is agnostic on the representation transferability to downstream tasks. Thus, preserving performance for this objective does not ensure that the pruned subnetwork remains effective for solving downstream tasks. In this work, we investigate the use of standard pruning methods, developed primarily for supervised learning, for networks trained without labels (i.e. on self-supervised tasks). We show that pruned masks obtained with or without labels reach comparable performance when re-trained on labels, suggesting that pruning operates similarly for self-supervised and supervised learning. Interestingly, we also find that pruning preserves the transfer performance of self-supervised subnetwork representations.",,"Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing",,
7,cashman_rnnbow:_2018,"Cashman, D.; Patterson, G.; Mosca, A.; Watts, N.; Robinson, S.; Chang, R.",RNNbow: Visualizing Learning Via Backpropagation Gradients in RNNs,IEEE Computer Graphics and Applications,November,2018,,"We present RNNbow, an interactive tool for visualizing the gradient flow during backpropagation in training of recurrent neural networks. By visualizing the gradient, as opposed to activations, RNNbow offers insight into how the network is learning. We show how it illustrates the vanishing gradient and the training process.",,"Backpropagation, Bars, Data visualization, RNNbow, Recurrent neural networks, Tools, Training, Visualization, backpropagation, backpropagation gradients, data visualisation, gradient flow, interactive systems, interactive tool, learning (artificial intelligence), recurrent neural nets, recurrent neural networks, vanishing gradient",,
7,cashman_user-based_2019,"Cashman, Dylan; Humayoun, Shah Rukh; Heimerl, Florian; Park, Kendall; Das, Subhajit; Thompson, John; Saket, Bahador; Mosca, Abigail; Stasko, John; Endert, Alex; Gleicher, Michael; Chang, Remco",A User-based Visual Analytics Workflow for Exploratory Model Analysis,Computer Graphics Forum,,2019,https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13681,"Many visual analytics systems allow users to interact with machine learning models towards the goals of data exploration and insight generation on a given dataset. However, in some situations, insights may be less important than the production of an accurate predictive model for future use. In that case, users are more interested in generating of diverse and robust predictive models, verifying their performance on holdout data, and selecting the most suitable model for their usage scenario. In this paper, we consider the concept of Exploratory Model Analysis (EMA), which is defined as the process of discovering and selecting relevant models that can be used to make predictions on a data source. We delineate the differences between EMA and the well-known term exploratory data analysis in terms of the desired outcome of the analytic process: insights into the data or a set of deployable models. The contributions of this work are a visual analytics system workflow for EMA, a user study, and two use cases validating the effectiveness of the workflow. We found that our system workflow enabled users to generate complex models, to assess them for various qualities, and to select the most relevant model for their task.",,"CCS Concepts, MK tag 1, â€¢ Computing methodologies â†’ Model development and analysis, â€¢ Human-centered computing â†’ Visual analytics, â€¢ Mathematics of computing â†’ Exploratory data analysis",,
7,chali_query-focused_2012,"Chali, Yllias; Hasan, Sadid A.",Query-focused multi-document summarization: automatic data annotations and supervised learning approaches,Natural Language Engineering,January,2012,https://www.cambridge.org/core/product/identifier/S1351324911000167/type/journal_article,"In this paper, we apply diï¬€erent supervised learning techniques to build query-focused multi-document summarization systems, where the task is to produce automatic summaries in response to a given query or speciï¬Åc information request stated by the user. A huge amount of labeled data is a prerequisite for supervised training. It is expensive and timeconsuming when humans perform the labeling task manually. Automatic labeling can be a good remedy to this problem. We employ ï¬Åve diï¬€erent automatic annotation techniques to build extracts from human abstracts using ROUGE, Basic Element overlap, syntactic similarity measure, semantic similarity measure, and Extended String Subsequence Kernel. The supervised methods we use are Support Vector Machines, Conditional Random Fields, Hidden Markov Models, Maximum Entropy, and two ensemble-based approaches. During diï¬€erent experiments, we analyze the impact of automatic labeling methods on the performance of the applied supervised methods. To our knowledge, no other study has deeply investigated and compared the eï¬€ects of using diï¬€erent automatic annotation techniques on diï¬€erent supervised learning approaches in the domain of query-focused multi-document summarization.",,"Supervised learning, extractive summarization",,
7,chawla_smote:_2002,"Chawla, N. V.; Bowyer, K. W.; Hall, L. O.; Kegelmeyer, W. P.",SMOTE: Synthetic Minority Over-sampling Technique,Journal of Artificial Intelligence Research,June,2002,https://jair.org/index.php/jair/article/view/10302,,,,,
7,chegini_interactive_2020,"Chegini, Mohammad; Bernard, JÃ_rgen; Cui, Jian; Chegini, Fatemeh; Sourin, Alexei; Andrews, Keith; Schreck, Tobias",Interactive visual labelling versus active learning: an experimental comparison,Frontiers of Information Technology \& Electronic Engineering,April,2020,http://link.springer.com/10.1631/FITEE.1900549,"Methods from supervised machine learning allow the classiï¬Åcation of new data automatically and are tremendously helpful for data analysis. The quality of supervised maching learning depends not only on the type of algorithm used, but also on the quality of the labelled dataset used to train the classiï¬Åer. Labelling instances in a training dataset is often done manually relying on selections and annotations by expert analysts, and is often a tedious and time-consuming process. Active learning algorithms can automatically determine a subset of data instances for which labels would provide useful input to the learning process. Interactive visual labelling techniques are a promising alternative, providing eï¬€ective visual overviews from which an analyst can simultaneously explore data records and select items to a label. By putting the analyst in the loop, higher accuracy can be achieved in the resulting classiï¬Åer. While initial results of interactive visual labelling techniques are promising in the sense that user labelling can improve supervised learning, many aspects of these techniques are still largely unexplored. This paper presents a study conducted using the mVis tool to compare three interactive visualisations, similarity map, scatterplot matrix (SPLOM), and parallel coordinates, with each other and with active learning for the purpose of labelling a multivariate dataset. The results show that all three interactive visual labelling techniques surpass active learning algorithms in terms of classiï¬Åer accuracy, and that users subjectively prefer the similarity map over SPLOM and parallel coordinates for labelling. Users also employ diï¬€erent labelling strategies depending on the visualisation used.",,,,
7,chen_deep_2018,"Chen, Matthew C.; Ball, Robyn L.; Yang, Lingyao; Moradzadeh, Nathaniel; Chapman, Brian E.; Larson, David B.; Langlotz, Curtis P.; Amrhein, Timothy J.; Lungren, Matthew P.",Deep Learning to Classify Radiology Free-Text Reports,Radiology,March,2018,http://pubs.rsna.org/doi/10.1148/radiol.2017171115,,,,,
7,chen_efficient_nodate,"Chen, Yu; Liu, Zhenming; Ren, Bin; Jin, Xin",On Efficient Constructions of Checkpoints,,,,,"Efï¬Åcient construction of checkpoints/snapshots is a critical tool for training and diagnosing deep learning models. In this paper, we propose a lossy compression scheme for checkpoint constructions (called LC-Checkpoint). LC-Checkpoint simultaneously maximizes the compression rate and optimizes the recovery speed, under the assumption that SGD is used to train the model. LCCheckpoint uses quantization and priority promotion to store the most crucial information for SGD to recover, and then uses a Huffman coding to leverage the non-uniform distribution of the gradient scales. Our extensive experiments show that LC-Checkpoint achieves a compression rate up to 28Ã— and recovery speedup up to 5.77Ã— over a state-of-the-art algorithm (SCAR).",,,,
7,chen_life_2017,"Chen, Wei-Ti; Barbour, Russell",Life priorities in the HIV-positive Asians: a text-mining analysis in young vs. old generation,AIDS care,April,2017,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5291763/,"HIV/AIDS is one of the most urgent and challenging public health issues, especially since it is now considered a chronic disease. In this project, we used text mining techniques to extract meaningful words and word patterns from 45 transcribed in-depth interviews of people living with HIV/AIDS (PLWHA) conducted in Taipei, Beijing, Shanghai, and San Francisco from 2006 to 2013. Text mining analysis can predict whether an emerging field will become a long-lasting source of academic interest or whether it is simply a passing source of interest that will soon disappear. The data were analyzed by age group (45 and older vs. 44 and younger). The highest ranking fragments in the order of frequency were: â€œcareâ€ù, â€œdaughterâ€ù, â€œdiseaseâ€ù, â€œfamilyâ€ù, â€œHIVâ€ù, â€œhospitalâ€ù, â€œhusbandâ€ù, â€œmedicinesâ€ù, â€œmoneyâ€ù, â€œpeopleâ€ù, â€œsonâ€ù, â€œtell/disclosureâ€ù, â€œthoughtâ€ù, â€œwantâ€ù, and â€œyearsâ€ù. Participants in the 44-year-old and younger group were focused mainly on disease disclosure, their families, and their financial condition. In older PLWHA, social supports were one of the main concerns. In this study, we learned that different age groups perceive the disease differently. Therefore, when designing intervention, researchers should consider to tailor an intervention to a specific population and to help PLWHA achieve a better quality of life. Promoting self-management can be an effective strategy for every encounter with HIV-positive individuals.",,,,
7,chen_study_2015,"Chen, Yukun; Lasko, Thomas A.; Mei, Qiaozhu; Denny, Joshua C.; Xu, Hua",A study of active learning methods for named entity recognition in clinical text,Journal of Biomedical Informatics,September,2015,https://www.sciencedirect.com/science/article/pii/S1532046415002038,"Objectives: Named entity recognition (NER), a sequential labeling task, is one of the fundamental tasks for building clinical natural language processing (NLP) systems. Machine learning (ML) based approaches can achieve good performance, but they often require large amounts of annotated samples, which are expensive to build due to the requirement of domain experts in annotation. Active learning (AL), a sample selection approach integrated with supervised ML, aims to minimize the annotation cost while maximizing the performance of ML-based models. In this study, our goal was to develop and evaluate both existing and new AL methods for a clinical NER task to identify concepts of medical problems, treatments, and lab tests from the clinical notes. Methods: Using the annotated NER corpus from the 2010 i2b2/VA NLP challenge that contained 349 clinical documents with 20,423 unique sentences, we simulated AL experiments using a number of existing and novel algorithms in three different categories including uncertainty-based, diversity-based, and baseline sampling strategies. They were compared with the passive learning that uses random sampling. Learning curves that plot performance of the NER model against the estimated annotation cost (based on number of sentences or words in the training set) were generated to evaluate different active learning and the passive learning methods and the area under the learning curve (ALC) score was computed. Results: Based on the learning curves of F-measure vs. number of sentences, uncertainty sampling algorithms outperformed all other methods in ALC. Most diversity-based methods also performed better than random sampling in ALC. To achieve an F-measure of 0.80, the best method based on uncertainty sampling could save 66\% annotations in sentences, as compared to random sampling. For the learning curves of Fmeasure vs. number of words, uncertainty sampling methods again outperformed all other methods in ALC. To achieve 0.80 in F-measure, in comparison to random sampling, the best uncertainty based method saved 42\% annotations in words. But the best diversity based method reduced only 7\% annotation effort. Conclusion: In the simulated setting, AL methods, particularly uncertainty-sampling based approaches, seemed to significantly save annotation cost for the clinical NER task. The actual benefit of active learning in clinical NER should be further evaluated in a real-time setting.",,,,
7,chen_visualizing_2019,"Chen, Chaomei; Song, Min",Visualizing a Field of Research: A Methodology of Systematic Scientometric Reviews,arXiv:1906.04800 [cs],June,2019,http://arxiv.org/abs/1906.04800,"Systematic scientometric reviews, empowered by scientometric and visual analytic techniques, offer opportunities to improve the timeliness, accessibility, and reproducibility of conventional systematic reviews. While increasingly accessible science mapping tools enable end users to visualize the structure and dynamics of a research field, a common bottleneck in the current practice is the construction of a collection of scholarly publications as the input of the subsequent scientometric analysis and visualization. End users often have to face a dilemma in the preparation process: the more they know about a knowledge domain, the easier it is for them to find the relevant data to meet their needs adequately; the little they know, the harder the problem is. What can we do to avoid missing something valuable but beyond our initial description? In this article, we introduce a flexible and generic methodology, cascading citation expansion, to increase the quality of constructing a bibliographic dataset for systematic reviews. Furthermore, the methodology simplifies the conceptualization of globalism and localism in science mapping and unifies them on a consistent and continuous spectrum. We demonstrate an application of the methodology to the research of literature-based discovery and compare five datasets constructed based on three use scenarios, namely a conventional keyword-based search (one dataset), an expansion process starting with a groundbreaking article of the knowledge domain (two datasets), and an expansion process starting with a recently published review article by a prominent expert in the domain (two datasets). The unique coverage of each of the datasets is inspected through network visualization overlays with reference to other datasets in a broad and integrated context.",,Computer Science - Digital Libraries,,
7,chen_xgboost:_2016,"Chen, Tianqi; Guestrin, Carlos",XGBoost: A Scalable Tree Boosting System,Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining - KDD '16,,2016,http://arxiv.org/abs/1603.02754,"Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.",,Computer Science - Machine Learning,,
7,chisholm_entity_2015,"Chisholm, Andrew; Hachey, Ben",Entity Disambiguation with Web Links,Transactions of the Association for Computational Linguistics,December,2015,https://doi.org/10.1162/tacl_a_00129,"Entity disambiguation with Wikipedia relies on structured information from redirect pages, article text, inter-article links, and categories. We explore whether web links can replace a curated encyclopaedia, obtaining entity prior, name, context, and coherence models from a corpus of web pages with links to Wikipedia. Experiments compare web link models to Wikipedia models on well-known conll and tac data sets. Results show that using 34 million web links approaches Wikipedia performance. Combining web link and Wikipedia models produces the best-known disambiguation accuracy of 88.7 on standard newswire test data.",,,,
7,choo_visual_2018,"Choo, Jaegul; Liu, Shixia",Visual Analytics for Explainable Deep Learning,IEEE Computer Graphics and Applications,July,2018,,"Recently, deep learning has been advancing the state of the art in artificial intelligence to a new level, and humans rely on artificial intelligence techniques more than ever. However, even with such unprecedented advancements, the lack of explanation regarding the decisions made by deep learning models and absence of control over their internal processes act as major drawbacks in critical decision-making processes, such as precision medicine and law enforcement. In response, efforts are being made to make deep learning interpretable and controllable by humans. This article reviews visual analytics, information visualization, and machine learning perspectives relevant to this aim, and discusses potential challenges and future research directions.",,"Computational modeling, Data visualization, Machine learning, Predictive models, Visual analytics, artificial intelligence techniques, computer graphics, critical decision-making processes, data visualisation, decision making, deep learning, deep learning models, explainable deep learning, information visualization, interactive visualization, internal processes, law enforcement, learning (artificial intelligence), machine learning perspectives, precision medicine, visual analytics",,
7,choo_visual_2018,"Choo, Jaegul; Liu, Shixia",Visual Analytics for Explainable Deep Learning,"arXiv:1804.02527 [cs, stat]",April,2018,http://arxiv.org/abs/1804.02527,"Recently, deep learning has been advancing the state of the art in artificial intelligence to a new level, and humans rely on artificial intelligence techniques more than ever. However, even with such unprecedented advancements, the lack of explanation regarding the decisions made by deep learning models and absence of control over their internal processes act as major drawbacks in critical decision-making processes, such as precision medicine and law enforcement. In response, efforts are being made to make deep learning interpretable and controllable by humans. In this paper, we review visual analytics, information visualization, and machine learning perspectives relevant to this aim, and discuss potential challenges and future research directions.",,"Computer Science - Human-Computer Interaction, Computer Science - Machine Learning, I.6.9.c, Statistics - Machine Learning",,
7,clark_what_2019,"Clark, Kevin; Khandelwal, Urvashi; Levy, Omer; Manning, Christopher D.",What Does BERT Look At? An Analysis of BERT's Attention,arXiv:1906.04341 [cs],June,2019,http://arxiv.org/abs/1906.04341,"Large pre-trained neural networks such as BERT have had great recent success in NLP, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data. Most recent analysis has focused on model outputs (e.g., language model surprisal) or internal vector representations (e.g., probing classifiers). Complementary to these works, we propose methods for analyzing the attention mechanisms of pre-trained models and apply them to BERT. BERT's attention heads exhibit patterns such as attending to delimiter tokens, specific positional offsets, or broadly attending over the whole sentence, with heads in the same layer often exhibiting similar behaviors. We further show that certain attention heads correspond well to linguistic notions of syntax and coreference. For example, we find heads that attend to the direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions with remarkably high accuracy. Lastly, we propose an attention-based probing classifier and use it to further demonstrate that substantial syntactic information is captured in BERT's attention.",,Computer Science - Computation and Language,,
7,coleman_similarity_2020,"Coleman, Cody; Chou, Edward; Culatana, Sean; Bailis, Peter; Berg, Alexander C.; Sumbaly, Roshan; Zaharia, Matei; Yalniz, I. Zeki",Similarity Search for Efficient Active Learning and Search of Rare Concepts,arXiv:2007.00077 [cs.LG],June,2020,https://arxiv.org/abs/2007.00077,"Many active learning and search approaches are intractable for industrial settings with billions of unlabeled examples. Existing approaches, such as uncertainty sampling or information density, search globally for the optimal examples to label, scaling linearly or even quadratically with the unlabeled data. However, in practice, data is often heavily skewed; only a small fraction of collected data will be relevant for a given learning task. For example, when identifying rare classes, detecting malicious content, or debugging model performance, the ratio of positive to negative examples can be 1 to 1,000 or more. In this work, we exploit this skew in large training datasets to reduce the number of unlabeled examples considered in each selection round by only looking at the nearest neighbors to the labeled examples. Empirically, we observe that learned representations effectively cluster unseen concepts, making active learning very effective and substantially reducing the number of viable unlabeled examples. We evaluate several active learning and search techniques in this setting on three large-scale datasets: ImageNet, Goodreads spoiler detection, and OpenImages. For rare classes, active learning methods need as little as 0.31\% of the labeled data to match the average precision of full supervision. By limiting active learning methods to only consider the immediate neighbors of the labeled data as candidates for labeling, we need only process as little as 1\% of the unlabeled data while achieving similar reductions in labeling costs as the traditional global approach. This process of expanding the candidate pool with the nearest neighbors of the labeled set can be done efficiently and reduces the computational complexity of selection by orders of magnitude.",,,,
7,conneau_supervised_2018,"Conneau, Alexis; Kiela, Douwe; Schwenk, Holger; Barrault, Loic; Bordes, Antoine",Supervised Learning of Universal Sentence Representations from Natural Language Inference Data,arXiv:1705.02364 [cs],July,2018,http://arxiv.org/abs/1705.02364,"Many modern NLP systems rely on word embeddings, previously trained in an unsupervised manner on large corpora, as base features. Efforts to obtain embeddings for larger chunks of text, such as sentences, have however not been so successful. Several attempts at learning unsupervised representations of sentences have not reached satisfactory enough performance to be widely adopted. In this paper, we show how universal sentence representations trained using the supervised data of the Stanford Natural Language Inference datasets can consistently outperform unsupervised methods like SkipThought vectors on a wide range of transfer tasks. Much like how computer vision uses ImageNet to obtain features, which can then be transferred to other tasks, our work tends to indicate the suitability of natural language inference for transfer learning to other NLP tasks. Our encoder is publicly available.",,Computer Science - Computation and Language,,
7,cormack_scalability_2016,"Cormack, Gordon V.; Grossman, Maura R.",Scalability of Continuous Active Learning for Reliable High-Recall Text Classification,CIKM '16: Proceedings of the 25th ACM International on Conference on Information and Knowledge,October,2016,https://doi.org/10.1145/2983323.2983776,"For finite document collections, continuous active learning ('CAL') has been observed to achieve high recall with high probability, at a labeling cost asymptotically proportional to the number of relevant documents. As the size of the collection increases, the number of relevant documents typically increases as well, thereby limiting the applicability of CAL to low-prevalence high-stakes classes, such as evidence in legal proceedings, or security threats, where human effort proportional to the number of relevant documents is justified. We present a scalable version of CAL ('S-CAL') that requires O(log N) labeling effort and O(N log N) computational effort---where N is the number of unlabeled training examples---to construct a classifier whose effectiveness for a given labeling cost compares favorably with previously reported methods. At the same time, S-CAL offers calibrated estimates of class prevalence, recall, and precision, facilitating both threshold setting and determination of the adequacy of the classifier.",,,,
7,correa_word_2018,"CorrÃªa, Edilson A.; Lopes, Alneu A.; Amancio, Diego R.",Word sense disambiguation: A complex network approach,Information Sciences,May,2018,http://www.sciencedirect.com/science/article/pii/S0020025518301300,"The word sense disambiguation (WSD) task aims at identifying the meaning of words in a given context for specific words conveying multiple meanings. This task plays a prominent role in a myriad of real world applications, such as machine translation, word processing and information retrieval. Recently, concepts and methods of complex networks have been employed to tackle this task by representing words as nodes, which are connected if they are semantically similar. Despite the increasingly number of studies carried out with such models, most of them use networks just to represent the data, while the pattern recognition performed on the attribute space is performed using traditional learning techniques. In other words, the structural relationships between words have not been explicitly used in the pattern recognition process. In addition, only a few investigations have probed the suitability of representations based on bipartite networks and graphs (bigraphs) for the problem, as many approaches consider all possible links between words. In this context, we assess the relevance of a bipartite network model representing both feature words (i.e. the words characterizing the context) and target (ambiguous) words to solve ambiguities in written texts. Here, we focus on semantical relationships between these two type of words, disregarding relationships between feature words. The adopted method not only serves to represent texts as graphs, but also constructs a structure on which the discrimination of senses is accomplished. Our results revealed that the adopted learning algorithm in such bipartite networks provides excellent results mostly when local features are employed to characterize the context. Surprisingly, our method even outperformed the support vector machine algorithm in particular cases, with the advantage of being robust even if a small training dataset is available. Taken together, the results obtained here show that the representation/classification used for the WSD problem might be useful to improve the semantical characterization of written texts without the use of deep linguistic information.",,"Bipartite graphs, Bipartite networks, Complex networks, Network science, Text classification, Word sense disambiguation",,
7,craswell_overview_2020,"Craswell, Nick; Mitra, Bhaskar; Yilmaz, Emine; Campos, Daniel; Voorhees, Ellen M.",Overview of the TREC 2019 deep learning track,arXiv:2003.07820 [cs],March,2020,http://arxiv.org/abs/2003.07820,"The Deep Learning Track is a new track for TREC 2019, with the goal of studying ad hoc ranking in a large data regime. It is the ï¬Årst track with large human-labeled training sets, introducing two sets corresponding to two tasks, each with rigorous TREC-style blind evaluation and reusable test sets. The document retrieval task has a corpus of 3.2 million documents with 367 thousand training queries, for which we generate a reusable test set of 43 queries. The passage retrieval task has a corpus of 8.8 million passages with 503 thousand training queries, for which we generate a reusable test set of 43 queries. This year 15 groups submitted a total of 75 runs, using various combinations of deep learning, transfer learning and traditional IR ranking methods. Deep learning runs signiï¬Åcantly outperformed traditional IR runs. Possible explanations for this result are that we introduced large training data and we included deep models trained on such data in our judging pools, whereas some past studies did not have such training data or pooling.",,"Computer Science - Computation and Language, Computer Science - Information Retrieval, Computer Science - Machine Learning",,
7,d_revitalizing_2018,"Carter, D.; Stojanovic, M.; de Bruijn, B.",Revitalizing the Global Public Health Intelligence Network (GPHIN).,Online Journal of Public Health Informatics,May,2018,https://europepmc.org/article/PMC/6088030,"Europe PMC is an archive of life sciences journal literature., Revitalizing the Global Public Health Intelligence Network (GPHIN).",,,,
7,dai_deeper_2019-1,"Dai, Zhuyun; Callan, Jamie",Deeper Text Understanding for IR with Contextual Neural Language Modeling,Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval - SIGIR'19,,2019,http://arxiv.org/abs/1905.09217,"Neural networks provide new possibilities to automatically learn complex language patterns and query-document relations. Neural IR models have achieved promising results in learning query-document relevance patterns, but few explorations have been done on understanding the text content of a query or a document. This paper studies leveraging a recently-proposed contextual neural language model, BERT, to provide deeper text understanding for IR. Experimental results demonstrate that the contextual text representations from BERT are more effective than traditional word embeddings. Compared to bag-of-words retrieval models, the contextual language model can better leverage language structures, bringing large improvements on queries written in natural languages. Combining the text understanding ability with search knowledge leads to an enhanced pre-trained BERT model that can benefit related search tasks where training data are limited.",,"Computer Science - Computation and Language, Computer Science - Information Retrieval",,
7,dai_document_2015,"Dai, Andrew M.; Olah, Christopher; Le, Quoc V.",Document Embedding with Paragraph Vectors,arXiv:1507.07998 [cs],July,2015,http://arxiv.org/abs/1507.07998,"Paragraph Vectors has been recently proposed as an unsupervised method for learning distributed representations for pieces of texts. In their work, the authors showed that the method can learn an embedding of movie review texts which can be leveraged for sentiment analysis. That proof of concept, while encouraging, was rather narrow. Here we consider tasks other than sentiment analysis, provide a more thorough comparison of Paragraph Vectors to other document modelling algorithms such as Latent Dirichlet Allocation, and evaluate performance of the method as we vary the dimensionality of the learned representation. We benchmarked the models on two document similarity data sets, one from Wikipedia, one from arXiv. We observe that the Paragraph Vector method performs significantly better than other methods, and propose a simple improvement to enhance embedding quality. Somewhat surprisingly, we also show that much like word embeddings, vector operations on Paragraph Vectors can perform useful semantic results.",,"Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning",,
7,dai_document_2015,"Dai, Andrew M.; Olah, Christopher; Le, Quoc V.",Document Embedding with Paragraph Vectors,,July,2015,https://arxiv.org/abs/1507.07998v1,"Paragraph Vectors has been recently proposed as an unsupervised method for learning distributed representations for pieces of texts. In their work, the authors showed that the method can learn an embedding of movie review texts which can be leveraged for sentiment analysis. That proof of concept, while encouraging, was rather narrow. Here we consider tasks other than sentiment analysis, provide a more thorough comparison of Paragraph Vectors to other document modelling algorithms such as Latent Dirichlet Allocation, and evaluate performance of the method as we vary the dimensionality of the learned representation. We benchmarked the models on two document similarity data sets, one from Wikipedia, one from arXiv. We observe that the Paragraph Vector method performs significantly better than other methods, and propose a simple improvement to enhance embedding quality. Somewhat surprisingly, we also show that much like word embeddings, vector operations on Paragraph Vectors can perform useful semantic results.",,,,
7,dai_scalable_nodate,"Dai, Hanjun; Nazi, Azade; Li, Yujia; Dai, Bo; Schuurmans, Dale",Scalable Deep Generative Modeling for Sparse Graphs,,,,,"Learning graph generative models is a challenging task for deep learning and has wide applicability to a range of domains like chemistry, biology and social science. However current deep neural methods suffer from limited scalability: for a graph with n nodes and m edges, existing deep neural methods require Î©(n2) complexity by building up the adjacency matrix. On the other hand, many real world graphs are actually sparse in the sense that m n2. Based on this, we develop a novel autoregressive model, named BiGG, that utilizes this sparsity to avoid generating the full adjacency matrix, and importantly reduces the graph generation time complexity to O((n + m) log n). Furthermore, during training this autoregressive model can be parallelized with O(log n) synchronization stages, which makes it much more efï¬Åcient than other autoregressive models that require Î©(n). Experiments on several benchmarks show that the proposed approach not only scales to orders of magnitude larger graphs than previously possible with deep autoregressive graph generative models, but also yields better graph generation quality.",,,,
7,dai_transformer-xl:_2019,"Dai, Zihang; Yang, Zhilin; Yang, Yiming; Carbonell, Jaime; Le, Quoc V.; Salakhutdinov, Ruslan",Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context,"arXiv:1901.02860 [cs, stat]",January,2019,http://arxiv.org/abs/1901.02860,"Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80\% longer than RNNs and 450\% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.",,"Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning",,
7,danka_modal_nodate,"Danka, Tivadar; Horvath, Peter",modAL: A modular active learning framework for Python,arXiv:1805.00979 [cs.LG],,,https://arxiv.org/abs/1805.00979,"modAL is a modular active learning framework for Python, aimed to make active learning research and practice simpler. Its distinguishing features are (i) clear and modular object oriented design (ii) full compatibility with scikit-learn models and workflows. These features make fast prototyping and easy extensibility possible, aiding the development of real-life active learning pipelines and novel algorithms as well. modAL is fully open source, hosted on GitHub at this https URL. To assure code quality, extensive unit tests are provided and continuous integration is applied. In addition, a detailed documentation with several tutorials are also available for ease of use. The framework is available in PyPI and distributed under the MIT license.",,,,
7,de_paulo_faleiros_optimizing_2017,"de Paulo Faleiros, Thiago; Geraldeli Rossi, Rafael; de Andrade Lopes, Alneu",Optimizing the class information divergence for transductive classification of texts using propagation in bipartite graphs,Pattern Recognition Letters,February,2017,https://linkinghub.elsevier.com/retrieve/pii/S0167865516300587,"Transductive classiï¬Åcation is an useful way to classify a collection of unlabeled textual documents when only a small fraction of this collection can be manually labeled. Graph-based algorithms have aroused considerable interests in recent years to perform transductive classiï¬Åcation since the graph-based representation facilitates label propagation through the graph edges. In a bipartite graph representation, nodes represent objects of two types, here documents and terms, and the edges between documents and terms represent the occurrences of the terms in the documents. In this context, the label propagation is performed from documents to terms and then from terms to documents iteratively. In this paper we propose a new graph-based transductive algorithm that use the bipartite graph structure to associate the available class information of labeled documents and then propagate these class information to assign labels for unlabeled documents. By associating the class information to edges linking documents to terms we guarantee that a single term can propagate different class information to its distinct neighbors. We also demonstrated that the proposed method surpasses the algorithms for transductive classiï¬Åcation based on vector space model or graphs when only a small number of labeled documents is available.",,,,
7,demner-fushman_preparing_2016,"Demner-Fushman, Dina; Kohli, Marc D.; Rosenman, Marc B.; Shooshan, Sonya E.; Rodriguez, Laritza; Antani, Sameer; Thoma, George R.; McDonald, Clement J.",Preparing a collection of radiology examinations for distribution and retrieval,Journal of the American Medical Informatics Association,March,2016,https://academic.oup.com/jamia/article/23/2/304/2572395,"Abstract.  Objective Clinical documents made available for secondary use play an increasingly important role in discovery of clinical knowledge, development of",,"Image, Text",,
7,devlin_bert_2019,"Devlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina",BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,arXiv:1810.04805 [cs],May,2019,http://arxiv.org/abs/1810.04805,"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be ï¬Ånetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspeciï¬Åc architecture modiï¬Åcations.",,Computer Science - Computation and Language,,
7,devlin_bert_2019,"Devlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina",BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,arXiv:1810.04805 [cs],May,2019,http://arxiv.org/abs/1810.04805,"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",,Computer Science - Computation and Language,,
7,dietz_trec_2017,"Dietz, Laura; Verma, Manisha; Radlinski, Filip; Craswell, Nick",TREC Complex Answer Retrieval Overview,,,2017,https://trec.nist.gov/pubs/trec26/trec2017.html,,,,,
7,dong_banditsum_2019,"Dong, Yue; Shen, Yikang; Crawford, Eric; van Hoof, Herke; Cheung, Jackie Chi Kit",BanditSum: Extractive Summarization as a Contextual Bandit,arXiv:1809.09672 [cs],May,2019,http://arxiv.org/abs/1809.09672,"In this work, we propose a novel method for training neural networks to perform singledocument extractive summarization without heuristically-generated extractive labels. We call our approach BANDITSUM as it treats extractive summarization as a contextual bandit (CB) problem, where the model receives a document to summarize (the context), and chooses a sequence of sentences to include in the summary (the action). A policy gradient reinforcement learning algorithm is used to train the model to select sequences of sentences that maximize ROUGE score. We perform a series of experiments demonstrating that BANDITSUM is able to achieve ROUGE scores that are better than or comparable to the state-of-the-art for extractive summarization, and converges using signiï¬Åcantly fewer update steps than competing approaches. In addition, we show empirically that BANDITSUM performs signiï¬Åcantly better than competing approaches when good summary sentences appear late in the source document1.",,Computer Science - Computation and Language,,
7,drozdov_supervised_2020,"Drozdov, Ignat; Forbes, Daniel; Szubert, Benjamin; Hall, Mark; Carlin, Chris; Lowe, David J.",Supervised and unsupervised language modelling in Chest X-Ray radiological reports,PLOS ONE,March,2020,https://dx.plos.org/10.1371/journal.pone.0229963,"Chest radiography (CXR) is the most commonly used imaging modality and deep neural network (DNN) algorithms have shown promise in effective triage of normal and abnormal radiograms. Typically, DNNs require large quantities of expertly labelled training exemplars, which in clinical contexts is a major bottleneck to effective modelling, as both considerable clinical skill and time is required to produce high-quality ground truths. In this work we evaluate thirteen supervised classifiers using two large free-text corpora and demonstrate that bidirectional long short-term memory (BiLSTM) networks with attention mechanism effectively identify Normal, Abnormal, and Unclear CXR reports in internal (n = 965 manually-labelled reports, f1-score = 0.94) and external (n = 465 manually-labelled reports, f1-score = 0.90) testing sets using a relatively small number of expert-labelled training observations (n = 3,856 annotated reports). Furthermore, we introduce a general unsupervised approach that accurately distinguishes Normal and Abnormal CXR reports in a large unlabelled corpus. We anticipate that the results presented in this work can be used to automatically extract standardized clinical information from free-text CXR radiological reports, facilitating the training of clinical decision support systems for CXR triage.",,,,
7,ducoffe_adversarial_2018,"Ducoffe, Melanie; Precioso, Frederic",Adversarial Active Learning for Deep Networks: a Margin Based Approach,"arXiv:1802.09841 [cs, stat]",February,2018,http://arxiv.org/abs/1802.09841,"We propose a new active learning strategy designed for deep neural networks. The goal is to minimize the number of data annotation queried from an oracle during training. Previous active learning strategies scalable for deep networks were mostly based on uncertain sample selection. In this work, we focus on examples lying close to the decision boundary. Based on theoretical works on margin theory for active learning, we know that such examples may help to considerably decrease the number of annotations. While measuring the exact distance to the decision boundaries is intractable, we propose to rely on adversarial examples. We do not consider anymore them as a threat instead we exploit the information they provide on the distribution of the input space in order to approximate the distance to decision boundaries. We demonstrate empirically that adversarial active queries yield faster convergence of CNNs trained on MNIST, the Shoe-Bag and the Quick-Draw datasets.",,"Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning",,
7,edizel_misspelling_2019,"Edizel, Bora; Piktus, Aleksandra; Bojanowski, Piotr; Ferreira, Rui; Grave, Edouard; Silvestri, Fabrizio",Misspelling Oblivious Word Embeddings,arXiv:1905.09755 [cs],May,2019,http://arxiv.org/abs/1905.09755,"In this paper we present a method to learn word embeddings that are resilient to misspellings. Existing word embeddings have limited applicability to malformed texts, which contain a non-negligible amount of out-of-vocabulary words. We propose a method combining FastText with subwords and a supervised task of learning misspelling patterns. In our method, misspellings of each word are embedded close to their correct variants. We train these embeddings on a new dataset we are releasing publicly. Finally, we experimentally show the advantages of this approach on both intrinsic and extrinsic NLP tasks using public test sets.",,"Computer Science - Computation and Language, Computer Science - Machine Learning",,
7,el-assady_semantic_2019,"El-Assady, M.; Kehlbeck, R.; Collins, C.; Keim, D.; Deussen, O.",Semantic Concept Spaces: Guided Topic Model Refinement using Word-Embedding Projections,IEEE Transactions on Visualization and Computer Graphics,,2019,,"We present a framework that allows users to incorporate the semantics of their domain knowledge for topic model refinement while remaining model-agnostic. Our approach enables users to (1) understand the semantic space of the model, (2) identify regions of potential conflicts and problems, and (3) readjust the semantic relation of concepts based on their understanding, directly influencing the topic modeling. These tasks are supported by an interactive visual analytics workspace that uses word-embedding projections to define concept regions which can then be refined. The user-refined concepts are independent of a particular document collection and can be transferred to related corpora. All user interactions within the concept space directly affect the semantic relations of the underlying vector space model, which, in turn, change the topic modeling. In addition to direct manipulation, our system guides the users' decisionmaking process through recommended interactions that point out potential improvements. This targeted refinement aims at minimizing the feedback required for an efficient human-in-the-loop process. We confirm the improvements achieved through our approach in two user studies that show topic model quality improvements through our visual knowledge externalization and learning process.",,"Analytical models, Computational modeling, Guided Visual Analytics, Machine learning, Mixed-Initiative Refinement, Semantic Mapping, Semantics, Task analysis, Topic Model Optimization, Visual analytics, Word Embedding",,
7,el-assady_semantic_2020,"El-Assady, Mennatallah; Kehlbeck, Rebecca; Collins, Christopher; Keim, Daniel; Deussen, Oliver",Semantic Concept Spaces: Guided Topic Model Refinement using Word-Embedding Projections,IEEE Transactions on Visualization and Computer Graphics,January,2020,,"We present a framework that allows users to incorporate the semantics of their domain knowledge for topic model refinement while remaining model-agnostic. Our approach enables users to (1) understand the semantic space of the model, (2) identify regions of potential conflicts and problems, and (3) readjust the semantic relation of concepts based on their understanding, directly influencing the topic modeling. These tasks are supported by an interactive visual analytics workspace that uses word-embedding projections to define concept regions which can then be refined. The user-refined concepts are independent of a particular document collection and can be transferred to related corpora. All user interactions within the concept space directly affect the semantic relations of the underlying vector space model, which, in turn, change the topic modeling. In addition to direct manipulation, our system guides the users' decisionmaking process through recommended interactions that point out potential improvements. This targeted refinement aims at minimizing the feedback required for an efficient human-in-the-loop process. We confirm the improvements achieved through our approach in two user studies that show topic model quality improvements through our visual knowledge externalization and learning process.",,"Analytical models, Computational modeling, Guided Visual Analytics, Machine learning, Mixed-Initiative Refinement, Semantic Mapping, Semantics, Task analysis, Topic Model Optimization, Visual analytics, Word Embedding, data analysis, data visualisation, document handling, guided topic model refinement, interactive systems, interactive visual analytics, learning (artificial intelligence), learning process, natural language processing, semantic concept spaces, topic model quality, topic modeling, user interactions, user-refined concepts, vector space model, vectors, visual knowledge externalization, word-embedding projections",,
7,el-assady_visual_2019,"El-Assady, Mennatallah; Sperrle, Fabian; Deussen, Oliver; Keim, Daniel; Collins, Christopher",Visual Analytics for Topic Model Optimization based on User-Steerable Speculative Execution,IEEE Transactions on Visualization and Computer Graphics,January,2019,,"To effectively assess the potential consequences of human interventions in model-driven analytics systems, we establish the concept of speculative execution as a visual analytics paradigm for creating user-steerable preview mechanisms. This paper presents an explainable, mixed-initiative topic modeling framework that integrates speculative execution into the algorithmic decision-making process. Our approach visualizes the model-space of our novel incremental hierarchical topic modeling algorithm, unveiling its inner-workings. We support the active incorporation of the user's domain knowledge in every step through explicit model manipulation interactions. In addition, users can initialize the model with expected topic seeds, the backbone priors. For a more targeted optimization, the modeling process automatically triggers a speculative execution of various optimization strategies, and requests feedback whenever the measured model quality deteriorates. Users compare the proposed optimizations to the current model state and preview their effect on the next model iterations, before applying one of them. This supervised human-in-the-Ioop process targets maximum improvement for minimum feedback and has proven to be effective in three independent studies that confirm topic model quality improvements.",,"Analytical models, Clustering algorithms, Computational modeling, Explainable Machine Learning, Mixed-Initiative Visual Analytics, Optimization, Speculative Execution, Task analysis, User-Steerable Topic Modeling, Visual analytics, algorithmic decision-making process, current model state, data analysis, data visualisation, decision making, expected topic seeds, explicit model manipulation interactions, human interventions, human-in-the-Ioop process, incremental hierarchical topic modeling algorithm, minimum feedback, mixed-initiative topic modeling framework, model iterations, model-driven analytics systems, model-space, optimisation, optimization strategies, potential consequences, requests feedback, topic model optimization, topic model quality improvements, user-steerable preview mechanisms, user-steerable speculative execution, users domain knowledge, visual analytics paradigm",,
7,ellinger_klout_2017,"Ellinger, Sarah; Bhattacharyya, Prantik; Bhargava, Preeti; Spasojevic, Nemanja",Klout Topics for Modeling Interests and Expertise of Users Across Social Networks,arXiv:1710.09824 [cs],October,2017,http://arxiv.org/abs/1710.09824,"This paper presents Klout Topics, a lightweight ontology to describe social media users' topics of interest and expertise. Klout Topics is designed to: be human-readable and consumer-friendly; cover multiple domains of knowledge in depth; and promote data extensibility via knowledge base entities. We discuss why this ontology is well-suited for text labeling and interest modeling applications, and how it compares to available alternatives. We show its coverage against common social media interest sets, and examples of how it is used to model the interests of over 780M social media users on Klout.com. Finally, we open the ontology for external use.",,"Computer Science - Artificial Intelligence, Computer Science - Information Retrieval, Computer Science - Social and Information Networks",,
7,ensan_ad_2019,"Ensan, Faezeh; Du, Weichang",Ad hoc retrieval via entity linking and semantic similarity,Knowledge and Information Systems,March,2019,http://link.springer.com/10.1007/s10115-018-1190-1,"Semantic search has emerged as a possible way for addressing the challenges of traditional keyword-based retrieval systems such as the vocabulary gap between the query and document spaces. In this paper, we propose a novel semantic retrieval framework that uses semantic entity linking systems for forming a graph representation of documents and queries, where nodes represent concepts extracted from documents and edges represent semantic relatedness between those concepts. The core of our proposed work is a semantic-enabled language model that estimates the probability of generating query concepts given values assigned to document concepts. The semantic retrieval framework also provides basis for interpolating keyword-based retrieval systems with the semantic-enabled language model. We conduct comprehensive experiments over several Trec document collections and analyze the performance of different conï¬Ågurations of the framework across multiple retrieval measures. Our experimental results show that the proposed semantic retrieval model has a synergistic impact on the results obtained through the state-of-the-art keyword-based systems, and the consideration of semantic information can complement and enhance the performance of such retrieval models.",,,,
7,fan_modeling_2018,"Fan, Yixing; Guo, Jiafeng; Lan, Yanyan; Xu, Jun; Zhai, Chengxiang; Cheng, Xueqi",Modeling Diverse Relevance Patterns in Ad-hoc Retrieval,The 41st International ACM SIGIR Conference on Research \& Development in Information Retrieval - SIGIR '18,,2018,http://arxiv.org/abs/1805.05737,"Assessing relevance between a query and a document is challenging in ad-hoc retrieval due to its diverse patterns, i.e., a document could be relevant to a query as a whole or partially as long as it provides sufficient information for users' need. Such diverse relevance patterns require an ideal retrieval model to be able to assess relevance in the right granularity adaptively. Unfortunately, most existing retrieval models compute relevance at a single granularity, either document-wide or passage-level, or use fixed combination strategy, restricting their ability in capturing diverse relevance patterns. In this work, we propose a data-driven method to allow relevance signals at different granularities to compete with each other for final relevance assessment. Specifically, we propose a HIerarchical Neural maTching model (HiNT) which consists of two stacked components, namely local matching layer and global decision layer. The local matching layer focuses on producing a set of local relevance signals by modeling the semantic matching between a query and each passage of a document. The global decision layer accumulates local signals into different granularities and allows them to compete with each other to decide the final relevance score. Experimental results demonstrate that our HiNT model outperforms existing state-of-the-art retrieval models significantly on benchmark ad-hoc retrieval datasets.",,Computer Science - Information Retrieval,,
7,fan_reducing_2019,"Fan, Angela; Grave, Edouard; Joulin, Armand",Reducing Transformer Depth on Demand with Structured Dropout,"arXiv:1909.11556 [cs, stat]",September,2019,http://arxiv.org/abs/1909.11556,"Overparameterized transformer networks have obtained state of the art results in various natural language processing tasks, such as machine translation, language modeling, and question answering. These models contain hundreds of millions of parameters, necessitating a large amount of computation and making them prone to overï¬Åtting. In this work, we explore LayerDrop, a form of structured dropout, which has a regularization effect during training and allows for efï¬Åcient pruning at inference time. In particular, we show that it is possible to select sub-networks of any depth from one large network without having to ï¬Ånetune them and with limited impact on performance. We demonstrate the effectiveness of our approach by improving the state of the art on machine translation, language modeling, summarization, question answering, and language understanding benchmarks. Moreover, we show that our approach leads to small BERT-like models of higher quality compared to training from scratch or using distillation.",,"Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning",,
7,farruque_augmenting_2019,"Farruque, Nawshad; Zaiane, Osmar; Goebel, Randy",Augmenting Semantic Representation of Depressive Language: from Forums to Microblogs,,,2019,,"We discuss and analyze the process of creating word embedding feature representations speciï¬Åcally designed for a learning task when annotated data is scarce, like depressive language detection from Tweets. We start from rich word embedding pre-trained from a general dataset, then enhance it with embedding learned from a domain speciï¬Åc but relatively much smaller dataset. Our strengthened representation portrays better the domain of depression we are interested in as it combines the semantics learned from the speciï¬Åc domain and word coverage from the general language. We present a comparative analyses of our word embedding representations with a simple bag-of-words model, a well known sentiment lexicon, a psycholinguistic lexicon, and a general pre-trained word embedding, based on their eï¬ƒcacy in accurately identifying depressive Tweets. We show that our representations achieve a signiï¬Åcantly better F1 score than the others when applied to a high quality dataset.",,"Biomedical Text, Depression, Twitter",,
7,feder_active_2020,"Feder, Amir; Vainstein, Danny; Rosenfeld, Roni; Hartman, Tzvika; Hassidim, Avinatan; Matias, Yossi",Active deep learning to detect demographic traits in free-form clinical notes,Journal of Biomedical Informatics,July,2020,https://doi.org/10.1016/j.jbi.2020.103436,"The free-form portions of clinical notes are a significant source of information for research, but before they can be used, they must be de-identified to protect patients' privacy. De-identification efforts have focused on known identifier types (names, ages, dates, addresses, ID's, etc.). However, a note can contain residual â€œDemographic Traitsâ€ù (DTs), unique enough to re-identify the patient when combined with other such facts. Here we examine whether any residual risks remain after removing these identifiers. After manually annotating over 140,000 words worth of medical notes, we found no remaining directly identifying information, and a low prevalence of demographic traits, such as marital status or housing type. We developed an annotation guide to the discovered Demographic Traits (DTs) and used it to label MIMIC-III and i2b2-2006 clinical notes as test sets. We then designed a â€œbootstrappedâ€ù active learning iterative process for identifying DTs: we tentatively labeled as positive all sentences in the DT-rich note sections, used these to train a binary classifier, manually corrected acute errors, and retrained the classifier. This train-and-correct process may be iterated. Our active learning process significantly improved the classifier's accuracy. Moreover, our BERT-based model outperformed non-neural models when trained on both tentatively labeled data and manually relabeled examples. To facilitate future research and benchmarking, we also produced and made publicly available our human annotated DT-tagged datasets. We conclude that directly identifying information is virtually non-existent in the multiple medical note types we investigated. Demographic traits are present in medical notes, but can be detected with high accuracy using a cost-effective human-in-the-loop active learning process, and redacted if desired.",,,,
7,fedus_maskgan:_2018,"Fedus, William; Goodfellow, Ian; Dai, Andrew M.",MaskGAN: Better Text Generation via Filling in the\_\_\_\_\_\_,"arXiv:1801.07736 [cs, stat]",January,2018,http://arxiv.org/abs/1801.07736,"Neural text generation models are often autoregressive language models or seq2seq models. These models generate text by sampling words sequentially, with each word conditioned on the previous word, and are state-of-the-art for several machine translation and summarization benchmarks. These benchmarks are often defined by validation perplexity even though this is not a direct measure of the quality of the generated text. Additionally, these models are typically trained via maxi- mum likelihood and teacher forcing. These methods are well-suited to optimizing perplexity but can result in poor sample quality since generating text requires conditioning on sequences of words that may have never been observed at training time. We propose to improve sample quality using Generative Adversarial Networks (GANs), which explicitly train the generator to produce high quality samples and have shown a lot of success in image generation. GANs were originally designed to output differentiable values, so discrete language generation is challenging for them. We claim that validation perplexity alone is not indicative of the quality of text generated by a model. We introduce an actor-critic conditional GAN that fills in missing text conditioned on the surrounding context. We show qualitatively and quantitatively, evidence that this produces more realistic conditional and unconditional text samples compared to a maximum likelihood trained model.",,"Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning",,
7,ferreira_assessing_2013,"Ferreira, Rafael; de Souza Cabral, Luciano; Lins, Rafael Dueire; Pereira e Silva, Gabriel; Freitas, Fred; Cavalcanti, George D. C.; Lima, Rinaldo; Simske, Steven J.; Favaro, Luciano",Assessing sentence scoring techniques for extractive text summarization,Expert Systems with Applications,October,2013,https://linkinghub.elsevier.com/retrieve/pii/S0957417413002601,"Text summarization is the process of automatically creating a shorter version of one or more text documents. It is an important way of ï¬Ånding relevant information in large text libraries or in the Internet. Essentially, text summarization techniques are classiï¬Åed as Extractive and Abstractive. Extractive techniques perform text summarization by selecting sentences of documents according to some criteria. Abstractive summaries attempt to improve the coherence among sentences by eliminating redundancies and clarifying the contest of sentences. In terms of extractive summarization, sentence scoring is the technique most used for extractive text summarization. This paper describes and performs a quantitative and qualitative assessment of 15 algorithms for sentence scoring available in the literature. Three different datasets (News, Blogs and Article contexts) were evaluated. In addition, directions to improve the sentence extraction results obtained are suggested.",,,,
7,fisher_query-focused_nodate,"Fisher, Seeger; Roark, Brian",Query-focused summarization by supervised sentence ranking and skewed word distributions,,,,,"We present a supervised sentence ranking approach for use in extractive summarization. The supervised approach achieves domain independence by making use of a range of word distribution statistics as features, of the sort typically used for unsupervised domain-independent ranking. We present empirical trials on the DUC 2006 query-directed multi-document summarization task, and demonstrate that the very general machine learning approaches taken can provide competitive results for this task. The general approach provides great ï¬‚exibility for incorporating many more features.",,summarization,,
7,fitzgerald_implicit_2017,"FitzGerald, ChloÃ«; Hurst, Samia",Implicit bias in healthcare professionals: a systematic review,BMC Medical Ethics,March,2017,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5333436/,"Background Implicit biases involve associations outside conscious awareness that lead to a negative evaluation of a person on the basis of irrelevant characteristics such as race or gender. This review examines the evidence that healthcare professionals display implicit biases towards patients.  Methods PubMed, PsychINFO, PsychARTICLE and CINAHL were searched for peer-reviewed articles published between 1st March 2003 and 31st March 2013. Two reviewers assessed the eligibility of the identified papers based on precise content and quality criteria. The references of eligible papers were examined to identify further eligible studies.  Results Forty two articles were identified as eligible. Seventeen used an implicit measure (Implicit Association Test in fifteen and subliminal priming in two), to test the biases of healthcare professionals. Twenty five articles employed a between-subjects design, using vignettes to examine the influence of patient characteristics on healthcare professionalsâ€™ attitudes, diagnoses, and treatment decisions. The second method was included although it does not isolate implicit attitudes because it is recognised by psychologists who specialise in implicit cognition as a way of detecting the possible presence of implicit bias. Twenty seven studies examined racial/ethnic biases; ten other biases were investigated, including gender, age and weight. Thirty five articles found evidence of implicit bias in healthcare professionals; all the studies that investigated correlations found a significant positive relationship between level of implicit bias and lower quality of care.  Discussion The evidence indicates that healthcare professionals exhibit the same levels of implicit bias as the wider population. The interactions between multiple patient characteristics and between healthcare professional and patient characteristics reveal the complexity of the phenomenon of implicit bias and its influence on clinician-patient interaction. The most convincing studies from our review are those that combine the IAT and a method measuring the quality of treatment in the actual world. Correlational evidence indicates that biases are likely to influence diagnosis and treatment decisions and levels of care in some circumstances and need to be further investigated. Our review also indicates that there may sometimes be a gap between the norm of impartiality and the extent to which it is embraced by healthcare professionals for some of the tested characteristics.  Conclusions Our findings highlight the need for the healthcare profession to address the role of implicit biases in disparities in healthcare. More research in actual care settings and a greater homogeneity in methods employed to test implicit biases in healthcare is needed.",,,,
7,fonferko-shadrach_using_2019,"Fonferko-Shadrach, Beata; Lacey, Arron S.; Roberts, Angus; Akbari, Ashley; Thompson, Simon; Ford, David V.; Lyons, Ronan A.; Rees, Mark I.; Pickrell, William Owen",Using natural language processing to extract structured epilepsy data from unstructured clinic letters: development and validation of the ExECT (extraction of epilepsy clinical text) system,BMJ Open,April,2019,https://bmjopen.bmj.com/content/9/4/e023232,"Objective Routinely collected healthcare data are a powerful research resource but often lack detailed disease-specific information that is collected in clinical free text, for example, clinic letters. We aim to use natural language processing techniques to extract detailed clinical information from epilepsy clinic letters to enrich routinely collected data. Design We used the general architecture for text engineering (GATE) framework to build an information extraction system, ExECT (extraction of epilepsy clinical text), combining rule-based and statistical techniques. We extracted nine categories of epilepsy information in addition to clinic date and date of birth across 200 clinic letters. We compared the results of our algorithm with a manual review of the letters by an epilepsy clinician. Setting De-identified and pseudonymised epilepsy clinic letters from a Health Board serving half a million residents in Wales, UK. Results We identified 1925 items of information with overall precision, recall and F1 score of 91.4\%, 81.4\% and 86.1\%, respectively. Precision and recall for epilepsy-specific categories were: epilepsy diagnosis (88.1\%, 89.0\%), epilepsy type (89.8\%, 79.8\%), focal seizures (96.2\%, 69.7\%), generalised seizures (88.8\%, 52.3\%), seizure frequency (86.3\%â€“53.6\%), medication (96.1\%, 94.0\%), CT (55.6\%, 58.8\%), MRI (82.4\%, 68.8\%) and electroencephalogram (81.5\%, 75.3\%). Conclusions We have built an automated clinical text extraction system that can accurately extract epilepsy information from free text in clinic letters. This can enhance routinely collected data for research in the UK. The information extracted with ExECT such as epilepsy type, seizure frequency and neurological investigations are often missing from routinely collected data. We propose that our algorithm can bridge this data gap enabling further epilepsy research opportunities. While many of the rules in our pipeline were tailored to extract epilepsy specific information, our methods can be applied to other diseases and also can be used in clinical practice to record patient information in a structured manner.",,"epilepsy, information extraction, natural language processing, validation",,
7,frantzi_automatic_2000,"Frantzi, Katerina; Ananiadou, Sophia; Mima, Hideki",Automatic recognition of multi-word terms:. the C-value/NC-value method,International Journal on Digital Libraries,August,2000,https://doi.org/10.1007/s007999900023,". Technical terms (henceforth called terms ), are important elements for digital libraries. In this paper we present a domain-independent method for the automatic extraction of multi-word terms, from machine-readable special language corpora. The method, (C-value/NC-value ), combines linguistic and statistical information. The first part, C-value, enhances the common statistical measure of frequency of occurrence for term extraction, making it sensitive to a particular type of multi-word terms, the nested terms. The second part, NC-value, gives: 1) a method for the extraction of term context words (words that tend to appear with terms); 2) the incorporation of information from term context words to the extraction of terms.",,Key words: Terms â€“ Automatic extraction â€“ Domain independence â€“ Automatic Term Recognition (ATR) â€“ Linguistic and statistical information,,
7,fraser_extracting_2019,"Fraser, Kathleen C.; Nejadgholi, Isar; De Bruijn, Berry; Li, Muqun; LaPlante, Astha; Abidine, Khaldoun Zine El",Extracting UMLS Concepts from Medical Text Using General and Domain-Specific Deep Learning Models,arXiv:1910.01274 [cs],October,2019,http://arxiv.org/abs/1910.01274,"Entity recognition is a critical first step to a number of clinical NLP applications, such as entity linking and relation extraction. We present the first attempt to apply state-of-the-art entity recognition approaches on a newly released dataset, MedMentions. This dataset contains over 4000 biomedical abstracts, annotated for UMLS semantic types. In comparison to existing datasets, MedMentions contains a far greater number of entity types, and thus represents a more challenging but realistic scenario in a real-world setting. We explore a number of relevant dimensions, including the use of contextual versus non-contextual word embeddings, general versus domain-specific unsupervised pre-training, and different deep learning architectures. We contrast our results against the well-known i2b2 2010 entity recognition dataset, and propose a new method to combine general and domain-specific information. While producing a state-of-the-art result for the i2b2 2010 task (F1 = 0.90), our results on MedMentions are significantly lower (F1 = 0.63), suggesting there is still plenty of opportunity for improvement on this new data.",,"Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing",,
7,fu_survey_2012,"Fu, Yifan; Zhu, Xingquan; Li, Bin",A survey on instance selection for active learning,Knowledge and information systems,June,2012,https://link.springer.com/article/10.1007/s10115-012-0507-8,"Active learning aims to train an accurate prediction model with minimum cost by labeling most informative instances. In this paper, we survey existing works on active learning from an instance-selection perspective and classify them into two categories with a progressive relationship: (1) active learning merely based on uncertainty of independent and identically distributed (IID) instances, and (2) active learning by further taking into account instance correlations. Using the above categorization, we summarize major approaches in the field, along with their technical strengths/weaknesses, followed by a simple runtime performance comparison, and discussion about emerging active learning applications and instance-selection challenges therein. This survey intends to provide a high-level summarization for active learning and motivates interested readers to consider instance-selection approaches for designing effective active learning solutions.",,,,
7,gal_deep_2017,"Gal, Yarin; Islam, Riashat; Ghahramani, Zoubin",Deep Bayesian Active Learning with Image Data,arXiv:1703.02910,March,2017,https://arxiv.org/abs/1703.02910,"Even though active learning forms an important pillar of machine learning, deep learning tools are not prevalent within it. Deep learning poses several difficulties when used in an active learning setting. First, active learning (AL) methods generally rely on being able to learn and update models from small amounts of data. Recent advances in deep learning, on the other hand, are notorious for their dependence on large amounts of data. Second, many AL acquisition functions rely on model uncertainty, yet deep learning methods rarely represent such model uncertainty. In this paper we combine recent advances in Bayesian deep learning into the active learning framework in a practical way. We develop an active learning framework for high dimensional data, a task which has been extremely challenging so far, with very sparse existing literature. Taking advantage of specialised models such as Bayesian convolutional neural networks, we demonstrate our active learning techniques with image data, obtaining a significant improvement on existing active learning approaches. We demonstrate this on both the MNIST dataset, as well as for skin cancer diagnosis from lesion images (ISIC2016 task).",,,,
7,gao_abstractive_2019,"Gao, Shen; Chen, Xiuying; Li, Piji; Ren, Zhaochun; Bing, Lidong; Zhao, Dongyan; Yan, Rui",Abstractive Text Summarization by Incorporating Reader Comments,Proceedings of the AAAI Conference on Artificial Intelligence,July,2019,http://www.aaai.org/ojs/index.php/AAAI/article/view/4603,"In neural abstractive summarization field, conventional sequence-to-sequence based models often suffer from summarizing the wrong aspect of the document with respect to the main aspect. To tackle this problem, we propose the task of reader-aware abstractive summary generation, which utilizes the reader comments to help the model produce better summary about the main aspect. Unlike traditional abstractive summarization task, reader-aware summarization confronts two main challenges: (1) Comments are informal and noisy; (2) jointly modeling the news document and the reader comments is challenging. To tackle the above challenges, we design an adversarial learning model named reader-aware summary generator (RASG), which consists of four components: (1) a sequence-to-sequence based summary generator; (2) a reader attention module capturing the reader focused aspects; (3) a supervisor modeling the semantic gap between the generated summary and reader focused aspects; (4) a goal tracker producing the goal for each generation step. The supervisor and the goal tacker are used to guide the training of our framework in an adversarial manner. Extensive experiments are conducted on our large-scale real-world text summarization dataset, and the results show that RASG achieves the stateof-the-art performance in terms of both automatic metrics and human evaluations. The experimental results also demonstrate the effectiveness of each module in our framework. We release our large-scale dataset for further research1.",,,,
7,garcia_biomedical_2015,"GarcÃ_a, Marcos Antonio MouriÃ±o; RodrÃ_guez, Roberto PÃ©rez; RifÃ_n, Luis E. Anido",Biomedical literature classification using encyclopedic knowledge: a Wikipedia-based bag-of-concepts approach,PeerJ,September,2015,https://peerj.com/articles/1279,,,,,
7,geifman_deep_2018,"Geifman, Yonatan; El-Yaniv, Ran",Deep Active Learning with a Neural Architecture Search,"arXiv:1811.07579 [cs, stat]",November,2018,http://arxiv.org/abs/1811.07579,"We consider active learning of deep neural networks. Most active learning works in this context have focused on studying effective querying mechanisms and assumed that an appropriate network architecture is a priori known for the problem at hand. We challenge this assumption and propose a novel active strategy whereby the learning algorithm searches for effective architectures on the fly, while actively learning. We apply our strategy using three known querying techniques (softmax response, MC-dropout, and coresets) and show that the proposed approach overwhelmingly outperforms active learning using fixed architectures.",,"Computer Science - Machine Learning, Statistics - Machine Learning",,
7,ghaeini_dr-bilstm:_2018,"Ghaeini, Reza; Hasan, Sadid A.; Datla, Vivek; Liu, Joey; Lee, Kathy; Qadir, Ashequl; Ling, Yuan; Prakash, Aaditya; Fern, Xiaoli Z.; Farri, Oladimeji",DR-BiLSTM: Dependent Reading Bidirectional LSTM for Natural Language Inference,arXiv:1802.05577 [cs],February,2018,http://arxiv.org/abs/1802.05577,"We present a novel deep learning architecture to address the natural language inference (NLI) task. Existing approaches mostly rely on simple reading mechanisms for independent encoding of the premise and hypothesis. Instead, we propose a novel dependent reading bidirectional LSTM network (DR-BiLSTM) to efficiently model the relationship between a premise and a hypothesis during encoding and inference. We also introduce a sophisticated ensemble strategy to combine our proposed models, which noticeably improves final predictions. Finally, we demonstrate how the results can be improved further with an additional preprocessing step. Our evaluation shows that DR-BiLSTM obtains the best single model and ensemble model results achieving the new state-of-the-art scores on the Stanford NLI dataset.",,Computer Science - Computation and Language,,
7,gibaja_tutorial_2015,"Gibaja, Eva; Ventura, SebastiÃ¡n",A Tutorial on Multilabel Learning,ACM Comput. Surv.,April,2015,http://doi.acm.org/10.1145/2716262,,,"Multilabel learning, classification, data mining, machine learning, ranking",,
7,gilpin_explaining_2018,"Gilpin, Leilani H.; Bau, David; Yuan, Ben Z.; Bajwa, Ayesha; Specter, Michael; Kagal, Lalana",Explaining Explanations: An Overview of Interpretability of Machine Learning,"arXiv:1806.00069 [cs, stat]",May,2018,http://arxiv.org/abs/1806.00069,"There has recently been a surge of work in explanatory artificial intelligence (XAI). This research area tackles the important problem that complex machines and algorithms often cannot provide insights into their behavior and thought processes. XAI allows users and parts of the internal system to be more transparent, providing explanations of their decisions in some level of detail. These explanations are important to ensure algorithmic fairness, identify potential bias/problems in the training data, and to ensure that the algorithms perform as expected. However, explanations produced by these systems is neither standardized nor systematically assessed. In an effort to create best practices and identify open challenges, we provide our definition of explainability and show how it can be used to classify existing literature. We discuss why current approaches to explanatory methods especially for deep neural networks are insufficient. Finally, based on our survey, we conclude with suggested future research directions for explanatory artificial intelligence.",,"Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning",,
7,gilpin_explaining_2019,"Gilpin, Leilani H.; Bau, David; Yuan, Ben Z.; Bajwa, Ayesha; Specter, Michael; Kagal, Lalana",Explaining Explanations: An Overview of Interpretability of Machine Learning,"arXiv:1806.00069 [cs, stat]",February,2019,http://arxiv.org/abs/1806.00069,"There has recently been a surge of work in explanatory artiï¬Åcial intelligence (XAI). This research area tackles the important problem that complex machines and algorithms often cannot provide insights into their behavior and thought processes. XAI allows users and parts of the internal system to be more transparent, providing explanations of their decisions in some level of detail. These explanations are important to ensure algorithmic fairness, identify potential bias/problems in the training data, and to ensure that the algorithms perform as expected. However, explanations produced by these systems is neither standardized nor systematically assessed. In an effort to create best practices and identify open challenges, we describe foundational concepts of explainability and show how they can be used to classify existing literature. We discuss why current approaches to explanatory methods especially for deep neural networks are insufï¬Åcient. Finally, based on our survey, we conclude with suggested future research directions for explanatory artiï¬Åcial intelligence.",,"Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning",,
7,giorgi_end--end_2019,"Giorgi, John; Wang, Xindi; Sahar, Nicola; Shin, Won Young; Bader, Gary D.; Wang, Bo",End-to-end Named Entity Recognition and Relation Extraction using Pre-trained Language Models,arXiv:1912.13415 [cs],December,2019,http://arxiv.org/abs/1912.13415,"Named entity recognition (NER) and relation extraction (RE) are two important tasks in information extraction and retrieval (IE {\textbackslash}\& IR). Recent work has demonstrated that it is beneficial to learn these tasks jointly, which avoids the propagation of error inherent in pipeline-based systems and improves performance. However, state-of-the-art joint models typically rely on external natural language processing (NLP) tools, such as dependency parsers, limiting their usefulness to domains (e.g. news) where those tools perform well. The few neural, end-to-end models that have been proposed are trained almost completely from scratch. In this paper, we propose a neural, end-to-end model for jointly extracting entities and their relations which does not rely on external NLP tools and which integrates a large, pre-trained language model. Because the bulk of our model's parameters are pre-trained and we eschew recurrence for self-attention, our model is fast to train. On 5 datasets across 3 domains, our model matches or exceeds state-of-the-art performance, sometimes by a large margin.",,"Computer Science - Computation and Language, Computer Science - Machine Learning",,
7,goyal_graph_2018,"Goyal, Palash; Ferrara, Emilio","Graph Embedding Techniques, Applications, and Performance: A Survey",Knowledge-Based Systems,July,2018,http://arxiv.org/abs/1705.02801,"Graphs, such as social networks, word co-occurrence networks, and communication networks, occur naturally in various real-world applications. Analyzing them yields insight into the structure of society, language, and diï¬€erent patterns of communication. Many approaches have been proposed to perform the analysis. Recently, methods which use the representation of graph nodes in vector space have gained traction from the research community. In this survey, we provide a comprehensive and structured analysis of various graph embedding techniques proposed in the literature. We ï¬Årst introduce the embedding task and its challenges such as scalability, choice of dimensionality, and features to be preserved, and their possible solutions. We then present three categories of approaches based on factorization methods, random walks, and deep learning, with examples of representative algorithms in each category and analysis of their performance on various tasks. We evaluate these state-of-the-art methods on a few common datasets and compare their performance against one another. Our analysis concludes by suggesting some potential applications and future directions. We ï¬Ånally present the open-source Python library we developed, named GEM (Graph Embedding Methods, available at https://github.com/palash1992/GEM), which provides all presented algorithms within a uniï¬Åed interface to foster and facilitate research on the topic.",,"Computer Science - Machine Learning, Computer Science - Social and Information Networks, Physics - Data Analysis, Statistics and Probability",,
7,griffiths_topics_2007,"Griffiths, Thomas L.; Steyvers, Mark; Tenenbaum, Joshua B.",Topics in semantic representation,Psychological Review,April,2007,,"Processing language requires the retrieval of concepts from memory in response to an ongoing stream of information. This retrieval is facilitated if one can infer the gist of a sentence, conversation, or document and use that gist to predict related concepts and disambiguate words. This article analyzes the abstract computational problem underlying the extraction and use of gist, formulating this problem as a rational statistical inference. This leads to a novel approach to semantic representation in which word meanings are represented in terms of a set of probabilistic topics. The topic model performs well in predicting word association and the effects of semantic association and ambiguity on a variety of language-processing and memory tasks. It also provides a foundation for developing more richly structured statistical models of language, as the generative process assumed in the topic model can easily be extended to incorporate other kinds of semantic and syntactic structure.",,"Bayes Theorem, Humans, Language, Models, Statistical, Semantics, Speech Perception",,
7,gu_extractive_2019,"Gu, Yang; Hu, Yanke",Extractive Summarization with Very Deep Pretrained Language Model,International Journal of Artificial Intelligence \& Applications,March,2019,http://aircconline.com/ijaia/V10N2/10219ijaia03.pdf,"Recent development of generative pretrained language models has been proven very successful on a wide range of NLP tasks, such as text classification, question answering, textual entailment and so on. In this work, we present a two-phase encoder decoder architecture based on Bidirectional Encoding Representation from Transformers(BERT) for extractive summarization task. We evaluated our model by both automatic metrics and human annotators, and demonstrated that the architecture achieves the stateof-the-art comparable result on large scale corpus â€“ CNN/Daily Mail1 . As the best of our knowledge, this is the first work that applies BERT based architecture to a text summarization task and achieved the stateof-the-art comparable result.",,,,
7,gu_lessons_2020,"Gu, Hongyan; Huang, Jingbin; Hung, Lauren; Chen, Xiang 'Anthony'",Lessons Learned from Designing an AI-Enabled Diagnosis Tool for Pathologists,arXiv:2006.12695 [cs.HC],June,2020,https://arxiv.org/abs/2006.12695,"Despite the promises of data-driven artificial intelligence (AI), little is known about how we can bridge the gulf between traditional physician-driven diagnosis and a plausible future of medicine automated by AI. Specifically, how can we involve AI usefully in physicians' diagnosis workflow given that most AI is still nascent and error-prone (e.g., in digital pathology)? To explore this question, we first propose a series of collaborative techniques to engage human pathologists with AI given AI's capabilities and limitations, based on which we prototype Impetus - a tool where an AI takes various degrees of initiatives to provide various forms of assistance to a pathologist in detecting tumors from histological slides. Finally, we summarize observations and lessons learned from a study with eight pathologists and discuss recommendations for future work on human-centered medical AI systems.",,,,
7,guan_diagnose_2018,"Guan, Qingji; Huang, Yaping; Zhong, Zhun; Zheng, Zhedong; Zheng, Liang; Yang, Yi",Diagnose like a Radiologist: Attention Guided Convolutional Neural Network for Thorax Disease Classification,arXiv:1801.09927 [cs],January,2018,http://arxiv.org/abs/1801.09927,"This paper considers the task of thorax disease classiï¬Åcation on chest X-ray images. Existing methods generally use the global image as input for network learning. Such a strategy is limited in two aspects. 1) A thorax disease usually happens in (small) localized areas which are disease speciï¬Åc. Training CNNs using global image may be affected by the (excessive) irrelevant noisy areas. 2) Due to the poor alignment of some CXR images, the existence of irregular borders hinders the network performance. In this paper, we address the above problems by proposing a three-branch attention guided convolution neural network (AG-CNN). AG-CNN 1) learns from disease-speciï¬Åc regions to avoid noise and improve alignment, 2) also integrates a global branch to compensate the lost discriminative cues by local branch. Speciï¬Åcally, we ï¬Årst learn a global CNN branch using global images. Then, guided by the attention heat map generated from the global branch, we inference a mask to crop a discriminative region from the global image. The local region is used for training a local CNN branch. Lastly, we concatenate the last pooling layers of both the global and local branches for ï¬Åne-tuning the fusion branch. The comprehensive experiment is conducted on the ChestX-ray14 dataset. We ï¬Årst report a strong global baseline producing an average AUC of 0.841 with ResNet50 as backbone. After combining the local cues with the global information, AG-CNN improves the average AUC to 0.868. While DenseNet-121 is used, the average AUC achieves 0.871, which is a new state of the art in the community.",,Computer Science - Computer Vision and Pattern Recognition,,
7,guo_deep_2016,"Guo, Jiafeng; Fan, Yixing; Ai, Qingyao; Croft, W. Bruce",A Deep Relevance Matching Model for Ad-hoc Retrieval,Proceedings of the 25th ACM International on Conference on Information and Knowledge Management - CIKM '16,,2016,http://arxiv.org/abs/1711.08611,"In recent years, deep neural networks have led to exciting breakthroughs in speech recognition, computer vision, and natural language processing (NLP) tasks. However, there have been few positive results of deep models on ad-hoc retrieval tasks. This is partially due to the fact that many important characteristics of the ad-hoc retrieval task have not been well addressed in deep models yet. Typically, the ad-hoc retrieval task is formalized as a matching problem between two pieces of text in existing work using deep models, and treated equivalent to many NLP tasks such as paraphrase identification, question answering and automatic conversation. However, we argue that the ad-hoc retrieval task is mainly about relevance matching while most NLP matching tasks concern semantic matching, and there are some fundamental differences between these two matching tasks. Successful relevance matching requires proper handling of the exact matching signals, query term importance, and diverse matching requirements. In this paper, we propose a novel deep relevance matching model (DRMM) for ad-hoc retrieval. Specifically, our model employs a joint deep architecture at the query term level for relevance matching. By using matching histogram mapping, a feed forward matching network, and a term gating network, we can effectively deal with the three relevance matching factors mentioned above. Experimental results on two representative benchmark collections show that our model can significantly outperform some well-known retrieval models as well as state-of-the-art deep matching models.",,Computer Science - Information Retrieval,,
7,guo_deep_2019,"Guo, Jiafeng; Fan, Yixing; Pang, Liang; Yang, Liu; Ai, Qingyao; Zamani, Hamed; Wu, Chen; Croft, W. Bruce; Cheng, Xueqi",A Deep Look into Neural Ranking Models for Information Retrieval,arXiv:1903.06902 [cs],March,2019,http://arxiv.org/abs/1903.06902,"Ranking models lie at the heart of research on information retrieval (IR). During the past decades, different techniques have been proposed for constructing ranking models, from traditional heuristic methods, probabilistic methods, to modern machine learning methods. Recently, with the advance of deep learning technology, we have witnessed a growing body of work in applying shallow or deep neural networks to the ranking problem in IR, referred to as neural ranking models in this paper. The power of neural ranking models lies in the ability to learn from the raw text inputs for the ranking problem to avoid many limitations of hand-crafted features. Neural networks have sufficient capacity to model complicated tasks, which is needed to handle the complexity of relevance estimation in ranking. Since there have been a large variety of neural ranking models proposed, we believe it is the right time to summarize the current status, learn from existing methodologies, and gain some insights for future development. In contrast to existing reviews, in this survey, we will take a deep look into the neural ranking models from different dimensions to analyze their underlying assumptions, major design principles, and learning strategies. We compare these models through benchmark tasks to obtain a comprehensive empirical understanding of the existing techniques. We will also discuss what is missing in the current literature and what are the promising and desired future directions.",,Computer Science - Information Retrieval,,
7,guu_realm_nodate,"Guu, Kelvin; Lee, Kenton; Tung, Zora; Pasupat, Panupong; Chang, Ming-Wei",REALM: Retrieval-Augmented Language Model Pre-Training,,,,,"Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring everlarger networks to cover more facts. To capture knowledge in a more modular and interpretable way, we augment language model pretraining with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, ï¬Åne-tuning and inference. For the ï¬Årst time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents.",,,,
7,han_active_2016,"Han, Xu; Kim, Jung-jae; Kwoh, Chee Keong",Active learning for ontological event extraction incorporating named entity recognition and unknown word handling,Journal of Biomedical Semantics,April,2016,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4849099/,"Background Biomedical text mining may target various kinds of valuable information embedded in the literature, but a critical obstacle to the extension of the mining targets is the cost of manual construction of labeled data, which are required for state-of-the-art supervised learning systems. Active learning is to choose the most informative documents for the supervised learning in order to reduce the amount of required manual annotations. Previous works of active learning, however, focused on the tasks of entity recognition and protein-protein interactions, but not on event extraction tasks for multiple event types. They also did not consider the evidence of event participants, which might be a clue for the presence of events in unlabeled documents. Moreover, the confidence scores of events produced by event extraction systems are not reliable for ranking documents in terms of informativity for supervised learning. We here propose a novel committee-based active learning method that supports multi-event extraction tasks and employs a new statistical method for informativity estimation instead of using the confidence scores from event extraction systems.  Methods Our method is based on a committee of two systems as follows: We first employ an event extraction system to filter potential false negatives among unlabeled documents, from which the system does not extract any event. We then develop a statistical method to rank the potential false negatives of unlabeled documents 1) by using a language model that measures the probabilities of the expression of multiple events in documents and 2) by using a named entity recognition system that locates the named entities that can be event arguments (e.g. proteins). The proposed method further deals with unknown words in test data by using word similarity measures. We also apply our active learning method for the task of named entity recognition.  Results and conclusion We evaluate the proposed method against the BioNLP Shared Tasks datasets, and show that our method can achieve better performance than such previous methods as entropy and Gibbs error based methods and a conventional committee-based method. We also show that the incorporation of named entity recognition into the active learning for event extraction and the unknown word handling further improve the active learning method. In addition, the adaptation of the active learning method into named entity recognition tasks also improves the document selection for manual annotation of named entities.",,,,
7,han_unsupervised_2019,"Han, Xiaochuang; Eisenstein, Jacob",Unsupervised Domain Adaptation of Contextualized Embeddings for Sequence Labeling,arXiv:1904.02817 [cs],April,2019,http://arxiv.org/abs/1904.02817,"Contextualized word embeddings such as ELMo and BERT provide a foundation for strong performance across a wide range of natural language processing tasks by pretraining on large corpora of unlabeled text. However, the applicability of this approach is unknown when the target domain varies substantially from the pretraining corpus. We are specifically interested in the scenario in which labeled data is available in only a canonical source domain such as newstext, and the target domain is distinct from both the labeled and pretraining texts. To address this scenario, we propose domain-adaptive fine-tuning, in which the contextualized embeddings are adapted by masked language modeling on text from the target domain. We test this approach on sequence labeling in two challenging domains: Early Modern English and Twitter. Both domains differ substantially from existing pretraining corpora, and domain-adaptive fine-tuning yields substantial improvements over strong BERT baselines, with particularly impressive results on out-of-vocabulary words. We conclude that domain-adaptive fine-tuning offers a simple and effective approach for the unsupervised adaptation of sequence labeling to difficult new domains.",,"Computer Science - Computation and Language, Computer Science - Digital Libraries, Computer Science - Machine Learning",,
7,harzing_microsoft_2017,"Harzing, Anne-Wil; Alakangas, Satu",Microsoft Academic: is the phoenix getting wings?,Scientometrics,January,2017,https://doi.org/10.1007/s11192-016-2185-x,,,"Average Citation Count, Citation Count, High Citation Count, Publication Record, Scopus Citation",,
7,hassanpour_performance_2017,"Hassanpour, Saeed; Langlotz, Curtis P.; Amrhein, Timothy J.; Befera, Nicholas T.; Lungren, Matthew P.",Performance of a Machine Learning Classifier of Knee MRI Reports in Two Large Academic Radiology Practices: A Tool to Estimate Diagnostic Yield,American Journal of Roentgenology,April,2017,http://www.ajronline.org/doi/10.2214/AJR.16.16128,"OBJECTIVE. The purpose of this study is to evaluate the performance of a natural language processing (NLP) system in classifying a database of free-text knee MRI reports at two separate academic radiology practices. MATERIALS AND METHODS. An NLP system that uses terms and patterns in manually classified narrative knee MRI reports was constructed. The NLP system was trained and tested on expert-classified knee MRI reports from two major health care organizations. Radiology reports were modeled in the training set as vectors, and a support vector machine framework was used to train the classifier. A separate test set from each organization was used to evaluate the performance of the system. We evaluated the performance of the system both within and across organizations. Standard evaluation metrics, such as accuracy, precision, recall, and F1 score (i.e., the weighted average of the precision and recall), and their respective 95\% CIs were used to measure the efficacy of our classification system. RESULTS. The accuracy for radiology reports that belonged to the modelâ€™s clinically significant concept classes after training data from the same institution was good, yielding an F1 score greater than 90\% (95\% CI, 84.6â€“97.3\%). Performance of the classifier on cross-institutional application without institution-specific training data yielded F1 scores of 77.6\% (95\% CI, 69.5â€“85.7\%) and 90.2\% (95\% CI, 84.5â€“95.9\%) at the two organizations studied. CONCLUSION. The results show excellent accuracy by the NLP machine learning classifier in classifying free-text knee MRI reports, supporting the institution-independent reproducibility of knee MRI report classification. Furthermore, the machine learning classifier performed well on free-text knee MRI reports from another institution. These data support the feasibility of multiinstitutional classification of radiologic imaging text reports with a single machine learning classifier without requiring institution-specific training data.",,,,
7,haussmann_scalable_2020,"Haussmann, Elmar; Fenzi, Michele; Chitta, Kashyap; Ivanecky, Jan; Xu, Hanson; Roy, Donna; Mittel, Akshita; Koumchatzky, Nicolas; Farabet, Clement; Alvarez, Jose M.",Scalable Active Learning for Object Detection,arXiv:2004.04699 [cs.CV],April,2020,https://arxiv.org/abs/2004.04699,"Deep Neural Networks trained in a fully supervised fashion are the dominant technology in perception-based autonomous driving systems. While collecting large amounts of unlabeled data is already a major undertaking, only a subset of it can be labeled by humans due to the effort needed for high-quality annotation. Therefore, finding the right data to label has become a key challenge. Active learning is a powerful technique to improve data efficiency for supervised learning methods, as it aims at selecting the smallest possible training set to reach a required performance. We have built a scalable production system for active learning in the domain of autonomous driving. In this paper, we describe the resulting high-level design, sketch some of the challenges and their solutions, present our current results at scale, and briefly describe the open problems and future directions.",,,,
7,he_paperpoles_2019,"He, Jiangen; Ping, Qing; Lou, Wen; Chen, Chaomei",PaperPoles: Facilitating adaptive visual exploration of scientific publications by citation links,Journal of the Association for Information Science and Technology,August,2019,https://onlinelibrary.wiley.com/doi/abs/10.1002/asi.24171,"Finding relevant publications is a common task. Typically, a researcher browses through a list of publications and traces additional relevant publications. When relevant publications are identiï¬Åed, the list may be expanded by the citation links of the relevant publications. The information needs of researchers may change as they go through such iterative processes. The exploration process quickly becomes cumbersome as the list expands. Most existing academic search systems tend to be limited in terms of the extent to which searchers can adapt their search as they proceed. In this paper, we introduce an adaptive visual exploration system named PaperPoles to support exploration of scientiï¬Åc publications in a context-aware environment. Searchers can express their information needs by intuitively formulating positive and negative queries. Search results are grouped and displayed in a cluster view, which shows aspects and relevance patterns of the results to support navigation and exploration. We conducted an experiment to compare PaperPoles with a list-based interface in performing two academic search tasks with different complexity. The results show that PaperPoles can improve the accuracy of searching for the simple and complex tasks. It can also reduce completion time of searching and exploration effectiveness in the complex task. PaperPoles demonstrates a potentially effective workï¬‚ow for adaptive visual search of complex information.",,,,
7,hebert_bert_nodate,"Hebert, Liam",BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding,,,,,,,,,
7,heilbrun_feasibility_2019,"Heilbrun, Marta E.; Chapman, Brian E.; Narasimhan, Evan; Patel, Neel; Mowery, Danielle",Feasibility of Natural Language Processingâ€“Assisted Auditing of Critical Findings in Chest Radiology,Journal of the American College of Radiology,September,2019,https://linkinghub.elsevier.com/retrieve/pii/S1546144019306386,"Objective: Time-sensitive communication of critical imaging ï¬Åndings like pneumothorax or pulmonary embolism to referring physicians is essential for patient safety. The deï¬Ånitive communication is the radiology free-text report. Quality assurance initiatives require that institutions audit these communications, a time-intensive manual task. We propose using a rule-based natural language processing system to improve the process for auditing critical ï¬Åndings communications.",,,,
7,heimerl_interactive_2018,"Heimerl, F.; Gleicher, M.",Interactive Analysis of Word Vector Embeddings,Computer Graphics Forum,,2018,https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13417,"Word vector embeddings are an emerging tool for natural language processing. They have proven beneficial for a wide variety of language processing tasks. Their utility stems from the ability to encode word relationships within the vector space. Applications range from components in natural language processing systems to tools for linguistic analysis in the study of language and literature. In many of these applications, interpreting embeddings and understanding the encoded grammatical and semantic relations between words is useful, but challenging. Visualization can aid in such interpretation of embeddings. In this paper, we examine the role for visualization in working with word vector embeddings. We provide a literature survey to catalogue the range of tasks where the embeddings are employed across a broad range of applications. Based on this survey, we identify key tasks and their characteristics. Then, we present visual interactive designs that address many of these tasks. The designs integrate into an exploration and analysis environment for embeddings. Finally, we provide example use cases for them and discuss domain user feedback.",,"CCS Concepts, Visual Analytics, â€¢Artificial Intelligence â†’ Natural Language Processing, â€¢Visualization â†’ Information Visualization",,
7,hemmer_deal_nodate,"Hemmer, Patrick; KÃ_hl, Niklas; SchÃ¶ffer, Jakob",DEAL: Deep Evidential Active Learning for Image Classification,arXiv:2007.11344 [cs.LG],,,https://arxiv.org/abs/2007.11344,"Convolutional Neural Networks (CNNs) have proven to be state-of-the-art models for supervised computer vision tasks, such as image classification. However, large labeled data sets are generally needed for the training and validation of such models. In many domains, unlabeled data is available but labeling is expensive, for instance when specific expert knowledge is required. Active Learning (AL) is one approach to mitigate the problem of limited labeled data. Through selecting the most informative and representative data instances for labeling, AL can contribute to more efficient learning of the model. Recent AL methods for CNNs propose different solutions for the selection of instances to be labeled. However, they do not perform consistently well and are often computationally expensive. In this paper, we propose a novel AL algorithm that efficiently learns from unlabeled data by capturing high prediction uncertainty. By replacing the softmax standard output of a CNN with the parameters of a Dirichlet density, the model learns to identify data instances that contribute efficiently to improving model performance during training. We demonstrate in several experiments with publicly available data that our method consistently outperforms other state-of-the-art AL approaches. It can be easily implemented and does not require extensive computational resources for training. Additionally, we are able to show the benefits of the approach on a real-world medical use case in the field of automated detection of visual signals for pneumonia on chest radiographs.",,,,
7,henry_vector_2018,"Henry, Sam; Cuffy, Clint; McInnes, Bridget T.",Vector representations of multi-word terms for semantic relatedness,Journal of Biomedical Informatics,January,2018,http://www.sciencedirect.com/science/article/pii/S1532046417302769,"This paper presents a comparison between several multi-word term aggregation methods of distributional context vectors applied to the task of semantic similarity and relatedness in the biomedical domain. We compare the multi-word term aggregation methods of summation of component word vectors, mean of component word vectors, direct construction of compound term vectors using the compoundify tool, and direct construction of concept vectors using the MetaMap tool. Dimensionality reduction is critical when constructing high quality distributional context vectors, so these baseline co-occurrence vectors are compared against dimensionality reduced vectors created using singular value decomposition (SVD), and word2vec word embeddings using continuous bag of words (CBOW), and skip-gram models. We also find optimal vector dimensionalities for the vectors produced by these techniques. Our results show that none of the tested multi-word term aggregation methods is statistically significantly better than any other. This allows flexibility when choosing a multi-word term aggregation method, and means expensive corpora preprocessing may be avoided. Results are shown with several standard evaluation datasets, and state of the results are achieved.",,"Distributional similarity, Natural language processing, Semantic similarity and relatedness",,
7,hermann_teaching_2015,"Hermann, Karl Moritz; KoÄçiskÃ_, TomÃ¡Å¡; Grefenstette, Edward; Espeholt, Lasse; Kay, Will; Suleyman, Mustafa; Blunsom, Phil",Teaching Machines to Read and Comprehend,arXiv:1506.03340 [cs],June,2015,http://arxiv.org/abs/1506.03340,"Teaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure.",,"Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing",,
7,hinton_distilling_2015,"Hinton, Geoffrey; Vinyals, Oriol; Dean, Jeff",Distilling the Knowledge in a Neural Network,"arXiv:1503.02531 [cs, stat]",March,2015,http://arxiv.org/abs/1503.02531,"A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.",,"Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning",,
7,ho_deep_2020,"Ho, David Joon; Agaram, Narasimhan P.; SchÃ_ffler, Peter J.; Vanderbilt, Chad M.; Jean, Marc-Henri; Hameed, Meera R.; Fuchs, Thomas J.",Deep Interactive Learning: An Eficient Labeling Approach for Deep Learning-Based Osteosarcoma Treatment Response Assessment,arXiv:2007.01383 [eess.IV],July,2020,https://arxiv.org/abs/2007.01383,"Osteosarcoma is the most common malignant primary bone tumor. Standard treatment includes pre-operative chemotherapy followed by surgical resection. The response to treatment as measured by ratio of necrotic tumor area to overall tumor area is a known prognostic factor for overall survival. This assessment is currently done manually by pathologists by looking at glass slides under the microscope which may not be reproducible due to its subjective nature. Convolutional neural networks (CNNs) can be used for automated segmentation of viable and necrotic tumor on osteosarcoma whole slide images. One bottleneck for supervised learning is that large amounts of accurate annotations are required for training which is a time-consuming and expensive process. In this paper, we describe Deep Interactive Learning (DIaL) as an efficient labeling approach for training CNNs. After an initial labeling step is done, annotators only need to correct mislabeled regions from previous segmentation predictions to improve the CNN model until the satisfactory predictions are achieved. Our experiments show that our CNN model trained by only 7 hours of annotation using DIaL can successfully estimate ratios of necrosis within expected inter-observer variation rate for non-standardized manual surgical pathology task.",,,,
7,hohman_visual_2018,"Hohman, Fred; Kahng, Minsuk; Pienta, Robert; Chau, Duen Horng",Visual Analytics in Deep Learning: An Interrogative Survey for the Next Frontiers,"arXiv:1801.06889 [cs, stat]",January,2018,http://arxiv.org/abs/1801.06889,"Deep learning has recently seen rapid development and received significant attention due to its state-of-the-art performance on previously-thought hard problems. However, because of the internal complexity and nonlinear structure of deep neural networks, the underlying decision making processes for why these models are achieving such performance are challenging and sometimes mystifying to interpret. As deep learning spreads across domains, it is of paramount importance that we equip users of deep learning with tools for understanding when a model works correctly, when it fails, and ultimately how to improve its performance. Standardized toolkits for building neural networks have helped democratize deep learning; visual analytics systems have now been developed to support model explanation, interpretation, debugging, and improvement. We present a survey of the role of visual analytics in deep learning research, which highlights its short yet impactful history and thoroughly summarizes the state-of-the-art using a human-centered interrogative framework, focusing on the Five W's and How (Why, Who, What, How, When, and Where). We conclude by highlighting research directions and open research problems. This survey helps researchers and practitioners in both visual analytics and deep learning to quickly learn key aspects of this young and rapidly growing body of research, whose impact spans a diverse range of domains.",,"Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning, H.5.2, I.2.6.g, I.5.1.d, I.6.9.c, I.6.9.f, Statistics - Machine Learning",,
7,holzinger_interactive_2016,"Holzinger, Andreas",Interactive machine learning for health informatics when do we need the human-in-the-loop,Springer,March,2016,https://link.springer.com/article/10.1007/s40708-016-0042-6,"Machine learning (ML) is the fastest growing field in computer science, and health informatics is among the greatest challenges. The goal of ML is to develop algorithms which can learn and improve over time and can be used for predictions. Most ML researchers concentrate on automatic machine learning (aML), where great advances have been made, for example, in speech recognition, recommender systems, or autonomous vehicles. Automatic approaches greatly benefit from big data with many training sets. However, in the health domain, sometimes we are confronted with a small number of data sets or rare events, where aML-approaches suffer of insufficient training samples. Here interactive machine learning (iML) may be of help, having its roots in reinforcement learning, preference learning, and active learning. The term iML is not yet well used, so we define it as â€œalgorithms that can interact with agents and can optimize their learning behavior through these interactions, where the agents can also be human.â€ù This â€œhuman-in-the-loopâ€ù can be beneficial in solving computationally hard problems, e.g., subspace clustering, protein folding, or k-anonymization of health data, where human expertise can help to reduce an exponential search space through heuristic selection of samples. Therefore, what would otherwise be an NP-hard problem, reduces greatly in complexity through the input and the assistance of a human agent involved in the learning phase.",,,,
7,hoover_exbert_2019,"Hoover, Benjamin; Strobelt, Hendrik; Gehrmann, Sebastian",exBERT: A Visual Analysis Tool to Explore Learned Representations in Transformers Models,arXiv:1910.05276 [cs],October,2019,http://arxiv.org/abs/1910.05276,"Large language models can produce powerful contextual representations that lead to improvements across many NLP tasks. Since these models are typically guided by a sequence of learned self attention mechanisms and may comprise undesired inductive biases, it is paramount to be able to explore what the attention has learned. While static analyses of these models lead to targeted insights, interactive tools are more dynamic and can help humans better gain an intuition for the model-internal reasoning process. We present exBERT, an interactive tool named after the popular BERT language model, that provides insights into the meaning of the contextual representations by matching a human-specified input to similar contexts in a large annotated dataset. By aggregating the annotations of the matching similar contexts, exBERT helps intuitively explain what each attention-head has learned.",,"Computer Science - Computation and Language, Computer Science - Machine Learning",,
7,hoover_exbert:_2019,"Hoover, Benjamin; Strobelt, Hendrik; Gehrmann, Sebastian",exBERT: A Visual Analysis Tool to Explore Learned Representations in Transformers Models,arXiv:1910.05276 [cs],October,2019,http://arxiv.org/abs/1910.05276,"Large language models can produce powerful contextual representations that lead to improvements across many NLP tasks. Since these models are typically guided by a sequence of learned self attention mechanisms and may comprise undesired inductive biases, it is paramount to be able to explore what the attention has learned. While static analyses of these models lead to targeted insights, interactive tools are more dynamic and can help humans better gain an intuition for the model-internal reasoning process. We present exBERT, an interactive tool named after the popular BERT language model, that provides insights into the meaning of the contextual representations by matching a human-specified input to similar contexts in a large annotated dataset. By aggregating the annotations of the matching similar contexts, exBERT helps intuitively explain what each attention-head has learned.",,"Computer Science - Computation and Language, Computer Science - Machine Learning",,
7,hoque_interactive_2016,"Hoque, Enamul; Carenini, Giuseppe",Interactive Topic Modeling for Exploring Asynchronous Online Conversations: Design and Evaluation of ConVisIT,ACM Trans. Interact. Intell. Syst.,February,2016,http://doi.acm.org/10.1145/2854158,"Since the mid-2000s, there has been exponential growth of asynchronous online conversations, thanks to the rise of social media. Analyzing and gaining insights from such conversations can be quite challenging for a user, especially when the discussion becomes very long. A promising solution to this problem is topic modeling, since it may help the user to understand quickly what was discussed in a long conversation and to explore the comments of interest. However, the results of topic modeling can be noisy, and they may not match the userâ€™s current information needs. To address this problem, we propose a novel topic modeling system for asynchronous conversations that revises the model on the fly on the basis of usersâ€™ feedback. We then integrate this system with interactive visualization techniques to support the user in exploring long conversations, as well as in revising the topic model when the current results are not adequate to fulfill the userâ€™s information needs. Finally, we report on an evaluation with real users that compared the resulting system with both a traditional interface and an interactive visual interface that does not support human-in-the-loop topic modeling. Both the quantitative results and the subjective feedback from the participants illustrate the potential benefits of our interactive topic modeling approach for exploring conversations, relative to its counterparts.",,"Interactive topic modeling, asynchronous conversation, computer mediated communication, text visualization",,
7,houlsby_bayesian_2011,"Houlsby, Neil; HuszÃ¡r, Ferenc; Ghahramani, Zoubin; Lengyel, MÃ¡tÃ©",Bayesian Active Learning for Classification and Preference Learning,arXiv:1112.5745 [stat.ML],December,2011,https://arxiv.org/abs/1112.5745,"Information theoretic active learning has been widely studied for probabilistic models. For simple regression an optimal myopic policy is easily tractable. However, for other tasks and with more complex models, such as classification with nonparametric models, the optimal solution is harder to compute. Current approaches make approximations to achieve tractability. We propose an approach that expresses information gain in terms of predictive entropies, and apply this method to the Gaussian Process Classifier (GPC). Our approach makes minimal approximations to the full information theoretic objective. Our experimental performance compares favourably to many popular active learning algorithms, and has equal or lower computational complexity. We compare well to decision theoretic approaches also, which are privy to more information and require much more computational time. Secondly, by developing further a reformulation of binary preference learning to a classification problem, we extend our algorithm to Gaussian Process preference learning.",,,,
7,huang_clinicalbert_2019,"Huang, Kexin; Altosaar, Jaan; Ranganath, Rajesh",ClinicalBERT: Modeling Clinical Notes and Predicting Hospital Readmission,arXiv:1904.05342 [cs],April,2019,http://arxiv.org/abs/1904.05342,"Clinical notes contain information about patients that goes beyond structured data like lab values and medications. However, clinical notes have been underused relative to structured data, because notes are high-dimensional and sparse. This work develops and evaluates representations of clinical notes using bidirectional transformers (ClinicalBERT). ClinicalBERT uncovers high-quality relationships between medical concepts as judged by humans. ClinicalBert outperforms baselines on 30-day hospital readmission prediction using both discharge summaries and the first few days of notes in the intensive care unit. Code and model parameters are available.",,"Biomedical Text, Computer Science - Computation and Language, Computer Science - Machine Learning",,
7,huang_empirical_2019,"Huang, Jinmiao; Osorio, Cesar; Sy, Luke Wicent",An empirical evaluation of deep learning for ICD-9 code assignment using MIMIC-III clinical notes,Computer Methods and Programs in Biomedicine,August,2019,https://doi.org/10.1016/j.cmpb.2019.05.024,"Background and Objective Code assignment is of paramount importance in many levels in modern hospitals, from ensuring accurate billing process to creating a valid record of patient care history. However, the coding process is tedious and subjective, and it requires medical coders with extensive training. This study aims to evaluate the performance of deep-learning-based systems to automatically map clinical notes to ICD-9 medical codes.  Methods The evaluations of this research are focused on end-to-end learning methods without manually defined rules. Traditional machine learning algorithms, as well as state-of-the-art deep learning methods such as Recurrent Neural Networks and Convolution Neural Networks, were applied to the Medical Information Mart for Intensive Care (MIMIC-III) dataset. An extensive number of experiments was applied to different settings of the tested algorithm.  Results Findings showed that the deep learning-based methods outperformed other conventional machine learning methods. From our assessment, the best models could predict the top 10 ICD-9 codes with 0.6957 F1 and 0.8967 accuracy and could estimate the top 10 ICD-9 categories with 0.7233 F1 and 0.8588 accuracy. Our implementation also outperformed existing work under certain evaluation metrics.  Conclusion A set of standard metrics was utilized in assessing the performance of ICD-9 code assignment on MIMIC-III dataset. All the developed evaluation tools and resources are available online, which can be used as a baseline for further research.",,,,
7,hug_citation_2017,"Hug, Sven E.; Ochsner, Michael; BrÃ_ndle, Martin P.",Citation analysis with microsoft academic,Scientometrics,April,2017,https://doi.org/10.1007/s11192-017-2247-8,,,"Citation analysis, Google Scholar, Microsoft Academic, Normalization, Percentiles, Scopus",,
7,hug_coverage_2017,"Hug, Sven E.; BrÃ_ndle, Martin P.",The coverage of Microsoft Academic: analyzing the publication output of a university,Scientometrics,December,2017,https://doi.org/10.1007/s11192-017-2535-3,,,"Citation analysis, Coverage, EPrints, Microsoft Academic, Publication language, Research fields, Scopus, Web of Science",,
7,irvin_chexpert_2019,"Irvin, Jeremy; Rajpurkar, Pranav; Ko, Michael; Yu, Yifan; Ciurea-Ilcus, Silviana; Chute, Chris; Marklund, Henrik; Haghgoo, Behzad; Ball, Robyn; Shpanskaya, Katie; Seekins, Jayne; Mong, David A.; Halabi, Safwan S.; Sandberg, Jesse K.; Jones, Ricky; Larson, David B.; Langlotz, Curtis P.; Patel, Bhavik N.; Lungren, Matthew P.; Ng, Andrew Y.",CheXpert: A Large Chest Radiograph Dataset with Uncertainty Labels and Expert Comparison,Proceedings of the AAAI Conference on Artificial Intelligence,July,2019,https://aaai.org/ojs/index.php/AAAI/article/view/3834,"Large, labeled datasets have driven deep learning methods to achieve expert-level performance on a variety of medical imaging tasks. We present CheXpert, a large dataset that contains 224,316 chest radiographs of 65,240 patients. We design a labeler to automatically detect the presence of 14 observations in radiology reports, capturing uncertainties inherent in radiograph interpretation. We investigate different approaches to using the uncertainty labels for training convolutional neural networks that output the probability of these observations given the available frontal and lateral radiographs. On a validation set of 200 chest radiographic studies which were manually annotated by 3 board-certified radiologists, we find that different uncertainty approaches are useful for different pathologies. We then evaluate our best model on a test set composed of 500 chest radiographic studies annotated by a consensus of 5 board-certified radiologists, and compare the performance of our model to that of 3 additional radiologists in the detection of 5 selected pathologies. On Cardiomegaly, Edema, and Pleural Effusion, the model ROC and PR curves lie above all 3 radiologist operating points. We release the dataset to the public as a standard benchmark to evaluate performance of chest radiograph interpretation models.",,,,
7,isenberg_toward_2017,"Isenberg, Petra; Isenberg, Tobias; Sedlmair, Michael; Chen, Jian; MÃ¶ller, Torsten",Toward a deeper understanding of Visualization through keyword analysis,IEEE Transactions on Visualization and Computer Graphics,January,2017,http://arxiv.org/abs/1408.3297,"We present the results of a comprehensive analysis of visualization paper keywords supplied for 4366 papers submitted to five main visualization conferences. We describe main keywords, topic areas, and 10-year historic trends from two datasets: (1) the standardized PCS taxonomy keywords in use for paper submissions for IEEE InfoVis, IEEE Vis-SciVis, IEEE VAST, EuroVis, and IEEE PacificVis since 2009 and (2) the author-chosen keywords for papers published in the IEEE Visualization conference series (now called IEEE VIS) since 2004. Our analysis of research topics in visualization can serve as a starting point to (a) help create a common vocabulary to improve communication among different visualization sub-groups, (b) facilitate the process of understanding differences and commonalities of the various research sub-fields in visualization, (c) provide an understanding of emerging new research trends, (d) facilitate the crucial step of finding the right reviewers for research submissions, and (e) it can eventually lead to a comprehensive taxonomy of visualization research. One additional tangible outcome of our work is an application that allows visualization researchers to easily browse the 2600+ keywords used for IEEE VIS papers during the past 10 years, aiming at more informed and, hence, more effective keyword selections for future visualization publications.",,Computer Science - Digital Libraries,,
7,iwana_explaining_2019,"Iwana, Brian Kenji; Kuroki, Ryohei; Uchida, Seiichi",Explaining Convolutional Neural Networks using Softmax Gradient Layer-wise Relevance Propagation,arXiv:1908.04351 [cs],November,2019,http://arxiv.org/abs/1908.04351,"Convolutional Neural Networks (CNN) have become state-of-the-art in the field of image classification. However, not everything is understood about their inner representations. This paper tackles the interpretability and explainability of the predictions of CNNs for multi-class classification problems. Specifically, we propose a novel visualization method of pixel-wise input attribution called Softmax-Gradient Layer-wise Relevance Propagation (SGLRP). The proposed model is a class discriminate extension to Deep Taylor Decomposition (DTD) using the gradient of softmax to back propagate the relevance of the output probability to the input image. Through qualitative and quantitative analysis, we demonstrate that SGLRP can successfully localize and attribute the regions on input images which contribute to a target object's classification. We show that the proposed method excels at discriminating the target objects class from the other possible objects in the images. We confirm that SGLRP performs better than existing Layer-wise Relevance Propagation (LRP) based methods and can help in the understanding of the decision process of CNNs.",,"Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing",,
7,jabeen_cprel:_2013,"Jabeen, Shahida; Gao, Xiaoying; Andreae, Peter",CPRel: Semantic Relatedness Computation Using Wikipedia based Context Profiles,Research in Computing Science,,2013,,"Semantic relatedness is a well known problem with its significance ranging from computational linguistics to Natural language Processing applications. Relatedness computation is restricted by the amount of common sense and background knowledge required to relate any two terms. This paper proposes a novel model of relatedness using context profile built on features extracted from encyclopedic knowledge. Proposed research makes use of Wikipedia to represent the context of a word in the high dimensional space of Wikipedia labels. Semantic relatedness of a word pair is then assessed by comparing their corresponding context profiles based on three different weighting schemes using traditional Cosine similarity metrics. To evaluate proposed relatedness approach, three well known benchmark datasets are used and it is shown that Wikipedia article contents can be used effectively to compute term relatedness. The experiments demonstrate that the proposed approach is computationally cheap as well as effective when correlated with human judgments.",,"Benchmark (computing), Computation, Computational linguistics, Context filtering, Cosine similarity, Experiment, General Instrument AY-3-8910, National Transfer Format, Natural language processing, Semantic similarity, Wikipedia",,
7,jacobi_quantitative_2016,"Jacobi, Carina; Atteveldt, Wouter van; Welbers, Kasper",Quantitative analysis of large amounts of journalistic texts using topic modelling,Digital Journalism,January,2016,https://doi.org/10.1080/21670811.2015.1093271,,,"Corrigendum, automatic content analysis, journalism, nuclear energy, topic models",,
7,jaeger_two_nodate,"Jaeger, Stefan; Candemir, Sema; Antani, Sameer; WÃ¡ng, YÃ¬-XiÃ¡ng J.; Lu, Pu-Xuan; Thoma, George",Two public chest X-ray datasets for computer-aided screening of pulmonary diseases,,,,,"The U.S. National Library of Medicine has made two datasets of postero-anterior (PA) chest radiographs available to foster research in computer-aided diagnosis of pulmonary diseases with a special focus on pulmonary tuberculosis (TB). The radiographs were acquired from the Department of Health and Human Services, Montgomery County, Maryland, USA and Shenzhen No. 3 Peopleâ€™s Hospital in China. Both datasets contain normal and abnormal chest X-rays with manifestations of TB and include associated radiologist readings.",,,,
7,jain_information_2020,"Jain, Siddhartha; Liu, Ge; Gifford, David",Information Condensing Active Learning,arXiv:2002.07916 [cs.LG],February,2020,https://arxiv.org/abs/2002.07916,"We introduce Information Condensing Active Learning (ICAL), a batch mode model agnostic Active Learning (AL) method targeted at Deep Bayesian Active Learning that focuses on acquiring labels for points which have as much information as possible about the still unacquired points. ICAL uses the Hilbert Schmidt Independence Criterion (HSIC) to measure the strength of the dependency between a candidate batch of points and the unlabeled set. We develop key optimizations that allow us to scale our method to large unlabeled sets. We show significant improvements in terms of model accuracy and negative log likelihood (NLL) on several image datasets compared to state of the art batch mode AL methods for deep learning.",,,,
7,ji_cost-sensitive_2019,"Ji, Zongcheng; Wei, Qiang; Franklin, Amy; Cohen, Trevor; Xu, Hua",Cost-sensitive Active Learning for Phenotyping of Electronic Health Records,AMIA Summits on Translational Science Proceedings,June,2019,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6568101/,"Developing high-throughput and high-performance phenotyping algorithms is critical to the secondary use of electronic health records for clinical research. Supervised machine learning-based methods have shown good performance, but often require large annotated datasets that are costly to build. Simulation studies have shown that active learning (AL) could reduce the number of annotated samples while improving the model performance when assuming that the time of labeling each sample is the same (i.e., cost-insensitive). In this study, we proposed a cost- sensitive AL (CostAL) algorithm for clinical phenotyping, using the identification of breast cancer patients as a use case. CostAL implements a linear regression model to estimate the actual time required for annotating each individual sample. We recruited two annotators to manual review medical records of 766 potential breast cancer patients and recorded the actual time of annotating each sample. We then compared CostAL, AL, and passive learning (PL, aka random sampling) using this annotated dataset and generated learning curves for each method. Our experimental results showed that CostAL achieved the highest area under the curve (AUC) score among the three algorithms (PL, AL, and CostAL are 0.784, 0.8501, and 0.8673 for user 1 and 0.8006, 0.8806 and 0.9006 for user 2). To achieve an accuracy of 0.94, AL and CostAL could save 36\% and 60\% annotation time for user 1 and 53\% and 70\% annotation time for user 2, when they were compared with PL, indicating the value of cost-sensitive AL approaches.",,,,
7,ji_dilated_2020,"Ji, Shaoxiong; Cambria, Erik; Marttinen, Pekka",Dilated Convolutional Attention Network for Medical Code Assignment from Clinical Text,arXiv:2009.14578 [cs],September,2020,http://arxiv.org/abs/2009.14578,"Medical code assignment, which predicts medical codes from clinical texts, is a fundamental task of intelligent medical information systems. The emergence of deep models in natural language processing has boosted the development of automatic assignment methods. However, recent advanced neural architectures with flat convolutions or multi-channel feature concatenation ignore the sequential causal constraint within a text sequence and may not learn meaningful clinical text representations, especially for lengthy clinical notes with long-term sequential dependency. This paper proposes a Dilated Convolutional Attention Network (DCAN), integrating dilated convolutions, residual connections, and label attention, for medical code assignment. It adopts dilated convolutions to capture complex medical patterns with a receptive field which increases exponentially with dilation size. Experiments on a real-world clinical dataset empirically show that our model improves the state of the art.",,"Computer Science - Computation and Language, Computer Science - Information Retrieval",,
7,ji_visual_2019,"Ji, X.; Shen, H.; Ritter, A.; Machiraju, R.; Yen, P.",Visual Exploration of Neural Document Embedding in Information Retrieval: Semantics and Feature Selection,IEEE Transactions on Visualization and Computer Graphics,June,2019,,"Neural embeddings are widely used in language modeling and feature generation with superior computational power. Particularly, neural document embedding - converting texts of variable-length to semantic vector representations - has shown to benefit widespread downstream applications, e.g., information retrieval (IR). However, the black-box nature makes it difficult to understand how the semantics are encoded and employed. We propose visual exploration of neural document embedding to gain insights into the underlying embedding space, and promote the utilization in prevalent IR applications. In this study, we take an IR application-driven view, which is further motivated by biomedical IR in healthcare decision-making, and collaborate with domain experts to design and develop a visual analytics system. This system visualizes neural document embeddings as a configurable document map and enables guidance and reasoning; facilitates to explore the neural embedding space and identify salient neural dimensions (semantic features) per task and domain interest; and supports advisable feature selection (semantic analysis) along with instant visual feedback to promote IR performance. We demonstrate the usefulness and effectiveness of this system and present inspiring findings in use cases. This work will help designers/developers of downstream applications gain insights and confidence in neural document embedding, and exploit that to achieve more favorable performance in application domains.",,"Analytical models, Feature extraction, IR application-driven view, Medical services, Neural document embedding, Semantics, Task analysis, Visual analytics, biomedical IR, data analysis, data visualisation, feature selection, health care, healthcare decision-making, information retrieval, neural document embedding, neural embedding space, neural nets, salient neural dimensions, semantic analysis, semantic vector representations, text analysis, text conversion, visual analytics system, visual exploration",,
7,ji_wordrank_2016,"Ji, Shihao; Yun, Hyokun; Yanardag, Pinar; Matsushima, Shin; Vishwanathan, S. V. N.",WordRank: Learning Word Embeddings via Robust Ranking,"arXiv:1506.02761 [cs, stat]",September,2016,http://arxiv.org/abs/1506.02761,"Embedding words in a vector space has gained a lot of attention in recent years. While stateof-the-art methods provide efï¬Åcient computation of word similarities via a low-dimensional matrix embedding, their motivation is often left unclear. In this paper, we argue that word embedding can be naturally viewed as a ranking problem due to the ranking nature of the evaluation metrics. Then, based on this insight, we propose a novel framework WordRank that efï¬Åciently estimates word representations via robust ranking, in which the attention mechanism and robustness to noise are readily achieved via the DCG-like ranking losses. The performance of WordRank is measured in word similarity and word analogy benchmarks, and the results are compared to the state-of-the-art word embedding techniques. Our algorithm is very competitive to the state-of-the- arts on large corpora, while outperforms them by a signiï¬Åcant margin when the training set is limited (i.e., sparse and noisy). With 17 million tokens, WordRank performs almost as well as existing methods using 7.2 billion tokens on a popular word similarity benchmark. Our multi-node distributed implementation of WordRank is publicly available for general usage.",,"Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning",,
7,jiang_combining_2019,"Jiang, Min; Sanger, Todd; Liu, Xiong",Combining Contextualized Embeddings and Prior Knowledge for Clinical Named Entity Recognition: Evaluation Study,JMIR Medical Informatics,,2019,https://medinform.jmir.org/2019/4/e14850/,"Background:  Named entity recognition (NER) is a key step in clinical natural language processing (NLP). Traditionally, rule-based systems leverage prior knowledge to define rules to identify named entities. Recently, deep learningâ€“based NER systems have become more and more popular. Contextualized word embedding, as a new type of representation of the word, has been proposed to dynamically capture word sense using context information and has proven successful in many deep learningâ€“based systems in either general domain or medical domain. However, there are very few studies that investigate the effects of combining multiple contextualized embeddings and prior knowledge on the clinical NER task.  Objective:  This study aims to improve the performance of NER in clinical text by combining multiple contextual embeddings and prior knowledge.  Methods:  In this study, we investigate the effects of combining multiple contextualized word embeddings with classic word embedding in deep neural networks to predict named entities in clinical text. We also investigate whether using a semantic lexicon could further improve the performance of the clinical NER system.  Results:  By combining contextualized embeddings such as ELMo and Flair, our system achieves the F-1 score of 87.30\% when only training based on a portion of the 2010 Informatics for Integrating Biology and the Bedside NER task dataset. After incorporating the medical lexicon into the word embedding, the F-1 score was further increased to 87.44\%. Another finding was that our system still could achieve an F-1 score of 85.36\% when the size of the training data was reduced to 40\%.  Conclusions:  Combined contextualized embedding could be beneficial for the clinical NER task. Moreover, the semantic lexicon could be used to further improve the performance of the clinical NER system.  [JMIR Med Inform 2019;7(4):e14850]",,"Alan\_Katz, Biomedical Text",,
7,jiang_familia_2018,"Jiang, Di; Song, Yuanfeng; Lian, Rongzhong; Bao, Siqi; Peng, Jinhua; He, Huang; Wu, Hua",Familia: A Configurable Topic Modeling Framework for Industrial Text Engineering,arXiv:1808.03733 [cs],August,2018,http://arxiv.org/abs/1808.03733,"In the last decade, a variety of topic models have been proposed for text engineering. However, except Probabilistic Latent Semantic Analysis (PLSA) and Latent Dirichlet Allocation (LDA), most of existing topic models are seldom applied or considered in industrial scenarios. This phenomenon is caused by the fact that there are very few convenient tools to support these topic models so far. Intimidated by the demanding expertise and labor of designing and implementing parameter inference algorithms, software engineers are prone to simply resort to PLSA/LDA, without considering whether it is proper for their problem at hand or not. In this paper, we propose a configurable topic modeling framework named Familia, in order to bridge the huge gap between academic research fruits and current industrial practice. Familia supports an important line of topic models that are widely applicable in text engineering scenarios. In order to relieve burdens of software engineers without knowledge of Bayesian networks, Familia is able to conduct automatic parameter inference for a variety of topic models. Simply through changing the data organization of Familia, software engineers are able to easily explore a broad spectrum of existing topic models or even design their own topic models, and find the one that best suits the problem at hand. With its superior extendability, Familia has a novel sampling mechanism that strikes balance between effectiveness and efficiency of parameter inference. Furthermore, Familia is essentially a big topic modeling framework that supports parallel parameter inference and distributed parameter storage. The utilities and necessity of Familia are demonstrated in real-life industrial applications. Familia would significantly enlarge software engineers' arsenal of topic models and pave the way for utilizing highly customized topic models in real-life problems.",,"Computer Science - Computation and Language, Computer Science - Information Retrieval, Computer Science - Machine Learning",,
7,jiao_tinybert_2019,"Jiao, Xiaoqi; Yin, Yichun; Shang, Lifeng; Jiang, Xin; Chen, Xiao; Li, Linlin; Wang, Fang; Liu, Qun",TinyBERT: Distilling BERT for Natural Language Understanding,arXiv:1909.10351 [cs],December,2019,http://arxiv.org/abs/1909.10351,"Language model pre-training, such as BERT, has signiï¬Åcantly improved the performances of many natural language processing tasks. However, the pre-trained language models are usually computationally expensive and memory intensive, so it is difï¬Åcult to effectively execute them on resource-restricted devices. To accelerate inference and reduce model size while maintaining accuracy, we ï¬Årstly propose a novel Transformer distillation method that is specially designed for knowledge distillation (KD) of the Transformer-based models. By leveraging this new KD method, the plenty of knowledge encoded in a large â€œteacherâ€ù BERT can be well transferred to a small â€œstudentâ€ù TinyBERT. Moreover, we introduce a new two-stage learning framework for TinyBERT, which performs Transformer distillation at both the pre-training and task-speciï¬Åc learning stages. This framework ensures that TinyBERT can capture the general-domain as well as the task-speciï¬Åc knowledge in BERT.",,"Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning",,
7,jiao_tinybert:_2019,"Jiao, Xiaoqi; Yin, Yichun; Shang, Lifeng; Jiang, Xin; Chen, Xiao; Li, Linlin; Wang, Fang; Liu, Qun",TinyBERT: Distilling BERT for Natural Language Understanding,arXiv:1909.10351 [cs],September,2019,http://arxiv.org/abs/1909.10351,"Language model pre-training, such as BERT, has significantly improved the performances of many natural language processing tasks. However, pre-trained language models are usually computationally expensive and memory intensive, so it is difficult to effectively execute them on some resource-restricted devices. To accelerate inference and reduce model size while maintaining accuracy, we firstly propose a novel transformer distillation method that is a specially designed knowledge distillation (KD) method for transformer-based models. By leveraging this new KD method, the plenty of knowledge encoded in a large teacher BERT can be well transferred to a small student TinyBERT. Moreover, we introduce a new two-stage learning framework for TinyBERT, which performs transformer distillation at both the pre-training and task-specific learning stages. This framework ensures that TinyBERT can capture both the general-domain and task-specific knowledge of the teacher BERT. TinyBERT is empirically effective and achieves comparable results with BERT in GLUE datasets, while being 7.5x smaller and 9.4x faster on inference. TinyBERT is also significantly better than state-of-the-art baselines, even with only about 28\% parameters and 31\% inference time of baselines.",,"Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning",,
7,jing_automatic_2018,"Jing, Baoyu; Xie, Pengtao; Xing, Eric",On the Automatic Generation of Medical Imaging Reports,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),,2018,http://arxiv.org/abs/1711.08195,"Medical imaging is widely used in clinical practice for diagnosis and treatment. Report-writing can be error-prone for unexperienced physicians, and timeconsuming and tedious for experienced physicians. To address these issues, we study the automatic generation of medical imaging reports. This task presents several challenges. First, a complete report contains multiple heterogeneous forms of information, including ï¬Åndings and tags. Second, abnormal regions in medical images are difï¬Åcult to identify. Third, the reports are typically long, containing multiple sentences. To cope with these challenges, we (1) build a multi-task learning framework which jointly performs the prediction of tags and the generation of paragraphs, (2) propose a co-attention mechanism to localize regions containing abnormalities and generate narrations for them, (3) develop a hierarchical LSTM model to generate long paragraphs. We demonstrate the effectiveness of the proposed methods on two publicly available datasets.",,"Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition",,
7,johnson_mimic-cxr_2019,"Johnson, Alistair E. W.; Pollard, Tom J.; Berkowitz, Seth J.; Greenbaum, Nathaniel R.; Lungren, Matthew P.; Deng, Chih-ying; Mark, Roger G.; Horng, Steven","MIMIC-CXR, a de-identified publicly available database of chest radiographs with free-text reports",Scientific Data,December,2019,http://www.nature.com/articles/s41597-019-0322-0,,,,,
7,johnson_mimic-cxr-jpg_2019,"Johnson, Alistair E. W.; Pollard, Tom J.; Greenbaum, Nathaniel R.; Lungren, Matthew P.; Deng, Chih-ying; Peng, Yifan; Lu, Zhiyong; Mark, Roger G.; Berkowitz, Seth J.; Horng, Steven","MIMIC-CXR-JPG, A LARGE PUBLICLY AVAILABLE DATABASE OF LABELED CHEST RADIOGRAPHS",,,2019,,"Chest radiography is an extremely powerful imaging modality, allowing for a detailed inspection of a patientâ€™s thorax, but requiring specialized training for proper interpretation. With the advent of high performance general purpose computer vision algorithms, the accurate automated analysis of chest radiographs is becoming increasingly of interest to researchers. However, a key challenge in the development of these techniques is the lack of sufï¬Åcient data. Here we describe MIMIC-CXR-JPG v2.0.0, a large dataset of 377,110 chest x-rays associated with 227,827 imaging studies sourced from the Beth Israel Deaconess Medical Center between 2011 - 2016. Images are provided with 14 labels derived from two natural language processing tools applied to the corresponding free-text radiology reports. MIMIC-CXR-JPG is derived entirely from the MIMIC-CXR database, and aims to provide a convenient processed version of MIMIC-CXR, as well as to provide a standard reference for data splits and image labels. All images have been de-identiï¬Åed to protect patient privacy. The dataset is made freely available to facilitate and encourage a wide range of research in medical computer vision.",,read,,
7,johnson_mimic-iii_2016,"Johnson, Alistair E. W.; Pollard, Tom J.; Shen, Lu; Lehman, Li-wei H.; Feng, Mengling; Ghassemi, Mohammad; Moody, Benjamin; Szolovits, Peter; Celi, Leo Anthony; Mark, Roger G.","MIMIC-III, a freely accessible critical care database",Scientific Data,May,2016,https://www.nature.com/articles/sdata201635,"MIMIC-III (â€˜Medical Information Mart for Intensive Careâ€™) is a large, single-center database comprising information relating to patients admitted to critical care units at a large tertiary care hospital. Data includes vital signs, medications, laboratory measurements, observations and notes charted by care providers, fluid balance, procedure codes, diagnostic codes, imaging reports, hospital length of stay, survival data, and more. The database supports applications including academic and industrial research, quality improvement initiatives, and higher education coursework.",,,,
7,joshi_comparison_2019,"Joshi, Aditya; Karimi, Sarvnaz; Sparks, Ross; Paris, CÃ©cile; MacIntyre, C. Raina",A Comparison of Word-based and Context-based Representations for Classification Problems in Health Informatics,arXiv:1906.05468 [cs.CL],June,2019,https://arxiv.org/abs/1906.05468,"Distributed representations of text can be used as features when training a statistical classifier. These representations may be created as a composition of word vectors or as contextbased sentence vectors. We compare the two kinds of representations (word versus context) for three classification problems: influenza infection classification, drug usage classification and personal health mention classification. For statistical classifiers trained for each of these problems, context-based representations based on ELMo, Universal Sentence Encoder, Neural-Net Language Model and FLAIR are better than Word2Vec, GloVe and the two adapted using the MESH ontology. There is an improvement of 2-4\% in the accuracy when these context-based representations are used instead of word-based representations.",,,,
7,jung_less-forgetting_2016,"Jung, Heechul; Ju, Jeongwoo; Jung, Minju; Kim, Junmo",Less-forgetting Learning in Deep Neural Networks,arXiv:1607.00122 [cs],July,2016,http://arxiv.org/abs/1607.00122,"A catastrophic forgetting problem makes deep neural networks forget the previously learned information, when learning data collected in new environments, such as by different sensors or in different light conditions. This paper presents a new method for alleviating the catastrophic forgetting problem. Unlike previous research, our method does not use any information from the source domain. Surprisingly, our method is very effective to forget less of the information in the source domain, and we show the effectiveness of our method using several experiments. Furthermore, we observed that the forgetting problem occurs between mini-batches when performing general training processes using stochastic gradient descent methods, and this problem is one of the factors that degrades generalization performance of the network. We also try to solve this problem using the proposed method. Finally, we show our less-forgetting learning method is also helpful to improve the performance of deep neural networks in terms of recognition rates.",,Computer Science - Machine Learning,,
7,kahle_ggmap:_2013,"Kahle, David; Wickham, Hadley",ggmap: Spatial Visualization with ggplot2,The R Journal,,2013,https://journal.r-project.org/archive/2013/RJ-2013-014/index.html,,,,,
7,kallianos_how_2019,"Kallianos, K.; Mongan, J.; Antani, S.; Henry, T.; Taylor, A.; Abuya, J.; Kohli, M.",How far have we come? Artificial intelligence for chest radiograph interpretation,Clinical Radiology,May,2019,https://linkinghub.elsevier.com/retrieve/pii/S0009926019300194,,,,,
7,katharopoulos_transformers_nodate,"Katharopoulos, Angelos; Vyas, Apoorv; Pappas, Nikolaos; Fleuret, FranÃ§ois",Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention,,,,,"Transformers achieve remarkable performance in several tasks but due to their quadratic complexity, with respect to the inputâ€™s length, they are prohibitively slow for very long sequences. To address this limitation, we express the self-attention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products to reduce the complexity from O N 2 to O (N ), where N is the sequence length. We show that this formulation permits an iterative implementation that dramatically accelerates autoregressive transformers and reveals their relationship to recurrent neural networks. Our linear transformers achieve similar performance to vanilla transformers and they are up to 4000x faster on autoregressive prediction of very long sequences.",,,,
7,khandelwal_sample_2019,"Khandelwal, Urvashi; Clark, Kevin; Jurafsky, Dan; Kaiser, Lukasz",Sample Efficient Text Summarization Using a Single Pre-Trained Transformer,arXiv:1905.08836 [cs],May,2019,http://arxiv.org/abs/1905.08836,"Language model (LM) pre-training has resulted in impressive performance and sample efï¬Åciency on a variety of language understanding tasks. However, it remains unclear how to best use pre-trained LMs for generation tasks such as abstractive summarization, particularly to enhance sample efï¬Åciency. In these sequence-to-sequence settings, prior work has experimented with loading pre-trained weights into the encoder and/or decoder networks, but used non-pre-trained encoder-decoder attention weights. We instead use a pre-trained decoder-only network, where the same Transformer LM both encodes the source and generates the summary. This ensures that all parameters in the network, including those governing attention over source states, have been pre-trained before the ï¬Åne-tuning step. Experiments on the CNN/Daily Mail dataset show that our pre-trained Transformer LM substantially improves over pre-trained Transformer encoder-decoder networks in limited-data settings. For instance, it achieves 13.1 ROUGE2 using only 1\% of the training data (âˆ_3000 examples), while pre-trained encoder-decoder models score 2.3 ROUGE-2.",,Computer Science - Computation and Language,,
7,khattak_survey_2019,"Khattak, Faiza Khan; Jeblee, Serena; Pou-Prom, ChloÃ©; Abdalla, Mohamed; Meaney, Christopher; Rudzicz, Frank",A survey of word embeddings for clinical text,Journal of Biomedical Informatics: X,December,2019,http://www.sciencedirect.com/science/article/pii/S2590177X19300563,"Representing words as numerical vectors based on the contexts in which they appear has become the de facto method of analyzing text with machine learning. In this paper, we provide a guide for training these representations on clinical text data, using a survey of relevant research. Specifically, we discuss different types of word representations, clinical text corpora, available pre-trained clinical word vector embeddings, intrinsic and extrinsic evaluation, applications, and limitations of these approaches. This work can be used as a blueprint for clinicians and healthcare workers who may want to incorporate clinical text features in their own models and applications.",,"Biomedical Text, Clinical data, Natural language processing, Word embeddings",,
7,kholghi_active_2017,"Kholghi, Mahnoosh; Sitbon, Laurianne; Zuccon, Guido; Nguyen, Anthony",Active learning reduces annotation time for clinical concept extraction,International Journal of Medical Informatics,October,2017,https://doi.org/10.1016/j.ijmedinf.2017.08.001,"Objective: To investigate: (1) the annotation time savings by various active learning query strategies compared to supervised learning and a random sampling baseline, and (2) the benefits of active learning-assisted pre-annotations in accelerating the manual annotation process compared to de novo annotation. Materials and methods: There are 73 and 120 discharge summary reports provided by Beth Israel institute in the train and test sets of the concept extraction task in the i2b2/VA 2010 challenge, respectively. The 73 reports were used in user study experiments for manual annotation. First, all sequences within the 73 reports were manually annotated from scratch. Next, active learning models were built to generate pre-annotations for the sequences selected by a query strategy. The annotation/reviewing time per sequence was recorded. The 120 test reports were used to measure the effectiveness of the active learning models. Results: When annotating from scratch, active learning reduced the annotation time up to 35\% and 28\% compared to a fully supervised approach and a random sampling baseline, respectively. Reviewing active learningassisted pre-annotations resulted in 20\% further reduction of the annotation time when compared to de novo annotation. Discussion: The number of concepts that require manual annotation is a good indicator of the annotation time for various active learning approaches as demonstrated by high correlation between time rate and concept annotation rate. Conclusion: Active learning has a key role in reducing the time required to manually annotate domain concepts from clinical free text, either when annotating from scratch or reviewing active learning-assisted pre-annotations.",,,,
7,kholghi_benefits_2016,"Kholghi, Mahnoosh; De Vine, Lance; Sitbon, Laurianne; Zuccon, Guido; Nguyen, Anthony",The Benefits of Word Embeddings Features for Active Learning in Clinical Information Extraction,arXiv:1607.02810 [cs],July,2016,http://arxiv.org/abs/1607.02810,"This study investigates the use of unsupervised word embeddings and sequence features for sample representation in an active learning framework built to extract clinical concepts from clinical free text. The objective is to further reduce the manual annotation effort while achieving higher effectiveness compared to a set of baseline features. Unsupervised features are derived from skip-gram word embeddings and a sequence representation approach. The comparative performance of unsupervised features and baseline hand-crafted features in an active learning framework are investigated using a wide range of selection criteria including least confidence, information diversity, information density and diversity, and domain knowledge informativeness. Two clinical datasets are used for evaluation: the i2b2/VA 2010 NLP challenge and the ShARe/CLEF 2013 eHealth Evaluation Lab. Our results demonstrate significant improvements in terms of effectiveness as well as annotation effort savings across both datasets. Using unsupervised features along with baseline features for sample representation lead to further savings of up to 9\% and 10\% of the token and concept annotation rates, respectively.",,"68U15, 68T50, Computer Science - Computation and Language",,
7,kholghi_clinical_2017,"Kholghi, Mahnoosh; Vine, Lance De; Sitbon, Laurianne; Zuccon, Guido; Nguyen, Anthony",Clinical information extraction using small data: An active learning approach based on sequence representations and word embeddings,Journal of the Association for Information Science and Technology,,2017,https://onlinelibrary.wiley.com/doi/abs/10.1002/asi.23936,"This article demonstrates the benefits of using sequence representations based on word embeddings to inform the seed selection and sample selection processes in an active learning pipeline for clinical information extraction. Seed selection refers to choosing an initial sample set to label to form an initial learning model. Sample selection refers to selecting informative samples to update the model at each iteration of the active learning process. Compared to supervised machine learning approaches, active learning offers the opportunity to build statistical classifiers with a reduced amount of training samples that require manual annotation. Reducing the manual annotation effort can support automating the clinical information extraction process. This is particularly beneficial in the clinical domain, where manual annotation is a time-consuming and costly task, as it requires extensive labor from clinical experts. Our empirical findings demonstrate that (a) using sequence representations along with the length of sequence for seed selection shows potential towards more effective initial models, and (b) using sequence representations for sample selection leads to significantly lower manual annotation efforts, with up to 3\% and 6\% fewer tokens and concepts requiring annotation, respectively, compared to state-of-the-art query strategies.",,,,
7,kim_abstractive_2019,"Kim, Byeongchang; Kim, Hyunwoo; Kim, Gunhee",Abstractive Summarization of Reddit Posts with Multi-level Memory Networks,arXiv:1811.00783 [cs],April,2019,http://arxiv.org/abs/1811.00783,"We address the problem of abstractive summarization in two directions: proposing a novel dataset and a new model. First, we collect Reddit TIFU dataset, consisting of 120K posts from the online discussion forum Reddit. We use such informal crowd-generated posts as text source, in contrast with existing datasets that mostly use formal documents as source such as news articles. Thus, our dataset could less suffer from some biases that key sentences usually locate at the beginning of the text and favorable summary candidates are already inside the text in similar forms. Second, we propose a novel abstractive summarization model named multi-level memory networks (MMN), equipped with multi-level memory to store the information of text from different levels of abstraction. With quantitative evaluation and user studies via Amazon Mechanical Turk, we show the Reddit TIFU dataset is highly abstractive and the MMN outperforms the stateof-the-art summarization models.",,Computer Science - Computation and Language,,
7,kim_topicsifter_2019,"Kim, Hannah; Choi, Dongjin; Drake, Barry; Endert, Alex; Park, Haesun",TopicSifter: Interactive Search Space Reduction Through Targeted Topic Modeling,arXiv:1907.12079 [cs],July,2019,http://arxiv.org/abs/1907.12079,"Topic modeling is commonly used to analyze and understand large document collections. However, in practice, users want to focus on specific aspects or targets"" rather than the entire corpus. For example", given a large collection of documents, users may want only a smaller subset which more closely aligns with their interests, tasks, and domains. In particular
7,kiritchenko_examining_2018,"Kiritchenko, Svetlana; Mohammad, Saif M.",Examining Gender and Race Bias in Two Hundred Sentiment Analysis Systems,arXiv:1805.04508 [cs],May,2018,http://arxiv.org/abs/1805.04508,"Automatic machine learning systems can inadvertently accentuate and perpetuate inappropriate human biases. Past work on examining inappropriate biases has largely focused on just individual systems. Further, there is no benchmark dataset for examining inappropriate biases in systems. Here for the first time, we present the Equity Evaluation Corpus (EEC), which consists of 8,640 English sentences carefully chosen to tease out biases towards certain races and genders. We use the dataset to examine 219 automatic sentiment analysis systems that took part in a recent shared task, SemEval-2018 Task 1 'Affect in Tweets'. We find that several of the systems show statistically significant bias; that is, they consistently provide slightly higher sentiment intensity predictions for one race or one gender. We make the EEC freely available.",,Computer Science - Computation and Language,,
7,kiritchenko_nrc-canada_2018,"Kiritchenko, Svetlana; Mohammad, Saif M.; Morin, Jason; de Bruijn, Berry",NRC-Canada at SMM4H Shared Task: Classifying Tweets Mentioning Adverse Drug Reactions and Medication Intake,arXiv:1805.04558 [cs],May,2018,http://arxiv.org/abs/1805.04558,"Our team, NRC-Canada, participated in two shared tasks at the AMIA-2017 Workshop on Social Media Mining for Health Applications (SMM4H): Task 1 - classification of tweets mentioning adverse drug reactions, and Task 2 - classification of tweets describing personal medication intake. For both tasks, we trained Support Vector Machine classifiers using a variety of surface-form, sentiment, and domain-specific features. With nine teams participating in each task, our submissions ranked first on Task 1 and third on Task 2. Handling considerable class imbalance proved crucial for Task 1. We applied an under-sampling technique to reduce class imbalance (from about 1:10 to 1:2). Standard n-gram features, n-grams generalized over domain terms, as well as general-domain and domain-specific word embeddings had a substantial impact on the overall performance in both tasks. On the other hand, including sentiment lexicon features did not result in any improvement.",,Computer Science - Computation and Language,,
7,kirsch_batchbald_2019,"Kirsch, Andreas; Amersfoort, Joost van; Gal, Yarin",BatchBALD: Efficient and Diverse Batch Acquisition for Deep Bayesian Active Learning,arXiv:1906.08158 [cs.LG],October,2019,https://arxiv.org/abs/1906.08158,"We develop BatchBALD, a tractable approximation to the mutual information between a batch of points and model parameters, which we use as an acquisition function to select multiple informative points jointly for the task of deep Bayesian active learning. BatchBALD is a greedy linear-time 1âˆ’1e-approximate algorithm amenable to dynamic programming and efficient caching. We compare BatchBALD to the commonly used approach for batch data acquisition and find that the current approach acquires similar and redundant points, sometimes performing worse than randomly acquiring data. We finish by showing that, using BatchBALD to consider dependencies within an acquisition batch, we achieve new state of the art performance on standard benchmarks, providing substantial data efficiency improvements in batch acquisition.",,,,
7,koch_siamese_nodate,"Koch, Gregory",Siamese Neural Networks for One-Shot Image Recognition,,,,,,,,,
7,kottke_limitations_2019,"Kottke, Daniel; Schellinger, Jim; Huseljic, Denis; Sick, Bernhard",Limitations of Assessing Active Learning Performance at Runtime,"arXiv:1901.10338 [cs, stat]",January,2019,http://arxiv.org/abs/1901.10338,"Classification algorithms aim to predict an unknown label (e.g., a quality class) for a new instance (e.g., a product). Therefore, training samples (instances and labels) are used to deduct classification hypotheses. Often, it is relatively easy to capture instances but the acquisition of the corresponding labels remain difficult or expensive. Active learning algorithms select the most beneficial instances to be labeled to reduce cost. In research, this labeling procedure is simulated and therefore a ground truth is available. But during deployment, active learning is a one-shot problem and an evaluation set is not available. Hence, it is not possible to reliably estimate the performance of the classification system during learning and it is difficult to decide when the system fulfills the quality requirements (stopping criteria). In this article, we formalize the task and review existing strategies to assess the performance of an actively trained classifier during training. Furthermore, we identified three major challenges: 1){\textasciitilde}to derive a performance distribution, 2){\textasciitilde}to preserve representativeness of the labeled subset, and 3) to correct against sampling bias induced by an intelligent selection strategy. In a qualitative analysis, we evaluate different existing approaches and show that none of them reliably estimates active learning performance stating a major challenge for future research for such systems. All plots and experiments are provided in a Jupyter notebook that is available for download.",,"Computer Science - Machine Learning, Statistics - Machine Learning",,
7,kryscinski_neural_2019,"KryÅ›ciÅ„ski, Wojciech; Keskar, Nitish Shirish; McCann, Bryan; Xiong, Caiming; Socher, Richard",Neural Text Summarization: A Critical Evaluation,arXiv:1908.08960 [cs],August,2019,http://arxiv.org/abs/1908.08960,"Text summarization aims at compressing long documents into a shorter form that conveys the most important parts of the original document. Despite increased interest in the community and notable research effort, progress on benchmark datasets has stagnated. We critically evaluate key ingredients of the current research setup: datasets, evaluation metrics, and models, and highlight three primary shortcomings: 1) automatically collected datasets leave the task underconstrained and may contain noise detrimental to training and evaluation, 2) current evaluation protocol is weakly correlated with human judgment and does not account for important characteristics such as factual correctness, 3) models overï¬Åt to layout biases of current datasets and offer limited diversity in their outputs.",,Computer Science - Computation and Language,,
7,kudo_sentencepiece:_2018,"Kudo, Taku; Richardson, John",SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing,arXiv:1808.06226 [cs],August,2018,http://arxiv.org/abs/1808.06226,"This paper describes SentencePiece, a language-independent subword tokenizer and detokenizer designed for Neural-based text processing, including Neural Machine Translation. It provides open-source C++ and Python implementations for subword units. While existing subword segmentation tools assume that the input is pre-tokenized into word sequences, SentencePiece can train subword models directly from raw sentences, which allows us to make a purely end-to-end and language independent system. We perform a validation experiment of NMT on English-Japanese machine translation, and find that it is possible to achieve comparable accuracy to direct subword training from raw sentences. We also compare the performance of subword training and segmentation with various configurations. SentencePiece is available under the Apache 2 license at https://github.com/google/sentencepiece.",,Computer Science - Computation and Language,,
7,kuhn_using_2018,"Kuhn, Kenneth D.",Using structural topic modeling to identify latent topics and trends in aviation incident reports,Transportation Research Part C: Emerging Technologies,February,2018,http://www.sciencedirect.com/science/article/pii/S0968090X17303881,"The Aviation Safety Reporting System includes over a million confidential reports describing aviation safety incidents. Natural language processing techniques allow for relatively rapid and largely automated analysis of large collections of text data. Interpretation of the results and further investigations by subject matter experts can produce meaningful results. This explains the many commercial and academic applications of natural language processing to aviation safety reports. Relatively few published articles have, however, employed topic modeling, an approach that can identify latent structure within a corpus of documents. Topic modeling is more flexible and relies less on subject matter experts than alternative document categorization and clustering methods. It can, for example, uncover any number of topics hidden in a set of incident reports that have been, or would be, assigned to the same category when using labels and methods applied in earlier research. This article describes the application of structural topic modeling to Aviation Safety Reporting System data. The application identifies known issues. The method also reveals previously unreported connections. Sample results reported here highlight fuel pump, tank, and landing gear issues and the relative insignificance of smoke and fire issues for private aircraft. The results also reveal the prominence of the Quiet Bridge Visual and Tip Toe Visual approach paths at San Francisco International Airport in safety incident reports. These results would, ideally, be verified by subject matter experts before being used to set priorities when planning future safety studies.",,"Aviation, Aviation safety, Aviation safety reporting system, Natural language processing, Text mining, Topic modeling",,
7,kusner_word_nodate,"Kusner, Matt J.; Sun, Yu; Kolkin, Nicholas I.; Weinberger, Kilian Q.",From Word Embeddings To Document Distances,,,,,"We present the Word Moverâ€™s Distance (WMD), a novel distance function between text documents. Our work is based on recent results in word embeddings that learn semantically meaningful representations for words from local cooccurrences in sentences. The WMD distance measures the dissimilarity between two text documents as the minimum amount of distance that the embedded words of one document need to â€œtravelâ€ù to reach the embedded words of another document. We show that this distance metric can be cast as an instance of the Earth Moverâ€™s Distance, a well studied transportation problem for which several highly efï¬Åcient solvers have been developed. Our metric has no hyperparameters and is straight-forward to implement. Further, we demonstrate on eight real world document classiï¬Åcation data sets, in comparison with seven stateof-the-art baselines, that the WMD metric leads to unprecedented low k-nearest neighbor document classiï¬Åcation error rates.",,,,
7,lai_how_2016,"Lai, S.; Liu, K.; He, S.; Zhao, J.",How to Generate a Good Word Embedding,IEEE Intelligent Systems,November,2016,,"The authors analyze three critical components in training word embeddings: model, corpus, and training parameters. They systematize existing neural-network-based word embedding methods and experimentally compare them using the same corpus. They then evaluate each word embedding in three ways: analyzing its semantic properties, using it as a feature for supervised tasks, and using it to initialize neural networks. They also provide several simple guidelines for training good word embeddings.",,"Analytical models, Distributed processing, Embedded systems, Neural networks, Object recognition, Semantics, Training, distributed representation, distributed word representation, intelligent systems, learning (artificial intelligence), natural language processing, neural nets, neural network, word embedding",,
7,lan_albert:_2019,"Lan, Zhenzhong; Chen, Mingda; Goodman, Sebastian; Gimpel, Kevin; Sharma, Piyush; Soricut, Radu",ALBERT: A Lite BERT for Self-supervised Learning of Language Representations,arXiv:1909.11942 [cs],September,2019,http://arxiv.org/abs/1909.11942,"Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations, longer training times, and unexpected model degradation. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having fewer parameters compared to BERT-large.",,"Computer Science - Artificial Intelligence, Computer Science - Computation and Language",,
7,lau_empirical_2016,"Lau, Jey Han; Baldwin, Timothy",An Empirical Evaluation of doc2vec with Practical Insights into Document Embedding Generation,arXiv:1607.05368 [cs],July,2016,http://arxiv.org/abs/1607.05368,"Recently, Le and Mikolov (2014) proposed doc2vec as an extension to word2vec (Mikolov et al., 2013a) to learn document-level embeddings. Despite promising results in the original paper, others have struggled to reproduce those results. This paper presents a rigorous empirical evaluation of doc2vec over two tasks. We compare doc2vec to two baselines and two state-of-the-art document embedding methodologies. We found that doc2vec performs robustly when using models trained on large external corpora, and can be further improved by using pre-trained word embeddings. We also provide recommendations on hyper-parameter settings for general purpose applications, and release source code to induce document embeddings using our trained doc2vec models.",,Computer Science - Computation and Language,,
7,laughlin_visual_2019,"Laughlin, Brandon; Collins, Christopher; Sankaranarayanan, Karthik; El-Khatib, Khalil",A Visual Analytics Framework for Adversarial Text Generation,arXiv:1909.11202 [cs],September,2019,http://arxiv.org/abs/1909.11202,"This paper presents a framework which enables a user to more easily make corrections to adversarial texts. While attack algorithms have been demonstrated to automatically build adversaries, changes made by the algorithms can often have poor semantics or syntax. Our framework is designed to facilitate human intervention by aiding users in making corrections. The framework extends existing attack algorithms to work within an evolutionary attack process paired with a visual analytics loop. Using an interactive dashboard a user is able to review the generation process in real time and receive suggestions from the system for edits to be made. The adversaries can be used to both diagnose robustness issues within a single classiï¬Åer or to compare various classiï¬Åer options. With the weaknesses identiï¬Åed, the framework can also be used as a ï¬Årst step in mitigating adversarial threats. The framework can be used as part of further research into defense methods in which the adversarial examples are used to evaluate new countermeasures. We demonstrate the framework with a word swapping attack for the task of sentiment classiï¬Åcation.",,"Computer Science - Cryptography and Security, Computer Science - Human-Computer Interaction",,
7,lecun_optimal_nodate,"LeCun, Yann; Denker, John S.; Solla, Sara A.",Optimal Brain Damage,,,,,"We have used information-theoretic ideas to derive a class of practical and nearly optimal schemes for adapting the size of a neural network. By removing unimportant weights from a network, several improvements can be expected: better generalization, fewer training examples required, and improved speed of learning and/or classification. The basic idea is to use second-derivative information to make a tradeoff between network complexity and training set error. Experiments confirm the usefulness of the methods on a real-world application.",,,,
7,lee_biobert_2019,"Lee, Jinhyuk; Yoon, Wonjin; Kim, Sungdong; Kim, Donghyeon; Kim, Sunkyu; So, Chan Ho; Kang, Jaewoo",BioBERT: a pre-trained biomedical language representation model for biomedical text mining,Bioinformatics,September,2019,https://academic.oup.com/bioinformatics/advance-article/doi/10.1093/bioinformatics/btz682/5566506,"Motivation: Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows. With the progress in natural language processing (NLP), extracting valuable information from biomedical literature has gained popularity among researchers, and deep learning has boosted the development of effective biomedical text mining models. However, directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora. In this article, we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora.",,Biomedical Text,,
7,lee_biobert:_2019,"Lee, Jinhyuk; Yoon, Wonjin; Kim, Sungdong; Kim, Donghyeon; Kim, Sunkyu; So, Chan Ho; Kang, Jaewoo",BioBERT: a pre-trained biomedical language representation model for biomedical text mining,arXiv:1901.08746 [cs],January,2019,http://arxiv.org/abs/1901.08746,"Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows. With the progress in machine learning, extracting valuable information from biomedical literature has gained popularity among researchers, and deep learning has boosted the development of effective biomedical text mining models. However, as deep learning models require a large amount of training data, applying deep learning to biomedical text mining is often unsuccessful due to the lack of training data in biomedical fields. Recent researches on training contextualized language representation models on text corpora shed light on the possibility of leveraging a large number of unannotated biomedical text corpora. We introduce BioBERT (Bidirectional Encoder Representations from Transformers for Biomedical Text Mining), which is a domain specific language representation model pre-trained on large-scale biomedical corpora. Based on the BERT architecture, BioBERT effectively transfers the knowledge from a large amount of biomedical texts to biomedical text mining models with minimal task-specific architecture modifications. While BERT shows competitive performances with previous state-of-the-art models, BioBERT significantly outperforms them on the following three representative biomedical text mining tasks: biomedical named entity recognition (0.51\% absolute improvement), biomedical relation extraction (3.49\% absolute improvement), and biomedical question answering (9.61\% absolute improvement). We make the pre-trained weights of BioBERT freely available at https://github.com/naver/biobert-pretrained, and the source code for fine-tuning BioBERT available at https://github.com/dmis-lab/biobert.",,Computer Science - Computation and Language,,
7,lee_biobert:_nodate,"Lee, Jinhyuk; Yoon, Wonjin; Kim, Sungdong; Kim, Donghyeon; Kim, Sunkyu; So, Chan Ho; Kang, Jaewoo",BioBERT: a pre-trained biomedical language representation model for biomedical text mining,Bioinformatics,,,https://academic.oup.com/bioinformatics/advance-article/doi/10.1093/bioinformatics/btz682/5566506,AbstractMotivation.  Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows. With the progress in natural,,,,
7,li_abstractive_nodate,"Li, Quanzhi; Zhang, Qiong",Abstractive Event Summarization on Twitter,,,,,"This paper presents a new approach for automatically summarizing a social media event. It utilizes the BERT model as the encoder and a Transformer architecture as the decoder. The framework also includes an event topic prediction component, and the predicted event topic will help the decoder focus more on the specific aspects of the topic category when generating summary. To make the summary more succinct and coherent, the most important messages from an event cluster are selected by a message selection model and encoded by the BERT model. Our preliminary experiment shows that our approach outperforms the baseline methods.",,,,
7,li_automated_2019,"Li, Min; Fei, Zhihui; Wu, Fang-Xiang; Li, Yaohang; Pan, Yi; Wang, Jianxin",Automated ICD-9 Coding via A Deep Learning Approach,IEEE/ACM Transactions on Computational Biology and Bioinformatics,July,2019,https://ieeexplore.ieee.org/document/8320340,"ICD-9 (the Ninth Revision of International Classification of Diseases) is widely used to describe a patient's diagnosis. Accurate automated ICD-9 coding is important because manual coding is expensive, time-consuming, and inefficient. Inspired by the recent successes of deep learning, in this study, we present a deep learning framework called DeepLabeler to automatically assign ICD-9 codes. DeepLabeler combines the convolutional neural network with the `Document to Vector' technique to extract and encode local and global features. Our proposed DeepLabeler demonstrates its effectiveness by achieving state-of-the-art performance, i.e., 0.335 micro F-measure on MIMIC-II dataset and 0.408 micro F-measure on MIMIC-III dataset. It outperforms classical hierarchy-based SVM and flat-SVM both on these two datasets by at least 14 percent. Furthermore, we analyze the deep neural network structure to discover the vital elements in the success of DeepLabeler. We find that the convolutional neural network is the most effective component in our network and the `Document to Vector' technique is also necessary for enhancing classification performance since it extracts well-recognized global features. Extensive experimental results demonstrate that the great promise of deep learning techniques in the field of text multi-label classification and automated medical coding.",,,,
7,li_efficient_2019,"Li, Muqun; Scaiano, Martin; Emam, Khaled El; Malin, Bradley A.",Efficient Active Learning for Electronic Medical Record De-identification,AMIA Summits on Translational Science Proceedings,June,2019,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6568071/,"Electronic medical records are often de-identified before disseminated for secondary uses. However, unstructured natural language records are challenging to de-identify while utilizing a considerable amount of expensive human annotation. In this investigation, we incorporate active learning into the de-identification workflow to reduce annotation requirements. We apply this approach to a real clinical trials dataset and a publicly available i2b2 dataset to illustrate that, when the machine learning de-identification system can actively request information to help create a better model from beyond the system (e.g., a knowledgeable human assistant), less training data will be needed to maintain or improve the performance of trained models in comparison to the typical passive learning framework. Specifically, with a batch size of 10 documents, it requires only 40 documents for an active learning approach to reach an F-measure of 0.9, while passive learning needs at least 25\% more data for training a comparable model.",,,,
7,li_hierarchical_2015,"Li, Jiwei; Luong, Minh-Thang; Jurafsky, Dan",A Hierarchical Neural Autoencoder for Paragraphs and Documents,arXiv:1506.01057 [cs],June,2015,http://arxiv.org/abs/1506.01057,"Natural language generation of coherent long texts like paragraphs or longer documents is a challenging problem for recurrent networks models. In this paper, we explore an important step toward this generation task: training an LSTM (Long-short term memory) auto-encoder to preserve and reconstruct multi-sentence paragraphs. We introduce an LSTM model that hierarchically builds an embedding for a paragraph from embeddings for sentences and words, then decodes this embedding to reconstruct the original paragraph. We evaluate the reconstructed paragraph using standard metrics like ROUGE and Entity Grid, showing that neural models are able to encode texts in a way that preserve syntactic, semantic, and discourse coherence. While only a first step toward generating coherent text units from neural models, our work has the potential to significantly impact natural language generation and summarization{\textbackslash}footnote\{\vphantom{\}}Code for the three models described in this paper can be found at www.stanford.edu/{\textasciitilde}jiweil/ .",,Computer Science - Computation and Language,,
7,li_macroscope:_2019,"Li, Ying; Engelthaler, Tomas; Siew, Cynthia S. Q.; Hills, Thomas T.",The Macroscope: A tool for examining the historical structure of language,Behavior Research Methods,February,2019,,"The recent rise in digitized historical text has made it possible to quantitatively study our psychological past. This involves understanding changes in what words meant, how words were used, and how these changes may have responded to changes in the environment, such as in healthcare, wealth disparity, and war. Here we make available a tool, the Macroscope, for studying historical changes in language over the last two centuries. The Macroscope uses over 155 billion words of historical text, which will grow as we include new historical corpora, and derives word properties from frequency-of-usage and co-occurrence patterns over time. Using co-occurrence patterns, the Macroscope can track changes in semantics, allowing researchers to identify semantically stable and unstable words in historical text and providing quantitative information about changes in a word's valence, arousal, and concreteness, as well as information about new properties, such as semantic drift. The Macroscope provides information about both the local and global properties of words, as well as information about how these properties change over time, allowing researchers to visualize and download data in order to make inferences about historical psychology. Although quantitative historical psychology represents a largely new field of study, we see this work as complementing a wealth of other historical investigations, offering new insights and new approaches to understanding existing theory. The Macroscope is available online at http://www.macroscope.tech .",,"Language evolution, Language statistics, Semantics, Word embedding",,
7,li_randomized_2019,"Li, Tianjing; Saldanha, Ian J.; Jap, Jens; Smith, Bryant T.; Canner, Joseph; Hutfless, Susan M.; Branch, Vernal; Carini, Simona; Chan, Wiley; de Bruijn, Berry; Wallace, Byron C.; Walsh, Sandra A.; Whamond, Elizabeth J.; Murad, M. Hassan; Sim, Ida; Berlin, Jesse A.; Lau, Joseph; Dickersin, Kay; Schmid, Christopher H.",A randomized trial provided new evidence on the accuracy and efficiency of traditional vs. electronically annotated abstraction approaches in systematic reviews,Journal of Clinical Epidemiology,November,2019,http://www.sciencedirect.com/science/article/pii/S0895435619302665,"Objectives Data Abstraction Assistant (DAA) is a software for linking items abstracted into a data collection form for a systematic review to their locations in a study report. We conducted a randomized cross-over trial that compared DAA-facilitated single-data abstraction plus verification (â€œDAA verificationâ€ù), single data abstraction plus verification (â€œregular verificationâ€ù), and independent dual data abstraction plus adjudication (â€œindependent abstractionâ€ù). Study Design and Setting This study is an online randomized cross-over trial with 26 pairs of data abstractors. Each pair abstracted data from six articles, two per approach. Outcomes were the proportion of errors and time taken. Results Overall proportion of errors was 17\% for DAA verification, 16\% for regular verification, and 15\% for independent abstraction. DAA verification was associated with higher odds of errors when compared with regular verification (adjusted odds ratio [OR]Â =Â 1.08; 95\% confidence interval [CI]: 0.99â€“1.17) or independent abstraction (adjusted ORÂ =Â 1.12; 95\% CI: 1.03â€“1.22). For each article, DAA verification took 20Â minutes (95\% CI: 1â€“40) longer than regular verification, but 46Â minutes (95\% CI: 26 to 66) shorter than independent abstraction. Conclusion Independent abstraction may only be necessary for complex data items. DAA provides an audit trail that is crucial for reproducible research.",,"Accuracy, Data abstraction, Efficiency, Randomized cross-over trial, Software application, Systematic review",,
7,liang_inferring_2017,"Liang, Shangsong; Ren, Zhaochun; Zhao, Yukun; Ma, Jun; Yilmaz, Emine; Rijke, Maarten De",Inferring Dynamic User Interests in Streams of Short Texts for User Clustering,ACM Trans. Inf. Syst.,July,2017,http://doi.acm.org/10.1145/3072606,"User clustering has been studied from different angles. In order to identify shared interests, behavior-based methods consider similar browsing or search patterns of users, whereas content-based methods use information from the contents of the documents visited by the users. So far, content-based user clustering has mostly focused on static sets of relatively long documents. Given the dynamic nature of social media, there is a need to dynamically cluster users in the context of streams of short texts. User clustering in this setting is more challenging than in the case of long documents, as it is difficult to capture the usersâ€™ dynamic topic distributions in sparse data settings. To address this problem, we propose a dynamic user clustering topic model (UCT). UCT adaptively tracks changes of each userâ€™s time-varying topic distributions based both on the short texts the user posts during a given time period and on previously estimated distributions. To infer changes, we propose a Gibbs sampling algorithm where a set of word pairs from each user is constructed for sampling. UCT can be used in two ways: (1) as a short-term dependency model that infers a userâ€™s current topic distribution based on the userâ€™s topic distributions during the previous time period only, and (2) as a long-term dependency model that infers a userâ€™s current topic distributions based on the userâ€™s topic distributions during multiple time periods in the past. The clustering results are explainable and human-understandable, in contrast to many other clustering algorithms. For evaluation purposes, we work with a dataset consisting of users and tweets from each user. Experimental results demonstrate the effectiveness of our proposed short-term and long-term dependency user clustering models compared to state-of-the-art baselines.",,"Diversity, ad hoc retrieval, data streams",,
7,lin_overview_nodate,"Lin, Jimmy; Efron, Miles; Wang, Yulu; Sherman, Garrick; Voorhees, Ellen",Overview of the TREC-2015 Microblog Track,,,,,,,,,
7,lin_rouge_2004,"Lin, Chin-Yew",ROUGE: A Package for Automatic Evaluation of Summaries,,,2004,,"ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. It includes measures to automatically determine the quality of a summary by comparing it to other (ideal) summaries created by humans. The measures count the number of overlapping units such as n-gram, word sequences, and word pairs between the computer-generated summary to be evaluated and the ideal summaries created by humans. This paper introduces four different ROUGE measures: ROUGE-N, ROUGE-L, ROUGE-W, and ROUGE-S included in the ROUGE summarization evaluation package and their evaluations. Three of them have been used in the Document Understanding Conference (DUC) 2004, a large-scale summarization evaluation sponsored by NIST.",,,,
7,lin_rouge_nodate,"Lin, Chin-Yew",ROUGE: A Package for Automatic Evaluation of Summaries,,,,,"ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. It includes measures to automatically determine the quality of a summary by comparing it to other (ideal) summaries created by humans. The measures count the number of overlapping units such as n-gram, word sequences, and word pairs between the computer-generated summary to be evaluated and the ideal summaries created by humans. This paper introduces four different ROUGE measures: ROUGE-N, ROUGE-L, ROUGE-W, and ROUGE-S included in the ROUGE summarization evaluation package and their evaluatio ns. Three of them have been used in the Document Understanding Conference (DUC) 2004, a large-scale summarization evaluation sponsored by NIST.",,summarization,,
7,lin_rouge_nodate,"Lin, Chin-Yew",ROUGE: A Package for Automatic Evaluation of Summaries,,,,,"ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. It includes measures to automatically determine the quality of a summary by comparing it to other (ideal) summaries created by humans. The measures count the number of overlapping units such as n-gram, word sequences, and word pairs between the computer-generated summary to be evaluated and the ideal summaries created by humans. This paper introduces four different ROUGE measures: ROUGE-N, ROUGE-L, ROUGE-W, and ROUGE-S included in the ROUGE summarization evaluation package and their evaluatio ns. Three of them have been used in the Document Understanding Conference (DUC) 2004, a large-scale summarization evaluation sponsored by NIST.",,,,
7,liu_bridging_2019,"Liu, Shixia; Wang, Xiting; Collins, Christopher; Dou, Wenwen; Ouyang, Fangxin; El-Assady, Mennatallah; Jiang, Liu; Keim, Daniel A.",Bridging Text Visualization and Mining: A Task-Driven Survey,IEEE Transactions on Visualization and Computer Graphics,July,2019,,"Visual text analytics has recently emerged as one of the most prominent topics in both academic research and the commercial world. To provide an overview of the relevant techniques and analysis tasks, as well as the relationships between them, we comprehensively analyzed 263 visualization papers and 4,346 mining papers published between 1992-2017 in two fields: visualization and text mining. From the analysis, we derived around 300 concepts (visualization techniques, mining techniques, and analysis tasks) and built a taxonomy for each type of concept. The co-occurrence relationships between the concepts were also extracted. Our research can be used as a stepping-stone for other researchers to 1) understand a common set of concepts used in this research topic; 2) facilitate the exploration of the relationships between visualization techniques, mining techniques, and analysis tasks; 3) understand the current practice in developing visual text analytics tools; 4) seek potential research opportunities by narrowing the gulf between visualization and mining techniques based on the analysis tasks; and 5) analyze other interdisciplinary research areas in a similar way. We have also contributed a web-based visualization tool for analyzing and understanding research trends and opportunities in visual text analytics.",,"Data visualization, Market research, Task analysis, Taxonomy, Text mining, Visualization, Web-based visualization tool, academic research, co-occurrence relationships, data analysis, data mining, data visualisation, interdisciplinary research areas, mining papers, relevant techniques, task-driven survey, text analysis, text mining, text visualization, visual text analytics, visual text analytics tools, visualization papers, visualization techniques",,
7,liu_clinically_nodate,"Liu, Guanxiong; Hsu, Tzu-Ming Harry; McDermott, Matthew; Boag, Willie; Weng, Wei-Hung; Szolovits, Peter; Ghassemi, Marzyeh",Clinically Accurate Chest X-Ray Report Generation,,,,,"The automatic generation of radiology reports given medical radiographs has signiï¬Åcant potential to operationally and improve clinical patient care. A number of prior works have focused on this problem, employing advanced methods from computer vision and natural language generation to produce readable reports. However, these works often fail to account for the particular nuances of the radiology domain, and, in particular, the critical importance of clinical accuracy in the resulting generated reports. In this work, we present a domain-aware automatic chest X-ray radiology report generation system which ï¬Årst predicts what topics will be discussed in the report, then conditionally generates sentences corresponding to these topics. The resulting system is ï¬Åne-tuned using reinforcement learning, considering both readability and clinical accuracy, as assessed by the proposed Clinically Coherent Reward. We verify this system on two datasets, Open-I and MIMICCXR, and demonstrate that our model oâ†µers marked improvements on both language generation metrics and CheXpert assessed accuracy over a variety of competitive baselines.",,,,
7,liu_fine-tune_2019,"Liu, Yang",Fine-tune BERT for Extractive Summarization,arXiv:1903.10318 [cs],September,2019,http://arxiv.org/abs/1903.10318,"BERT, a pre-trained Transformer model, has achieved ground-breaking performance on multiple NLP tasks. In this paper, we describe BERTSUM, a simple variant of BERT, for extractive summarization. Our system is the state of the art on the CNN/Dailymail dataset, outperforming the previous best-performed system by 1.65 on ROUGE-L. The codes to reproduce our results are available at https://github.com/nlpyang/BertSum",,"Computer Science - Computation and Language, summarization",,
7,liu_fine-tune_2019,"Liu, Yang",Fine-tune BERT for Extractive Summarization,arXiv:1903.10318 [cs],September,2019,http://arxiv.org/abs/1903.10318,"BERT, a pre-trained Transformer model, has achieved ground-breaking performance on multiple NLP tasks. In this paper, we describe BERTSUM, a simple variant of BERT, for extractive summarization. Our system is the state of the art on the CNN/Dailymail dataset, outperforming the previous best-performed system by 1.65 on ROUGE-L. The codes to reproduce our results are available at https://github.com/nlpyang/BertSum",,"Computer Science - Computation and Language, unread",,
7,liu_fine-tune_2019-1,"Liu, Yang",Fine-tune BERT for Extractive Summarization,arXiv:1903.10318 [cs],September,2019,http://arxiv.org/abs/1903.10318,"BERT, a pre-trained Transformer model, has achieved ground-breaking performance on multiple NLP tasks. In this paper, we describe BERTSUM, a simple variant of BERT, for extractive summarization. Our system is the state of the art on the CNN/Dailymail dataset, outperforming the previous best-performed system by 1.65 on ROUGE-L. The codes to reproduce our results are available at https://github.com/nlpyang/BertSum",,"Computer Science - Computation and Language, summarization",,
7,liu_hard_2011,"Liu, Yufeng; Zhang, Hao Helen; Wu, Yichao",Hard or Soft Classification? Large-margin Unified Machines,Journal of the American Statistical Association,March,2011,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3233196/,,,,,
7,liu_representation_2015,"Liu, Xiaodong; Gao, Jianfeng; He, Xiaodong; Deng, Li; Duh, Kevin; Wang, Ye-Yi",Representation Learning Using Multi-Task Deep Neural Networks for Semantic Classification and Information Retrieval,,May,2015,https://www.microsoft.com/en-us/research/publication/representation-learning-using-multi-task-deep-neural-networks-for-semantic-classification-and-information-retrieval/,"Methods of deep neural networks (DNNs) have recently demonstrated superior performance on a number of natural language processing tasks. However, in most previous work, the models are learned based on either unsupervised objectives, which does not directly optimize the desired task, or singletask supervised objectives, which often suffer from insufficient training data. We develop a multi-task DNN for learning representations across multiple tasks, not only leveraging large amounts of cross-task data, but also benefiting from a regularization effect that leads to more general representations to help tasks in new domains. Our multi-task DNN approach combines tasks of multiple-domain classification (for query classification) and information retrieval (ranking for web search), and demonstrates significant gains over strong baselines in a comprehensive set of domain adaptation.",,,,
7,liu_representation_2015,"Liu, Xiaodong; Gao, Jianfeng; He, Xiaodong; Deng, Li; Duh, Kevin; Wang, Ye-Yi",Representation Learning Using Multi-Task Deep Neural Networks for Semantic Classification and Information Retrieval,,May,2015,https://www.microsoft.com/en-us/research/publication/representation-learning-using-multi-task-deep-neural-networks-for-semantic-classification-and-information-retrieval/,"Methods of deep neural networks (DNNs) have recently demonstrated superior performance on a number of natural language processing tasks. However, in most previous work, the models are learned based on either unsupervised objectives, which does not directly optimize the desired task, or singletask supervised objectives, which often suffer from insufficient training data. We develop a â€_",,,,
7,liu_roberta_2019,"Liu, Yinhan; Ott, Myle; Goyal, Naman; Du, Jingfei; Joshi, Mandar; Chen, Danqi; Levy, Omer; Lewis, Mike; Zettlemoyer, Luke; Stoyanov, Veselin",RoBERTa: A Robustly Optimized BERT Pretraining Approach,arXiv:1907.11692 [cs],July,2019,http://arxiv.org/abs/1907.11692,"Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.",,Computer Science - Computation and Language,,
7,liu_roberta_nodate,"Liu, Yinhan; Ott, Myle; Goyal, Naman; Du, Jingfei; Joshi, Mandar; Chen, Danqi; Levy, Omer; Lewis, Mike; Zettlemoyer, Luke; Stoyanov, Veselin",RoBERTa: A Robustly Optimized BERT Pretraining Approach,,,,,,,unread,,
7,liu_roberta:_2019,"Liu, Yinhan; Ott, Myle; Goyal, Naman; Du, Jingfei; Joshi, Mandar; Chen, Danqi; Levy, Omer; Lewis, Mike; Zettlemoyer, Luke; Stoyanov, Veselin",RoBERTa: A Robustly Optimized BERT Pretraining Approach,arXiv:1907.11692 [cs],July,2019,http://arxiv.org/abs/1907.11692,"Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.",,Computer Science - Computation and Language,,
7,liu_text_2019,"Liu, Yang; Lapata, Mirella",Text Summarization with Pretrained Encoders,arXiv:1908.08345 [cs],September,2019,http://arxiv.org/abs/1908.08345,"Bidirectional Encoder Representations from Transformers (BERT) represents the latest incarnation of pretrained language models which have recently advanced a wide range of natural language processing tasks. In this paper, we showcase how BERT can be usefully applied in text summarization and propose a general framework for both extractive and abstractive models. We introduce a novel document-level encoder based on BERT which is able to express the semantics of a document and obtain representations for its sentences. Our extractive model is built on top of this encoder by stacking several inter-sentence Transformer layers. For abstractive summarization, we propose a new fine-tuning schedule which adopts different optimizers for the encoder and the decoder as a means of alleviating the mismatch between the two (the former is pretrained while the latter is not). We also demonstrate that a two-staged fine-tuning approach can further boost the quality of the generated summaries. Experiments on three datasets show that our model achieves state-of-the-art results across the board in both extractive and abstractive settings. Our code is available at https://github.com/nlpyang/PreSumm",,"Computer Science - Computation and Language, Computer Science - Machine Learning",,
7,louizos_learning_2018,"Louizos, Christos; Welling, Max; Kingma, Diederik P.",Learning Sparse Neural Networks through \$L\_0\$ Regularization,"arXiv:1712.01312 [cs, stat]",June,2018,http://arxiv.org/abs/1712.01312,"We propose a practical method for L0 norm regularization for neural networks: pruning the network during training by encouraging weights to become exactly zero. Such regularization is interesting since (1) it can greatly speed up training and inference, and (2) it can improve generalization. AIC and BIC, well-known model selection criteria, are special cases of L0 regularization. However, since the L0 norm of weights is non-differentiable, we cannot incorporate it directly as a regularization term in the objective function. We propose a solution through the inclusion of a collection of non-negative stochastic gates, which collectively determine which weights to set to zero. We show that, somewhat surprisingly, for certain distributions over the gates, the expected L0 regularized objective is differentiable with respect to the distribution parameters. We further propose the hard concrete distribution for the gates, which is obtained by â€œstretchingâ€ù a binary concrete distribution and then transforming its samples with a hard-sigmoid. The parameters of the distribution over the gates can then be jointly optimized with the original network parameters. As a result our method allows for straightforward and efï¬Åcient learning of model structures with stochastic gradient descent and allows for conditional computation in a principled way. We perform various experiments to demonstrate the effectiveness of the resulting approach and regularizer.",,"Computer Science - Machine Learning, Statistics - Machine Learning",,
7,luong_effective_2015,"Luong, Minh-Thang; Pham, Hieu; Manning, Christopher D.",Effective Approaches to Attention-based Neural Machine Translation,arXiv:1508.04025 [cs],August,2015,http://arxiv.org/abs/1508.04025,"An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches over the WMT translation tasks between English and German in both directions. With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems which already incorporate known techniques such as dropout. Our ensemble model using different attention architectures has established a new state-of-the-art result in the WMT'15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker.",,"Attention, Computer Science - Computation and Language",,
7,ma_active_2020,"Ma, Lin; Ding, Bailu; Das, Sudipto; Swaminathan, Adith",Active Learning for ML Enhanced Database Systems,Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data,June,2020,https://dl.acm.org/doi/abs/10.1145/3318464.3389768,"Recent research has shown promising results by using machine learning (ML) techniques to improve the performance of database systems, e.g., in query optimization or index recommendation. However, in many production deployments, the ML modelsâ€™ performance degrades significantly when the test data diverges from the data used to train these models. In this paper, we address this performance degradation by using B-instances to collect additional data during deployment. We propose an active data collection platform, ADCP, that employs active learning (AL) to gather relevant data cost-effectively. We develop a novel AL technique, Holistic Active Learner (HAL), that robustly combines multiple noisy signals for data gathering in the context of database applications. HAL applies to various ML tasks, budget sizes, cost types, and budgeting interfaces for database applications. We evaluate ADCP on both industry-standard benchmarks and real customer workloads. Our evaluation shows that, compared with other baselines, our technique improves ML modelsâ€™ prediction performance by up to 2Ã— with the same cost budget. In particular, on production workloads, our technique reduces the prediction error of ML models by 75\% using about 100 additionally collected queries.",,,,
7,macavaney_cedr_2019-1,"MacAvaney, Sean; Yates, Andrew; Cohan, Arman; Goharian, Nazli",CEDR: Contextualized Embeddings for Document Ranking,Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval - SIGIR'19,,2019,http://arxiv.org/abs/1904.07094,"Although considerable attention has been given to neural ranking architectures recently, far less attention has been paid to the term representations that are used as input to these models. In this work, we investigate how two pretrained contextualized language models (ELMo and BERT) can be utilized for ad-hoc document ranking. Through experiments on TREC benchmarks, we find that several existing neural ranking architectures can benefit from the additional context provided by contextualized language models. Furthermore, we propose a joint approach that incorporates BERT's classification vector into existing neural models and show that it outperforms state-of-the-art ad-hoc ranking baselines. We call this joint approach CEDR (Contextualized Embeddings for Document Ranking). We also address practical challenges in using these models for ranking, including the maximum input length imposed by BERT and runtime performance impacts of contextualized language models.",,"Computer Science - Computation and Language, Computer Science - Information Retrieval",,
7,majkowska_chest_2020,"Majkowska, Anna; Mittal, Sid; Steiner, David F.; Reicher, Joshua J.; McKinney, Scott Mayer; Duggan, Gavin E.; Eswaran, Krish; Cameron Chen, Po-Hsuan; Liu, Yun; Kalidindi, Sreenivasa Raju; Ding, Alexander; Corrado, Greg S.; Tse, Daniel; Shetty, Shravya",Chest Radiograph Interpretation with Deep Learning Models: Assessment with Radiologist-adjudicated Reference Standards and Population-adjusted Evaluation,Radiology,February,2020,http://pubs.rsna.org/doi/10.1148/radiol.2019191293,"Background: Deep learning has the potential to augment the use of chest radiography in clinical radiology, but challenges include poor generalizability, spectrum bias, and difficulty comparing across studies. Purpose:â€ƒ To develop and evaluate deep learning models for chest radiograph interpretation by using radiologist-adjudicated reference standards. Materials and Methods:â€ƒ Deep learning models were developed to detect four findings (pneumothorax, opacity, nodule or mass, and fracture) on frontal chest radiographs. This retrospective study used two data sets. Data set 1 (DS1) consisted of 759â€‰611 images from a multicity hospital network and ChestX-ray14 is a publicly available data set with 112â€‰120 images. Natural language processing and expert review of a subset of images provided labels for 657â€‰954 training images. Test sets consisted of 1818 and 1962 images from DS1 and ChestX-ray14, respectively. Reference standards were defined by radiologist-adjudicated image review. Performance was evaluated by area under the receiver operating characteristic curve analysis, sensitivity, specificity, and positive predictive value. Four radiologists reviewed test set images for performance comparison. Inverse probability weighting was applied to DS1 to account for positive radiograph enrichment and estimate population-level performance. Results:â€ƒ In DS1, population-adjusted areas under the receiver operating characteristic curve for pneumothorax, nodule or mass, airspace opacity, and fracture were, respectively, 0.95 (95\% confidence interval [CI]: 0.91, 0.99), 0.72 (95\% CI: 0.66, 0.77), 0.91 (95\% CI: 0.88, 0.93), and 0.86 (95\% CI: 0.79, 0.92). With ChestX-ray14, areas under the receiver operating characteristic curve were 0.94 (95\% CI: 0.93, 0.96), 0.91 (95\% CI: 0.89, 0.93), 0.94 (95\% CI: 0.93, 0.95), and 0.81 (95\% CI: 0.75, 0.86), respectively. Conclusion:â€ƒ Expert-level models for detecting clinically relevant chest radiograph findings were developed for this study by using adjudicated reference standards and with population-level performance estimation. Radiologist-adjudicated labels for 2412 ChestXray14 validation set images and 1962 test set images are provided.",,,,
7,makki_atr-vis_2018,"Makki, Raheleh; Carvalho, Eder; Soto, Axel J.; Brooks, Stephen; Oliveira, Maria Cristina Ferreira De; Milios, Evangelos; Minghim, Rosane",ATR-Vis: Visual and Interactive Information Retrieval for Parliamentary Discussions in Twitter,ACM Transactions on Knowledge Discovery from Data,February,2018,https://dl.acm.org/doi/10.1145/3047010,,,,,
7,mamou_emergence_nodate,"Mamou, Jonathan; Le, Hang; Rio, Miguel A. Del; Stephenson, Cory; Tang, Hanlin; Kim, Yoon; Chung, SueYeon",Emergence of Separable Manifolds in Deep Language Representations,,,,,,,,,
7,mastakouri_necessary_2020,"Mastakouri, Atalanti A.; SchÃ¶lkopf, Bernhard; Janzing, Dominik",Necessary and sufficient conditions for causal feature selection in time series with latent common causes,arXiv:2005.08543 [stat],June,2020,http://arxiv.org/abs/2005.08543,"We study the identification of direct and indirect causes on time series and provide necessary and sufficient conditions in the presence of latent variables. Our theoretical results and estimation algorithms require two conditional independence tests for each observed candidate time series to determine whether or not it is a cause of an observed target time series. We provide experimental results in simulations, where the ground truth is known, as well as in real data. Our results show that our method leads to essentially no false positives and relatively low false negative rates, even in confounded environments with non-unique lag effects, outperforming the widely used Granger causality and two more methods.",,"Statistics - Machine Learning, Statistics - Methodology",,
7,mastronardo_enhancing_nodate,"Mastronardo, Claudio; Tamburini, Fabio",Enhancing a Text Summarization System with ELMo,,,,,Text summarization has gained a considerable amount of research interest due to deep learning based techniques. We leverage recent results in transfer learning for Natural Language Processing (NLP) using pre-trained deep contextualized word embeddings in a sequence-to-sequence architecture based on pointer-generator networks. We evaluate our approach on the two largest summarization datasets: CNN/Daily Mail and the recent Newsroom dataset. We show how using pre-trained contextualized embeddings on Newsroom improves signiï¬Åcantly the state-of-the-art ROUGE-1 measure and obtains comparable scores on the other ROUGE values.,,,,
7,may_measuring_2019,"May, Chandler; Wang, Alex; Bordia, Shikha; Bowman, Samuel R.; Rudinger, Rachel",On Measuring Social Biases in Sentence Encoders,arXiv:1903.10561 [cs],March,2019,http://arxiv.org/abs/1903.10561,"The Word Embedding Association Test shows that GloVe and word2vec word embeddings exhibit human-like implicit biases based on gender, race, and other social constructs (Caliskan et al., 2017). Meanwhile, research on learning reusable text representations has begun to explore sentence-level texts, with some sentence encoders seeing enthusiastic adoption. Accordingly, we extend the Word Embedding Association Test to measure bias in sentence encoders. We then test several sentence encoders, including state-of-the-art methods such as ELMo and BERT, for the social biases studied in prior work and two important biases that are difficult or impossible to test at the word level. We observe mixed results including suspicious patterns of sensitivity that suggest the test's assumptions may not hold in general. We conclude by proposing directions for future work on measuring bias in sentence encoders.",,"Computer Science - Computation and Language, Computer Science - Computers and Society",,
7,mckeown_predicting_2016,"McKeown, Kathy; Daume, Hal; Chaturvedi, Snigdha; Paparrizos, John; Thadani, Kapil; Barrio, Pablo; Biran, Or; Bothe, Suvarna; Collins, Michael; Fleischmann, Kenneth R.; Gravano, Luis; Jha, Rahul; King, Ben; McInerney, Kevin; Moon, Taesun; Neelakantan, Arvind; O'Seaghdha, Diarmuid; Radev, Dragomir; Templeton, Clay; Teufel, Simone",Predicting the impact of scientific concepts using full-text features,Journal of the Association for Information Science and Technology,,2016,https://onlinelibrary.wiley.com/doi/abs/10.1002/asi.23612,"New scientific concepts, interpreted broadly, are continuously introduced in the literature, but relatively few concepts have a long-term impact on society. The identification of such concepts is a challenging prediction task that would help multiple partiesâ€”including researchers and the general publicâ€”focus their attention within the vast scientific literature. In this paper we present a system that predicts the future impact of a scientific concept, represented as a technical term, based on the information available from recently published research articles. We analyze the usefulness of rich features derived from the full text of the articles through a variety of approaches, including rhetorical sentence analysis, information extraction, and time-series analysis. The results from two large-scale experiments with 3.8 million full-text articles and 48 million metadata records support the conclusion that full-text features are significantly more useful for prediction than metadata-only features and that the most accurate predictions result from combining the metadata and full-text features. Surprisingly, these results hold even when the metadata features are available for a much larger number of documents than are available for the full-text features.",,"machine learning, natural language processing, scientometrics",,
7,michel_are_2019,"Michel, Paul; Levy, Omer; Neubig, Graham",Are Sixteen Heads Really Better than One?,arXiv:1905.10650 [cs],November,2019,http://arxiv.org/abs/1905.10650,"Attention is a powerful and ubiquitous mechanism for allowing neural models to focus on particular salient pieces of information by taking their weighted average when making predictions. In particular, multi-headed attention is a driving force behind many recent state-of-the-art natural language processing (NLP) models such as Transformer-based MT models and BERT. These models apply multiple attention mechanisms in parallel, with each attention â€œheadâ€ù potentially focusing on different parts of the input, which makes it possible to express sophisticated functions beyond the simple weighted average. In this paper we make the surprising observation that even if models have been trained using multiple heads, in practice, a large percentage of attention heads can be removed at test time without signiï¬Åcantly impacting performance. In fact, some layers can even be reduced to a single head. We further examine greedy algorithms for pruning down models, and the potential speed, memory efï¬Åciency, and accuracy improvements obtainable therefrom. Finally, we analyze the results with respect to which parts of the model are more reliant on having multiple heads, and provide precursory evidence that training dynamics play a role in the gains provided by multi-head attention1.",,Computer Science - Computation and Language,,
7,mikolov_efficient_2013,"Mikolov, Tomas; Chen, Kai; Corrado, Greg; Dean, Jeffrey",Efficient Estimation of Word Representations in Vector Space,arXiv:1301.3781 [cs],January,2013,http://arxiv.org/abs/1301.3781,"We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.",,Computer Science - Computation and Language,,
7,miller_leveraging_2019,"Miller, Derek",Leveraging BERT for Extractive Text Summarization on Lectures,"arXiv:1906.04165 [cs, eess, stat]",June,2019,http://arxiv.org/abs/1906.04165,"In the last two decades, automatic extractive text summarization on lectures has demonstrated to be a useful tool for collecting key phrases and sentences that best represent the content. However, many current approaches utilize dated approaches, producing sub-par outputs or requiring several hours of manual tuning to produce meaningful results. Recently, new machine learning architectures have provided mechanisms for extractive summarization through the clustering of output embeddings from deep learning models. This paper reports on the project called Lecture Summarization Service, a python based RESTful service that utilizes the BERT model for text embeddings and KMeans clustering to identify sentences closes to the centroid for summary selection. The purpose of the service was to provide students a utility that could summarize lecture content, based on their desired number of sentences. On top of the summary work, the service also includes lecture and summary management, storing content on the cloud which can be used for collaboration. While the results of utilizing BERT for extractive summarization were promising, there were still areas where the model struggled, providing feature research opportunities for further improvement.",,"Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning",,
7,miller_leveraging_nodate,"Miller, Derek",Leveraging BERT for Extractive Text Summarization on Lectures,,,,,"In the last two decades, automatic extractive text summarization on lectures has demonstrated to be a useful tool for collecting key phrases and sentences that best represent the content. However, many current approaches utilize dated approaches, producing sub-par outputs or requiring several hours of manual tuning to produce meaningful results. Recently, new machine learning architectures have provided mechanisms for extractive summarization through the clustering of output embeddings from deep learning models. This paper reports on the project called â€œlecture summarization serviceâ€ù, a python-based RESTful service that utilizes the BERT model for text embeddings and K-Means clustering to identify sentences closest to the centroid for summary selection. The purpose of the service was to provide studentâ€™s a utility that could summarize lecture content, based on their desired number of sentences. On top of summary work, the service also includes lecture and summary management, storing content on the cloud which can be used for collaboration. While the results of utilizing BERT for extractive text summarization were promising, there were still areas where the model struggled, providing future research opportunities for further improvement. All code and results can be found here: https://github.com/dmmiller612/lecture-summarizer.",,,,
7,miller_wordnet:_1995,"Miller, George A.",WordNet: A Lexical Database for English,Commun. ACM,,1995,http://doi.acm.org/10.1145/219717.219748,"Because meaningful sentences are composed of meaningful words, any system that hopes to process natural languages as people do must have information about words and their meanings. This information is traditionally provided through dictionaries, and machine-readable dictionaries are now widely available. But dictionary entries evolved for the convenience of human readers, not for machines. WordNet1 provides a more effective combination of traditional lexicographic information and modern computing. WordNet is an online lexical database designed for use under program control. English nouns, verbs, adjectives, and adverbs are organized into sets of synonyms, each representing a lexicalized concept. Semantic relations link the synonym sets [4].",,,,
7,milne_open-source_2013,"Milne, David; Witten, Ian H.",An open-source toolkit for mining Wikipedia,Artificial Intelligence,January,2013,http://www.sciencedirect.com/science/article/pii/S000437021200077X,,,"Annotation, Disambiguation, Ontology extraction, Semantic relatedness, Toolkit, Wikipedia",,
7,miotto_identifying_2020,"Miotto, Riccardo; Percha, Bethany L.; Glicksberg, Benjamin S.; Lee, Hao-Chih; Cruz, Lisanne; Dudley, Joel T.; Nabeel, Ismail",Identifying Acute Low Back Pain Episodes in Primary Care Practice From Clinical Notes: Observational Study,JMIR Medical Informatics,,2020,https://medinform.jmir.org/2020/2/e16878/,"Background:  Acute and chronic low back pain (LBP) are different conditions with different treatments. However, they are coded in electronic health records with the same International Classification of Diseases, 10th revision (ICD-10) code (M54.5) and can be differentiated only by retrospective chart reviews. This prevents an efficient definition of data-driven guidelines for billing and therapy recommendations, such as return-to-work options.  Objective:  The objective of this study was to evaluate the feasibility of automatically distinguishing acute LBP episodes by analyzing free-text clinical notes.  Methods:  We used a dataset of 17,409 clinical notes from different primary care practices; of these, 891 documents were manually annotated as  acute LBP  and 2973 were generally associated with LBP via the recorded ICD-10 code. We compared different supervised and unsupervised strategies for automated identification: keyword search, topic modeling, logistic regression with bag of n-grams and manual features, and deep learning (a convolutional neural network-based architecture [ConvNet]). We trained the supervised models using either manual annotations or ICD-10 codes as positive labels.  Results:  ConvNet trained using manual annotations obtained the best results with an area under the receiver operating characteristic curve of 0.98 and an F score of 0.70. ConvNetâ€™s results were also robust to reduction of the number of manually annotated documents. In the absence of manual annotations, topic models performed better than methods trained using ICD-10 codes, which were unsatisfactory for identifying LBP acuity.  Conclusions:  This study uses clinical notes to delineate a potential path toward systematic learning of therapeutic strategies, billing guidelines, and management options for acute LBP at the point of care.  [JMIR Med Inform 2020;8(2):e16878]",,,,
7,mitra_introduction_2018,"Mitra, Bhaskar; Craswell, Nick",An Introduction to Neural Information Retrieval,Foundations and TrendsÂ® in Information Retrieval,December,2018,https://www.microsoft.com/en-us/research/publication/introduction-neural-information-retrieval/,"Neural ranking models for information retrieval (IR) use shallow or deep neural networks to rank search results in response to a query. Traditional learning to rank models employ supervised machine learning (ML) techniquesâ€”including neural networksâ€”over hand-crafted IR features. By contrast, more recently proposed neural models learn representations of language from raw text that can bridge â€_",,,,
7,mitra_learning_2016,"Mitra, Bhaskar; Diaz, Fernando; Craswell, Nick",Learning to Match Using Local and Distributed Representations of Text for Web Search,arXiv:1610.08136 [cs],October,2016,http://arxiv.org/abs/1610.08136,"Models such as latent semantic analysis and those based on neural embeddings learn distributed representations of text, and match the query against the document in the latent semantic space. In traditional information retrieval models, on the other hand, terms have discrete or local representations, and the relevance of a document is determined by the exact matches of query terms in the body text. We hypothesize that matching with distributed representations complements matching with traditional local representations, and that a combination of the two is favorable. We propose a novel document ranking model composed of two separate deep neural networks, one that matches the query and the document using a local representation, and another that matches the query and the document using learned distributed representations. The two networks are jointly trained as part of a single neural network. We show that this combination or `duet' performs significantly better than either neural network individually on a Web page ranking task, and also significantly outperforms traditional baselines and other recently proposed models based on neural networks.",,Computer Science - Information Retrieval,,
7,mitra_neural_2017,"Mitra, Bhaskar; Craswell, Nick",Neural Models for Information Retrieval,arXiv:1705.01509 [cs],May,2017,http://arxiv.org/abs/1705.01509,"Neural ranking models for information retrieval (IR) use shallow or deep neural networks to rank search results in response to a query. Traditional learning to rank models employ machine learning techniques over hand-crafted IR features. By contrast, neural models learn representations of language from raw text that can bridge the gap between query and document vocabulary. Unlike classical IR models, these new machine learning based approaches are data-hungry, requiring large scale training data before they can be deployed. This tutorial introduces basic concepts and intuitions behind neural IR models, and places them in the context of traditional retrieval models. We begin by introducing fundamental concepts of IR and different neural and non-neural approaches to learning vector representations of text. We then review shallow neural IR methods that employ pre-trained neural term embeddings without learning the IR task end-to-end. We introduce deep neural networks next, discussing popular deep architectures. Finally, we review the current DNN models for information retrieval. We conclude with a discussion on potential future directions for neural IR.",,Computer Science - Information Retrieval,,
7,momin_towards_2016,"Momin, Afiz",Towards Expertise Modeling Using Hierarchical Classification and Wikipedia Knowledge,,December,2016,https://DalSpace.library.dal.ca//handle/10222/72603,,,,,
7,mottaghi_medical_2020,"Mottaghi, Ali; Sarma, Prathusha K.; Amatriain, Xavier; Yeung, Serena; Kannan, Anitha",Medical symptom recognition from patient text: An active learning approach for long-tailed multilabel distributions,arXiv:2011.06874 [cs],November,2020,http://arxiv.org/abs/2011.06874,"We study the problem of medical symptoms recognition from patient text, for the purposes of gathering pertinent information from the patient (known as history-taking). We introduce an active learning method that leverages underlying structure of a continually refined, learned latent space to select the most informative examples to label. This enables the selection of the most informative examples that progressively increases the coverage on the universe of symptoms via the learned model, despite the long tail in data distribution.",,"Computer Science - Computation and Language, Computer Science - Machine Learning",,
7,mullenbach_explainable_2018,"Mullenbach, James; Wiegreffe, Sarah; Duke, Jon; Sun, Jimeng; Eisenstein, Jacob",Explainable Prediction of Medical Codes from Clinical Text,"arXiv:1802.05695 [cs, stat]",April,2018,http://arxiv.org/abs/1802.05695,"Clinical notes are text documents that are created by clinicians for each patient encounter. They are typically accompanied by medical codes, which describe the diagnosis and treatment. Annotating these codes is labor intensive and error prone; furthermore, the connection between the codes and the text is not annotated, obscuring the reasons and details behind specific diagnoses and treatments. We present an attentional convolutional network that predicts medical codes from clinical text. Our method aggregates information across the document using a convolutional neural network, and uses an attention mechanism to select the most relevant segments for each of the thousands of possible codes. The method is accurate, achieving precision@8 of 0.71 and a Micro-F1 of 0.54, which are both better than the prior state of the art. Furthermore, through an interpretability evaluation by a physician, we show that the attention mechanism identifies meaningful explanations for each code assignment",,"Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning",,
7,nallapati_abstractive_2016,"Nallapati, Ramesh; Zhou, Bowen; santos, Cicero Nogueira dos; Gulcehre, Caglar; Xiang, Bing",Abstractive Text Summarization Using Sequence-to-Sequence RNNs and Beyond,arXiv:1602.06023 [cs],August,2016,http://arxiv.org/abs/1602.06023,"In this work, we model abstractive text summarization using Attentional EncoderDecoder Recurrent Neural Networks, and show that they achieve state-of-the-art performance on two different corpora. We propose several novel models that address critical problems in summarization that are not adequately modeled by the basic architecture, such as modeling key-words, capturing the hierarchy of sentence-toword structure, and emitting words that are rare or unseen at training time. Our work shows that many of our proposed models contribute to further improvement in performance. We also propose a new dataset consisting of multi-sentence summaries, and establish performance benchmarks for further research.",,Computer Science - Computation and Language,,
7,narayan_ranking_2018,"Narayan, Shashi; Cohen, Shay B.; Lapata, Mirella",Ranking Sentences for Extractive Summarization with Reinforcement Learning,arXiv:1802.08636 [cs],April,2018,http://arxiv.org/abs/1802.08636,Single document summarization is the task of producing a shorter version of a document while preserving its principal information content. In this paper we conceptualize extractive summarization as a sentence ranking task and propose a novel training algorithm which globally optimizes the ROUGE evaluation metric through a reinforcement learning objective. We use our algorithm to train a neural summarization model on the CNN and DailyMail datasets and demonstrate experimentally that it outperforms state-of-the-art extractive and abstractive systems when evaluated automatically and by humans.,,Computer Science - Computation and Language,,
7,nasir_semantic_2013,"Nasir, Jamal A.; Varlamis, Iraklis; Karim, Asim; Tsatsaronis, George",Semantic smoothing for text clustering,Knowledge-Based Systems,December,2013,http://www.sciencedirect.com/science/article/pii/S0950705113002906,"In this paper we present a new semantic smoothing vector space kernel (S-VSM) for text documents clustering. In the suggested approach semantic relatedness between words is used to smooth the similarity and the representation of text documents. The basic hypothesis examined is that considering semantic relatedness between two text documents may improve the performance of the text document clustering task. For our experimental evaluation we analyze the performance of several semantic relatedness measures when embedded in the proposed (S-VSM) and present results with respect to different experimental conditions, such as: (i) the datasets used, (ii) the underlying knowledge sources of the utilized measures, and (iii) the clustering algorithms employed. To the best of our knowledge, the current study is the first to systematically compare, analyze and evaluate the impact of semantic smoothing in text clustering based on â€˜wisdom of linguistsâ€™, e.g., WordNets, â€˜wisdom of crowdsâ€™, e.g., Wikipedia, and â€˜wisdom of corporaâ€™, e.g., large text corpora represented with the traditional Bag of Words (BoW) model. Three semantic relatedness measures for text are considered; two knowledge-based (Omiotis [1] that uses WordNet, and WLM [2] that uses Wikipedia), and one corpus-based (PMI [3] trained on a semantically tagged SemCor version). For the comparison of different experimental conditions we use the BCubed F-Measure evaluation metric which satisfies all formal constraints of good quality cluster. The experimental results show that the clustering performance based on the S-VSM is better compared to the traditional VSM model and compares favorably against the standard GVSM kernel which uses word co-occurrences to compute the latent similarities between document terms.",,"Generalized vector space model kernel, Semantic smoothing kernels, Text clustering, Wikipedia, WordNet",,
7,ng_better_2015,"Ng, Jun-Ping; Abrecht, Viktoria",Better Summarization Evaluation with Word Embeddings for ROUGE,arXiv:1508.06034 [cs],August,2015,http://arxiv.org/abs/1508.06034,"ROUGE is a widely adopted, automatic evaluation measure for text summarization. While it has been shown to correlate well with human judgements, it is biased towards surface lexical similarities. This makes it unsuitable for the evaluation of abstractive summarization, or summaries with substantial paraphrasing. We study the effectiveness of word embeddings to overcome this disadvantage of ROUGE. Speciï¬Åcally, instead of measuring lexical overlaps, word embeddings are used to compute the semantic similarity of the words used in summaries instead. Our experimental results show that our proposal is able to achieve better correlations with human judgements when measured with the Spearman and Kendall rank coefï¬Åcients.",,"Computer Science - Computation and Language, Computer Science - Information Retrieval",,
7,nguyen_bertweet_2020,"Nguyen, Dat Quoc; Vu, Thanh; Nguyen, Anh Tuan",BERTweet: A pre-trained language model for English Tweets,arXiv:2005.10200 [cs],May,2020,http://arxiv.org/abs/2005.10200,"We present BERTweet, the first public large-scale pre-trained language model for English Tweets. Our BERTweet is trained using the RoBERTa pre-training procedure (Liu et al., 2019), with the same model configuration as BERT-base (Devlin et al., 2019). Experiments show that BERTweet outperforms strong baselines RoBERTa-base and XLM-R-base (Conneau et al., 2020), producing better performance results than the previous state-of-the-art models on three Tweet NLP tasks: Part-of-speech tagging, Named-entity recognition and text classification. We release BERTweet to facilitate future research and downstream applications on Tweet data. Our BERTweet is available at: https://github.com/VinAIResearch/BERTweet",,"Computer Science - Computation and Language, Computer Science - Machine Learning, Test",,
7,nguyen_social_2018,"Nguyen, Minh-Tien; Tran, Duc-Vu; Nguyen, Le-Minh",Social context summarization using user-generated content and third-party sources,Knowledge-Based Systems,March,2018,https://linkinghub.elsevier.com/retrieve/pii/S0950705117306019,"In the context of social media, users mutually share their interests of an event mentioned in a Web document. Its content can also be found in different news providers with a writing variation. This paper presents a framework which exploits the support of social context (user-generated content such as comments or tweets and third-party sources such as relevant documents retrieved from a search engine) to extract high-quality summaries. The extraction was formulated in two steps: sentence scoring and selection. The scoring is modeled as a learning to rank problem, which employs Ranking SVM to mutually exploits sentences, user-generated content, and third-party sources in the form of features to cover summary aspects. For the selection, summaries are extracted by using a score-based or voting method. For evaluation, three datasets of sentence and highlight extraction in two languages were taken as a case study. Experimental results indicate that by integrating user-generated content and third-party sources, our framework obtains improvements of ROUGE-scores over state-of-the-art methods for singledocument summarization.",,,,
7,nguyen_toward_2016,"Nguyen, Gia-Hung; Tamine, Lynda; Soulier, Laure; Bricon-Souf, Nathalie",Toward a Deep Neural Approach for Knowledge-Based IR,arXiv:1606.07211 [cs],June,2016,http://arxiv.org/abs/1606.07211,"This paper tackles the problem of the semantic gap between a document and a query within an ad-hoc information retrieval task. In this context, knowledge bases (KBs) have already been acknowledged as valuable means since they allow the representation of explicit relations between entities. However, they do not necessarily represent implicit relations that could be hidden in a corpora. This latter issue is tackled by recent works dealing with deep representation learn ing of texts. With this in mind, we argue that embedding KBs within deep neural architectures supporting documentquery matching would give rise to fine-grained latent representations of both words and their semantic relations. In this paper, we review the main approaches of neural-based document ranking as well as those approaches for latent representation of entities and relations via KBs. We then propose some avenues to incorporate KBs in deep neural approaches for document ranking. More particularly, this paper advocates that KBs can be used either to support enhanced latent representations of queries and documents based on both distributional and relational semantics or to serve as a semantic translator between their latent distributional representations.",,"Computer Science - Computation and Language, Computer Science - Information Retrieval",,
7,nicolau_parallel_2016,"Nicolau, Dan V.; Lard, Mercy; Korten, Till; Delft, Falco C. M. J. M. van; Persson, Malin; Bengtsson, Elina; MÃ¥nsson, Alf; Diez, Stefan; Linke, Heiner; Nicolau, Dan V.",Parallel computation with molecular-motor-propelled agents in nanofabricated networks,Proceedings of the National Academy of Sciences,March,2016,https://www.pnas.org/content/113/10/2591,"The combinatorial nature of many important mathematical problems, including nondeterministic-polynomial-time (NP)-complete problems, places a severe limitation on the problem size that can be solved with conventional, sequentially operating electronic computers. There have been significant efforts in conceiving parallel-computation approaches in the past, for example: DNA computation, quantum computation, and microfluidics-based computation. However, these approaches have not proven, so far, to be scalable and practical from a fabrication and operational perspective. Here, we report the foundations of an alternative parallel-computation system in which a given combinatorial problem is encoded into a graphical, modular network that is embedded in a nanofabricated planar device. Exploring the network in a parallel fashion using a large number of independent, molecular-motor-propelled agents then solves the mathematical problem. This approach uses orders of magnitude less energy than conventional computers, thus addressing issues related to power consumption and heat dissipation. We provide a proof-of-concept demonstration of such a device by solving, in a parallel fashion, the small instance \{2, 5, 9\} of the subset sum problem, which is a benchmark NP-complete problem. Finally, we discuss the technical advances necessary to make our system scalable with presently available technology.",,"NP complete, biocomputation, molecular motors, nanotechnology, parallel computing",,
7,noauthor_effective_nodate,,Effective Multi-Label Active Learning for Text Classification,,,,http://www.cs.cmu.edu/~bishan/papers/multi-label-active-learning-kdd09.pdf,,,,,
7,noauthor_semantic_2015,,Semantic expansion using word embedding clustering and convolutional neural network for improving short text classification,,,2015,,"Text classification can help users to effectively handle and exploit useful information hidden in large- scale documents. However, the sparsity of data and the semantic sensitivity to context often hinder the  classification performance of short texts. In order to overcome the weakness, we propose a unified framework to expand short texts based on word embedding clustering and convolutional neural network (CNN). Empirically, the semantically related words are usually close to each other in embedding spaces. Thus, we first discover semantic cliques via fast clustering. Then, by using additive composition over word embeddings from context with variable window width, the representations of multi-scale semantic units1 in short texts are computed. In embedding spaces, the restricted nearest word embeddings (NWEs)2 of the semantic units are chosen to constitute expanded matrices, where the semantic cliques are used as supervision information. Finally, for a short text, the projected matrix3 and expanded matrices are combined and fed into CNN in parallel. Experimental results on two open benchmarks validate the effectiveness of the proposed method.",,,,
7,noauthor_visualization_2018,,Visualization and the Digital Humanities,IEEE Computer Graphics and Applications,December,2018,https://ieeexplore.ieee.org/abstract/document/8617736,"A. J. Bradley et al., Visualization and the Digital Humanities:"," in IEEE Computer Graphics and Applications, vol. 38, no. 6, pp. 26-38, 1 Nov.-Dec. 2018, doi: 10.1109/MCG.2018.2878900. Abstract: For the past two years, researchers from the visualization community and the digital humanities have come together at the IEEE VIS conference to discuss how both disciplines can work together to push research goals in their respective disciplines. In this paper, we present our experiences as a result of this collaboration. keywords: \{Data visualization;Conferences;Collaboration;Scholarships;Organizations;Complexity theory;Visualization\}, URL: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=\&arnumber=8617736\&isnumber=8617719",,,
7,nogueira_passage_2020,"Nogueira, Rodrigo; Cho, Kyunghyun",Passage Re-ranking with BERT,arXiv:1901.04085 [cs],April,2020,http://arxiv.org/abs/1901.04085,"Recently, neural models pretrained on a language modeling task, such as ELMo (Peters et al., 2017), OpenAI GPT (Radford et al., 2018), and BERT (Devlin et al., 2018), have achieved impressive results on various natural language processing tasks such as question-answering and natural language inference. In this paper, we describe a simple re-implementation of BERT for query-based passage re-ranking. Our system is the state of the art on the TREC-CAR dataset and the top entry in the leaderboard of the MS MARCO passage retrieval task, outperforming the previous state of the art by 27\% (relative) in MRR@10. The code to reproduce our results is available at https://github.com/nyu-dl/dl4marco-bert",,"Computer Science - Computation and Language, Computer Science - Information Retrieval, Computer Science - Machine Learning",,
7,oakden-rayner_exploring_2020,"Oakden-Rayner, Luke",Exploring Large-scale Public Medical Image Datasets,Academic Radiology,January,2020,https://linkinghub.elsevier.com/retrieve/pii/S107663321930488X,,,,,
7,oele_simple_nodate,"Oele, Dieke; van Noord, Gertjan",Simple Embedding-Based Word Sense Disambiguation,,,,,"We present a simple knowledge-based WSD method that uses word and sense embeddings to compute the similarity between the gloss of a sense and the context of the word. Our method is inspired by the Lesk algorithm as it exploits both the context of the words and the deï¬Ånitions of the senses. It only requires large unlabeled corpora and a sense inventory such as WordNet, and therefore does not rely on annotated data. We explore whether additional extensions to Lesk are compatible with our method. The results of our experiments show that by lexically extending the amount of words in the gloss and context, although it works well for other implementations of Lesk, harms our method. Using a lexical selection method on the context words, on the other hand, improves it. The combination of our method with lexical selection enables our method to outperform state-of the art knowledgebased systems.",,,,
7,onal_neural_2018,"Onal, Kezban Dilek; Zhang, Ye; Altingovde, Ismail Sengor; Rahman, Md Mustafizur; Karagoz, Pinar; Braylan, Alex; Dang, Brandon; Chang, Heng-Lu; Kim, Henna; McNamara, Quinten; Angert, Aaron; Banner, Edward; Khetan, Vivek; McDonnell, Tyler; Nguyen, An Thanh; Xu, Dan; Wallace, Byron C.; de Rijke, Maarten; Lease, Matthew",Neural information retrieval: at the end of the early years,Information Retrieval Journal,June,2018,https://doi.org/10.1007/s10791-017-9321-y,"A recent â€œthird waveâ€ù of neural network (NN) approaches now delivers state-of-the-art performance in many machine learning tasks, spanning speech recognition, computer vision, and natural language processing. Because these modern NNs often comprise multiple interconnected layers, work in this area is often referred to as deep learning. Recent years have witnessed an explosive growth of research into NN-based approaches to information retrieval (IR). A significant body of work has now been created. In this paper, we survey the current landscape of Neural IR research, paying special attention to the use of learned distributed representations of textual units. We highlight the successes of neural IR thus far, catalog obstacles to its wider adoption, and suggest potentially promising directions for future research.",,"Deep learning, Distributed representation, Neural network, Recurrent neural network, Search engine, Semantic compositionality, Semantic matching, Word embedding",,
7,ostendorff_enriching_2019,"Ostendorff, Malte; Bourgonje, Peter; Berger, Maria; Moreno-Schneider, Julian; Rehm, Georg; Gipp, Bela",Enriching BERT with Knowledge Graph Embeddings for Document Classification,arXiv:1909.08402 [cs],September,2019,http://arxiv.org/abs/1909.08402,"In this paper we focus on the classiï¬Åcation of books using short descriptive texts (cover blurbs) and additional metadata. Building upon BERT, a deep neural language model, we demonstrate how to combine text representations with metadata and knowledge graph embeddings, which encode author information. Compared to the standard BERT approach we achieve considerably better results for the classiï¬Åcation task. For a more coarse-grained classiï¬Åcation using eight labels we achieve an F1score of 87.20, while a detailed classiï¬Åcation using 343 labels yields an F1-score of 64.70. We make the source code and trained models of our experiments publicly available.",,"Computer Science - Computation and Language, Computer Science - Information Retrieval, Computer Science - Machine Learning",,
7,ozyurt_bio-answerfinder_2020,"Ozyurt, Ibrahim Burak; Bandrowski, Anita; Grethe, Jeffrey S.",Bio-AnswerFinder: a system to find answers to questions from biomedical texts,Database,January,2020,https://academic.oup.com/database/article/doi/10.1093/database/baz137/5700339,Abstract.  The ever accelerating pace of biomedical research results in corresponding acceleration in the volume of biomedical literature created. Since new res,,Biomedical Text,,
7,padigela_investigating_2019,"Padigela, Harshith; Zamani, Hamed; Croft, W. Bruce",Investigating the Successes and Failures of BERT for Passage Re-Ranking,arXiv:1905.01758 [cs],May,2019,http://arxiv.org/abs/1905.01758,"The bidirectional encoder representations from transformers (BERT) model has recently advanced the state-of-the-art in passage re-ranking. In this paper, we analyze the results produced by a fine-tuned BERT model to better understand the reasons behind such substantial improvements. To this aim, we focus on the MS MARCO passage re-ranking dataset and provide potential reasons for the successes and failures of BERT for retrieval. In more detail, we empirically study a set of hypotheses and provide additional analysis to explain the successful performance of BERT.",,"Computer Science - Computation and Language, Computer Science - Information Retrieval",,
7,pal_debunking_2019,"Pal, Anjan; Chua, Alton Y. K.; Hoe-Lian Goh, Dion",Debunking rumors on social media: The use of denials,Computers in Human Behavior,July,2019,http://www.sciencedirect.com/science/article/pii/S0747563219300809,"The literature currently lacks an understanding of how denials can be crafted to effectively debunk rumors on social media. Underpinned by the theory of planned behavior, this research develops denials by incorporating salient beliefs to enhance users' likelihood to share such messages. Two related studies were conducted. The first was a survey of 276 participants to identify salient beliefs that could be incorporated to develop rumor denials. The following salient beliefs were identified in the survey: (i) Sharing denials helps to spread the truth; (ii) Friends and the online community encourage the behavior of sharing denials; and (iii) Source credibility of denials encourages sharing. From among the pool of survey participants, 206 took part in a second study that employed an experiment to measure the efficacy of the developed denials. The experiment revealed that denials incorporating all the salient beliefs had the greatest potential to influence users' likelihood of sharing. With a theory-driven approach to develop denials, this research offers insights to practitioners such as social media managers and website authorities on ways to debunk rumors.",,"Denial, Intention to share, Refutation, Rumor, Salient belief, Social media",,
7,pang_deeprank:_2017,"Pang, Liang; Lan, Yanyan; Guo, Jiafeng; Xu, Jun; Xu, Jingfang; Cheng, Xueqi",DeepRank: A New Deep Architecture for Relevance Ranking in Information Retrieval,Proceedings of the 2017 ACM on Conference on Information and Knowledge Management - CIKM '17,,2017,http://arxiv.org/abs/1710.05649,"This paper concerns a deep learning approach to relevance ranking in information retrieval (IR). Existing deep IR models such as DSSM and CDSSM directly apply neural networks to generate ranking scores, without explicit understandings of the relevance. According to the human judgement process, a relevance label is generated by the following three steps: 1) relevant locations are detected, 2) local relevances are determined, 3) local relevances are aggregated to output the relevance label. In this paper we propose a new deep learning architecture, namely DeepRank, to simulate the above human judgment process. Firstly, a detection strategy is designed to extract the relevant contexts. Then, a measure network is applied to determine the local relevances by utilizing a convolutional neural network (CNN) or two-dimensional gated recurrent units (2D-GRU). Finally, an aggregation network with sequential integration and term gating mechanism is used to produce a global relevance score. DeepRank well captures important IR characteristics, including exact/semantic matching signals, proximity heuristics, query term importance, and diverse relevance requirement. Experiments on both benchmark LETOR dataset and a large scale clickthrough data show that DeepRank can significantly outperform learning to ranking methods, and existing deep learning methods.",,Computer Science - Information Retrieval,,
7,pang_study_2016,"Pang, Liang; Lan, Yanyan; Guo, Jiafeng; Xu, Jun; Cheng, Xueqi",A Study of MatchPyramid Models on Ad-hoc Retrieval,arXiv:1606.04648 [cs],June,2016,http://arxiv.org/abs/1606.04648,"Deep neural networks have been successfully applied to many text matching tasks, such as paraphrase identification, question answering, and machine translation. Although ad-hoc retrieval can also be formalized as a text matching task, few deep models have been tested on it. In this paper, we study a state-of-the-art deep matching model, namely MatchPyramid, on the ad-hoc retrieval task. The MatchPyramid model employs a convolutional neural network over the interactions between query and document to produce the matching score. We conducted extensive experiments to study the impact of different pooling sizes, interaction functions and kernel sizes on the retrieval performance. Finally, we show that the MatchPyramid models can significantly outperform several recently introduced deep matching models on the retrieval task, but still cannot compete with the traditional retrieval models, such as BM25 and language models.",,Computer Science - Information Retrieval,,
7,pannucci_identifying_2010,"Pannucci, Christopher J.; Wilkins, Edwin G.",Identifying and Avoiding Bias in Research,Plastic and reconstructive surgery,August,2010,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2917255/,"This narrative review provides an overview on the topic of bias as part of Plastic and Reconstructive Surgery's series of articles on evidence-based medicine. Bias can occur in the planning, data collection, analysis, and publication phases of research. Understanding research bias allows readers to critically and independently review the scientific literature and avoid treatments which are suboptimal or potentially harmful. A thorough understanding of bias and how it affects study results is essential for the practice of evidence-based medicine.",,,,
7,park_visualhypertuner_nodate,"Park, Heungseok; Kim, Jinwoong; Kim, Minkyu; Kim, Ji-Hoon; Choo, Jaegul; Ha, Jung-Woo; Sung, Nako",VisualHyperTuner: Visual Analytics for User-Driven Hyperparamter Tuning of Deep Neural Networks,,,,,"Deep learning researchers and practitioners often struggle to ï¬Ånd an optimal set of hyperparameters to maximize model performance due to a large combinatorial search space. Existing hyperparameter optimization methods, which mostly rely on a fully automatic approach, only made a limited success, leaving room for human intervention via a visual analytic approach. In response, we propose VisualHyperTuner, a web-based visual analytics system that supports user-driven, in-depth analysis and hyperparameter tuning processes in a model-agnostic environment. VisualHyperTuner utilizes a new approach to effectively control hyperparameter optimization through an iterative, interactive tuning procedure allowing users to ï¬Åne-tune the optimal hyperparameters based on their prior knowledge from the given results. By tightly integrating multiple coordinated views, users can explore the obtained results and get insights into the optimization behavior. To demonstrate the utility of VisualHyperTuner, we present a usage scenario with real-world examples.",,"Hyper parameter tuning, Model-Agnostic Meta-Learning, VisualHyperTuner, iterative, interactive tuning procedure",,
7,patel_humanmachine_2019,"Patel, Bhavik N.; Rosenberg, Louis; Willcox, Gregg; Baltaxe, David; Lyons, Mimi; Irvin, Jeremy; Rajpurkar, Pranav; Amrhein, Timothy; Gupta, Rajan; Halabi, Safwan; Langlotz, Curtis; Lo, Edward; Mammarappallil, Joseph; Mariano, A. J.; Riley, Geoffrey; Seekins, Jayne; Shen, Luyao; Zucke, Evan; Lungren, Matthew P.",Humanâ€“machine partnership with artificial intelligence for chest radiograph diagnosis,Nature Publishing Group,November,2019,https://www.nature.com/articles/s41746-019-0189-7,"Human-in-the-loop (HITL) AI may enable an ideal symbiosis of human experts and AI models, harnessing the advantages of both while at the same time overcoming their respective limitations. The purpose of this study was to investigate a novel collective intelligence technology designed to amplify the diagnostic accuracy of networked human groups by forming real-time systems modeled on biological swarms. Using small groups of radiologists, the swarm-based technology was applied to the diagnosis of pneumonia on chest radiographs and compared against human experts alone, as well as two state-of-the-art deep learning AI models. Our work demonstrates that both the swarm-based technology and deep-learning technology achieved superior diagnostic accuracy than the human experts alone. Our work further demonstrates that when used in combination, the swarmbased technology and deep-learning technology outperformed either method alone. The superior diagnostic accuracy of the combined HITL AI solution compared to radiologists and AI alone has broad implications for the surging clinical AI deployment and implementation strategies in future practice.",,,,
7,peng_negbio_2017,"Peng, Yifan; Wang, Xiaosong; Lu, Le; Bagheri, Mohammadhadi; Summers, Ronald; Lu, Zhiyong",NegBio: a high-performance tool for negation and uncertainty detection in radiology reports,arXiv:1712.05898 [cs],December,2017,http://arxiv.org/abs/1712.05898,"Negative and uncertain medical ï¬Åndings are frequent in radiology reports, but discriminating them from positive ï¬Åndings remains challenging for information extraction. Here, we propose a new algorithm, NegBio, to detect negative and uncertain ï¬Åndings in radiology reports. Unlike previous rule-based methods, NegBio utilizes patterns on universal dependencies to identify the scope of triggers that are indicative of negation or uncertainty. We evaluated NegBio on four datasets, including two public benchmarking corpora of radiology reports, a new radiology corpus that we annotated for this work, and a public corpus of general clinical texts. Evaluation on these datasets demonstrates that NegBio is highly accurate for detecting negative and uncertain ï¬Åndings and compares favorably to a widely-used state-of-the-art system NegEx (an average of 9.5\% improvement in precision and 5.1\% in F1-score).",,Computer Science - Computation and Language,,
7,perotte_diagnosis_2014,"Perotte, Adler; Pivovarov, Rimma; Natarajan, Karthik; Weiskopf, Nicole; Wood, Frank; Elhadad, NoÃ©mie",Diagnosis code assignment: models and evaluation metrics,Journal of the American Medical Informatics Association,March,2014,https://academic.oup.com/jamia/article-lookup/doi/10.1136/amiajnl-2013-002159,,,,,
7,peters_deep_2018,"Peters, Matthew E.; Neumann, Mark; Iyyer, Mohit; Gardner, Matt; Clark, Christopher; Lee, Kenton; Zettlemoyer, Luke",Deep contextualized word representations,arXiv:1802.05365 [cs],February,2018,http://arxiv.org/abs/1802.05365,"We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",,Computer Science - Computation and Language,,
7,peters_elmo:_2018,"Peters, Matthew E.; Neumann, Mark; Iyyer, Mohit; Gardner, Matt; Clark, Christopher; Lee, Kenton; Zettlemoyer, Luke",ELMo: Deep contextualized word representations,arXiv:1802.05365 [cs],February,2018,http://arxiv.org/abs/1802.05365,"We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",,Computer Science - Computation and Language,,
7,plaza_resolving_2012,"Plaza, Laura; Stevenson, Mark; DÃ_az, Alberto",Resolving ambiguity in biomedical text to improve summarization,Information Processing \& Management,July,2012,http://www.sciencedirect.com/science/article/pii/S0306457311001099,Access to the vast body of research literature that is now available on biomedicine and related fields can be improved with automatic summarization. This paper describes a summarization system for the biomedical domain that represents documents as graphs formed from concepts and relations in the UMLS Metathesaurus. This system has to deal with the ambiguities that occur in biomedical documents. We describe a variety of strategies that make use of MetaMap and Word Sense Disambiguation (WSD) to accurately map biomedical documents onto UMLS Metathesaurus concepts. Evaluation is carried out using a collection of 150 biomedical scientific articles from the BioMed Central corpus. We find that using WSD improves the quality of the summaries generated.,,"Biomedical summarization, Graph-based, MetaMap, UMLS, Unified medical language system, WSD, Word sense disambiguation, summarization",,
7,pons_natural_2016,"Pons, Ewoud; Braun, Loes M. M.; Hunink, M. G. Myriam; Kors, Jan A.",Natural Language Processing in Radiology: A Systematic Review,Radiology,May,2016,http://pubs.rsna.org/doi/10.1148/radiol.16142770,,,,,
7,ponzetto_knowledge_2007,"Ponzetto, Simone Paolo; Strube, Michael",Knowledge Derived from Wikipedia for Computing Semantic Relatedness,J. Artif. Int. Res.,October,2007,http://dl.acm.org/citation.cfm?id=1622637.1622642,"Wikipedia provides a semantic network for computing semantic relatedness in a more structured fashion than a search engine and with more coverage than WordNet. We present experiments on using Wikipedia for computing semantic relatedness and compare it to WordNet on various benchmarking datasets. Existing relatedness measures perform better using Wikipedia than a baseline given by Google counts, and we show that Wikipedia outperforms WordNet on some datasets. We also address the question whether and how Wikipedia can be integrated into NLP applications as a knowledge base. Including Wikipedia improves the performance of a machine learning based coreference resolution system, indicating that it represents a valuable resource for NLP applications. Finally, we show that our method can be easily used for languages other than English by computing semantic relatedness for a German dataset.",,,,
7,putha_can_2019,"Putha, Preetham; Tadepalli, Manoj; Reddy, Bhargava; Raj, Tarun; Chiramal, Justy Antony; Govil, Shalini; Sinha, Namita; Manjunath, K. S.; Reddivari, Sundeep; Jagirdar, Ammar; Rao, Pooja; Warier, Prashant",Can Artificial Intelligence Reliably Report Chest X-Rays?: Radiologist Validation of an Algorithm trained on 2.3 Million X-Rays,arXiv:1807.07455 [cs],June,2019,http://arxiv.org/abs/1807.07455,"Background: Chest X-rays are the most commonly performed, cost-effective diagnostic imaging tests ordered by physicians. A clinically validated AI system that can reliably separate normals from abnormals can be invaluble particularly in low-resource settings. The aim of this study was to develop and validate a deep learning system to detect various abnormalities seen on a chest X-ray. Methods: A deep learning system was trained on 2.3 million chest X-rays and their corresponding radiology reports to identify various abnormalities seen on a Chest X-ray. The system was tested against - 1. A three-radiologist majority on an independent, retrospectively collected set of 2000 X-rays(CQ2000) 2. Radiologist reports on a separate validation set of 100,000 scans(CQ100k). The primary accuracy measure was area under the ROC curve (AUC), estimated separately for each abnormality and for normal versus abnormal scans. Results: On the CQ2000 dataset, the deep learning system demonstrated an AUC of 0.92(CI 0.91-0.94) for detection of abnormal scans, and AUC(CI) of 0.96(0.94-0.98), 0.96(0.94-0.98), 0.95(0.87-1), 0.95(0.92-0.98), 0.93(0.90-0.96), 0.89(0.83-0.94), 0.91(0.87-0.96), 0.94(0.93-0.96), 0.98(0.97-1) for the detection of blunted costophrenic angle, cardiomegaly, cavity, consolidation, fibrosis, hilar enlargement, nodule, opacity and pleural effusion. The AUCs were similar on the larger CQ100k dataset except for detecting normals where the AUC was 0.86(0.85-0.86). Interpretation: Our study demonstrates that a deep learning algorithm trained on a large, well-labelled dataset can accurately detect multiple abnormalities on chest X-rays. As these systems improve in accuracy, applying deep learning to widen the reach of chest X-ray interpretation and improve reporting efficiency will add tremendous value in radiology workflows and public health screenings globally.",,Computer Science - Computer Vision and Pattern Recognition,,
7,qiang_short_2020,"Qiang, Jipeng; Qian, Zhenyu; Li, Yun; Yuan, Yunhao; Wu, Xindong","Short Text Topic Modeling Techniques, Applications, and Performance: A Survey",IEEE Transactions on Knowledge and Data Engineering,,2020,,"Analyzing short texts infers discriminative and coherent latent topics that is a critical and fundamental task since many real-world applications require semantic understanding of short texts. Traditional long text topic modeling algorithms (e.g., PLSA and LDA) based on word co-occurrences cannot solve this problem very well since only very limited word co-occurrence information is available in short texts. Therefore, short text topic modeling has already attracted much attention from the machine learning research community in recent years, which aims at overcoming the problem of sparseness in short texts. In this survey, we conduct a comprehensive review of various short text topic modeling techniques proposed in the literature. We present three categories of methods based on Dirichlet multinomial mixture, global word co-occurrences, and self-aggregation, with example of representative approaches in each category and analysis of their performance on various tasks. We develop the first comprehensive open-source library, called STTM, for use in Java that integrates all surveyed algorithms within a unified interface, benchmark datasets, to facilitate the expansion of new methods in this research field. Finally, we evaluate these state-of-the-art methods on many real-world datasets and compare their performance against one another and versus long text topic modeling algorithm.",,"Libraries, Metadata, Semantics, Short text, Short text topic modeling, Sparseness, Tagging, Task analysis, Topic modeling, Twitter, Vocabulary",,
7,radev_centroid-based_nodate,"Radev, Dragomir R.; Jing, Hongyan; Budzikowska, Malgorzata","Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies",,,,,"We present a multi-document summarizer, called MEAD, which generates summaries using cluster centroids produced by a topic detection and tracking system. We also describe two new techniques, based on sentence utility and subsumption, which we have applied to the evaluation of both single and multiple document summaries. Finally, we describe two user studies that test our models of multi-document summarization.",,,,
7,radford_language_nodate,"Radford, Alec; Wu, Jeffrey; Child, Rewon; Luan, David; Amodei, Dario; Sutskever, Ilya",Language Models are Unsupervised Multitask Learners,,,,https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf,"Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspeciï¬Åc datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underï¬Åts WebText. Samples from the model reï¬‚ect these improvements and contain coherent paragraphs of text. These ï¬Åndings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.",,,,
7,radford_language_nodate,"Radford, Alec; Wu, Jeffrey; Child, Rewon; Luan, David; Amodei, Dario; Sutskever, Ilya",Language Models are Unsupervised Multitask Learners,,,,,"Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspeciï¬Åc datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underï¬Åts WebText. Samples from the model reï¬‚ect these improvements and contain coherent paragraphs of text. These ï¬Åndings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.",,,,
7,rael_exploring_nodate,"Raï¬€el, Colin; Narang, Sharan; Shazeer, Noam; Matena, Michael; Roberts, Adam; Lee, Katherine; Zhou, Yanqi; Li, Wei; Liu, Peter J.",Exploring the Limits of Transfer Learning with a Uniï¬Åed Text-to-Text Transformer,,,,,,,unread,,
7,raffel_exploring_2019,"Raffel, Colin; Shazeer, Noam; Roberts, Adam; Lee, Katherine; Narang, Sharan; Matena, Michael; Zhou, Yanqi; Li, Wei; Liu, Peter J.",Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,"arXiv:1910.10683 [cs, stat]",October,2019,http://arxiv.org/abs/1910.10683,"Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new Colossal Clean Crawled Corpus""", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP
7,raffel_exploring_2020,"Raffel, Colin; Shazeer, Noam; Roberts, Adam; Lee, Katherine; Narang, Sharan; Matena, Michael; Zhou, Yanqi; Li, Wei; Liu, Peter J.",Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,"arXiv:1910.10683 [cs, stat]",July,2020,http://arxiv.org/abs/1910.10683,"Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.",,"Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning",,
7,rahimi_wikiumls_2020,"Rahimi, Afshin; Baldwin, Timothy; Verspoor, Karin",WikiUMLS: Aligning UMLS to Wikipedia via Cross-lingual Neural Ranking,arXiv:2005.01281 [cs],May,2020,http://arxiv.org/abs/2005.01281,"We present our work on aligning the Unified Medical Language System (UMLS) to Wikipedia, to facilitate manual alignment of the two resources. We propose a cross-lingual neural reranking model to match a UMLS concept with a Wikipedia page, which achieves a recall@1 of 71\%, a substantial improvement of 20\% over word- and char-level BM25, enabling manual alignment with minimal effort. We release our resources, including ranked Wikipedia pages for 700k UMLS concepts, and WikiUMLS, a dataset for training and evaluation of alignment models between UMLS and Wikipedia. This will provide easier access to Wikipedia for health professionals, patients, and NLP systems, including in multilingual settings.",,Computer Science - Computation and Language,,
7,rajpurkar_squad:_2016,"Rajpurkar, Pranav; Zhang, Jian; Lopyrev, Konstantin; Liang, Percy","SQuAD: 100,000+ Questions for Machine Comprehension of Text",arXiv:1606.05250 [cs],June,2016,http://arxiv.org/abs/1606.05250,"We present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0\%, a significant improvement over a simple baseline (20\%). However, human performance (86.8\%) is much higher, indicating that the dataset presents a good challenge problem for future research. The dataset is freely available at https://stanford-qa.com",,Computer Science - Computation and Language,,
7,ramanujan_whats_2020,"Ramanujan, Vivek; Wortsman, Mitchell; Kembhavi, Aniruddha; Farhadi, Ali; Rastegari, Mohammad",What's Hidden in a Randomly Weighted Neural Network?,arXiv:1911.13299 [cs],March,2020,http://arxiv.org/abs/1911.13299,"Training a neural network is synonymous with learning the values of the weights. In contrast, we demonstrate that randomly weighted neural networks contain subnetworks which achieve impressive performance without ever modifying the weight values. Hidden in a randomly weighted Wide ResNet-50 [32] we ï¬Ånd a subnetwork (with random weights) that is smaller than, but matches the performance of a ResNet-34 [9] trained on ImageNet [4]. Not only do these â€œuntrained subnetworksâ€ù exist, but we provide an algorithm to effectively ï¬Ånd them. We empirically show that as randomly weighted neural networks with ï¬Åxed weights grow wider and deeper, an â€œuntrained subnetworkâ€ù approaches a network with learned weights in accuracy. Our code and pretrained models are available at: https://github.com/allenai/hidden-networks.",,"Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning",,
7,ratinov_local_nodate,"Ratinov, Lev; Roth, Dan; Downey, Doug; Anderson, Mike",Local and Global Algorithms for Disambiguation to Wikipedia âˆ—,,,,,"Disambiguating concepts and entities in a context sensitive way is a fundamental problem in natural language processing. The comprehensiveness of Wikipedia has made the online encyclopedia an increasingly popular target for disambiguation. Disambiguation to Wikipedia is similar to a traditional Word Sense Disambiguation task, but distinct in that the Wikipedia link structure provides additional information about which disambiguations are compatible. In this work we analyze approaches that utilize this information to arrive at coherent sets of disambiguations for a given document (which we call â€œglobalâ€ù approaches), and compare them to more traditional (local) approaches. We show that previous approaches for global disambiguation can be improved, but even then the local disambiguation provides a baseline which is very hard to beat.",,,,
7,raza_taxonomy_2019,"Raza, Muhammad Ahsan; Mokhtar, Rahmah; Ahmad, Noraziah; Pasha, Maruf; Pasha, Urooj",A Taxonomy and Survey of Semantic Approaches for Query Expansion,IEEE Access,,2019,,"Conventional approaches to query expansion (QE) rely on the integration of an unstructured corpus and probabilistic rules for the extraction of candidate expansion terms. These methods do not consider search query semantics, thereby resulting in ineffective retrieval of information. The semantic approaches for QE overcome this limitation, whereby a search query is expanded with meaningful terms that accord with user information needs. This paper surveys recent approaches to semantic QE that employ different models and strategies and leverages various knowledge structures. We organize these approaches into a taxonomy that includes linguistic methods, ontology-based methods, and mixed-mode methods. We also discuss the strengths and limitations of each type of semantic QE method. In addition, we evaluate various semantic QE approaches in terms of knowledge structure utilization, corpus collection, baseline model adaptation, and retrieval performance. Finally, future directions in exploiting personalized social information and multiple ontologies for semantic QE are suggested.",,"Information retrieval, Linguistics, Ontologies, Semantics, Taxonomy, Thesauri, Vocabulary, baseline model adaptation, candidate expansion terms, corpus collection, document handling, knowledge structure utilization, linguistic methods, mixed-mode methods, morphological expansion, ontologies (artificial intelligence), ontology, ontology-based methods, probabilistic rules, query expansion, query formulation, query processing, retrieval performance, search query, semantic QE approaches, semantic query expansion, taxonomy, unstructured corpus rules, user information needs",,
7,reimers_sentence-bert_2019,"Reimers, Nils; Gurevych, Iryna",Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks,arXiv:1908.10084 [cs],August,2019,http://arxiv.org/abs/1908.10084,"BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations ({\textasciitilde}65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering.",,Computer Science - Computation and Language,,
7,reimers_sentence-bert:_2019,"Reimers, Nils; Gurevych, Iryna",Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks,arXiv:1908.10084 [cs],August,2019,http://arxiv.org/abs/1908.10084,"BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations ({\textasciitilde}65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.",,Computer Science - Computation and Language,,
7,rong_wevi:_2014,"Rong, Xin",wevi: word2vec Parameter Learning Explained,arXiv:1411.2738 [cs],November,2014,http://arxiv.org/abs/1411.2738,"The word2vec model and application by Mikolov et al. have attracted a great amount of attention in recent two years. The vector representations of words learned by word2vec models have been shown to carry semantic meanings and are useful in various NLP tasks. As an increasing number of researchers would like to experiment with word2vec or similar techniques, I notice that there lacks a material that comprehensively explains the parameter learning process of word embedding models in details, thus preventing researchers that are non-experts in neural networks from understanding the working mechanism of such models. This note provides detailed derivations and explanations of the parameter update equations of the word2vec models, including the original continuous bag-of-word (CBOW) and skip-gram (SG) models, as well as advanced optimization techniques, including hierarchical softmax and negative sampling. Intuitive interpretations of the gradient equations are also provided alongside mathematical derivations. In the appendix, a review on the basics of neuron networks and backpropagation is provided. I also created an interactive demo, wevi, to facilitate the intuitive understanding of the model.",,Computer Science - Computation and Language,,
7,ross_right_2017,"Ross, Andrew Slavin; Hughes, Michael C.; Doshi-Velez, Finale",Right for the Right Reasons: Training Differentiable Models by Constraining their Explanations,"arXiv:1703.03717 [cs, stat]",March,2017,http://arxiv.org/abs/1703.03717,"Neural networks are among the most accurate supervised learning methods in use today, but their opacity makes them difficult to trust in critical applications, especially when conditions in training differ from those in test. Recent work on explanations for black-box models has produced tools (e.g. LIME) to show the implicit rules behind predictions, which can help us identify when models are right for the wrong reasons. However, these methods do not scale to explaining entire datasets and cannot correct the problems they reveal. We introduce a method for efficiently explaining and regularizing differentiable models by examining and selectively penalizing their input gradients, which provide a normal to the decision boundary. We apply these penalties both based on expert annotation and in an unsupervised fashion that encourages diverse models with qualitatively different decision boundaries for the same classification problem. On multiple datasets, we show our approach generates faithful explanations and models that generalize much better when conditions differ between training and test.",,"Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning",,
7,ruotsalo_interactive_2018,"Ruotsalo, Tuukka; Peltonen, Jaakko; Eugster, Manuel J. A.; G{\textbackslash}lowacka, Dorota; FlorÃ©en, Patrik; MyllymÃ_ki, Petri; Jacucci, Giulio; Kaski, Samuel",Interactive Intent Modeling for Exploratory Search,ACM Trans. Inf. Syst.,October,2018,http://doi.acm.org/10.1145/3231593,"Exploratory search requires the system to assist the user in comprehending the information space and expressing evolving search intents for iterative exploration and retrieval of information. We introduce interactive intent modeling, a technique that models a userâ€™s evolving search intents and visualizes them as keywords for interaction. The user can provide feedback on the keywords, from which the system learns and visualizes an improved intent estimate and retrieves information. We report experiments comparing variants of a system implementing interactive intent modeling to a control system. Data comprising search logs, interaction logs, essay answers, and questionnaires indicate significant improvements in task performance, information retrieval performance over the session, information comprehension performance, and user experience. The improvements in retrieval effectiveness can be attributed to the intent modeling and the effect on usersâ€™ task performance, breadth of information comprehension, and user experience are shown to be dependent on a richer visualization. Our results demonstrate the utility of combining interactive modeling of search intentions with interactive visualization of the models that can benefit both directing the exploratory search process and making sense of the information space. Our findings can help design personalized systems that support exploratory information seeking and discovery of novel information.",,"Proactive search, user intent modeling",,
7,sakai_query-focused_nodate,"Sakai, Tetsuya",Query-Focused Extractive Summarization based on Deep Learning: Comparison of Similarity Measures for Pseudo Ground Truth Generation,,,,,"Query-focused summarization aims to produce a single, short document that summarizes a set of documents that are relevant to a given query. While some deep learning approaches have recently been applied to solve this task, how to automatically generate reliable ground truth labels for training remains an open problem. In this study, we employ eight existing textual similarity measures to generate ground truth labels at the sentence level given a reference summary. We then feed these diï¬€erent labelled data to deep learning approaches to generate extractive summaries. We use the DUC 2005-2007 benchmark datasets in our experiment. Our study shows that ROUGE-WE2 and ROUGE-SU measures achieved the best ROUGE scores in all deep neural models we employed.",,summarization,,
7,saldanha_evaluating_2016,"Saldanha, Ian J.; Schmid, Christopher H.; Lau, Joseph; Dickersin, Kay; Berlin, Jesse A.; Jap, Jens; Smith, Bryant T.; Carini, Simona; Chan, Wiley; De Bruijn, Berry; Wallace, Byron C.; Hutfless, Susan M.; Sim, Ida; Murad, M. Hassan; Walsh, Sandra A.; Whamond, Elizabeth J.; Li, Tianjing","Evaluating Data Abstraction Assistant, a novel software application for data abstraction during systematic reviews: protocol for a randomized controlled trial",Systematic Reviews,November,2016,https://doi.org/10.1186/s13643-016-0373-7,"Data abstraction, a critical systematic review step, is time-consuming and prone to errors. Current standards for approaches to data abstraction rest on a weak evidence base. We developed the Data Abstraction Assistant (DAA), a novel software application designed to facilitate the abstraction process by allowing users to (1) view study article PDFs juxtaposed to electronic data abstraction forms linked to a data abstraction system, (2) highlight (or â€œpinâ€ù) the location of the text in the PDF, and (3) copy relevant text from the PDF into the form. We describe the design of a randomized controlled trial (RCT) that compares the relative effectiveness of (A) DAA-facilitated single abstraction plus verification by a second person, (B) traditional (non-DAA-facilitated) single abstraction plus verification by a second person, and (C) traditional independent dual abstraction plus adjudication to ascertain the accuracy and efficiency of abstraction.",,,,
7,salles_estimating_2019,"Salles, Arghavan; Awad, Michael; Goldin, Laurel; Krus, Kelsey; Lee, Jin Vivian; Schwabe, Maria T.; Lai, Calvin K.",Estimating Implicit and Explicit Gender Bias Among Health Care Professionals and Surgeons,JAMA network open,July,2019,,"Importance: The Implicit Association Test (IAT) is a validated tool used to measure implicit biases, which are mental associations shaped by one's environment that influence interactions with others. Direct evidence of implicit gender biases about women in medicine has yet not been reported, but existing evidence is suggestive of subtle or hidden biases that affect women in medicine. Objectives: To use data from IATs to assess (1) how health care professionals associate men and women with career and family and (2) how surgeons associate men and women with surgery and family medicine. Design, Setting, and Participants: This data review and cross-sectional study collected data from January 1, 2006, through December 31, 2017, from self-identified health care professionals taking the Gender-Career IAT hosted by Project Implicit to explore bias among self-identified health care professionals. A novel Gender-Specialty IAT was also tested at a national surgical meeting in October 2017. All health care professionals who completed the Gender-Career IAT were eligible for the first analysis. Surgeons of any age, gender, title, and country of origin at the meeting were eligible to participate in the second analysis. Data were analyzed from January 1, 2018, through March 31, 2019. Main Outcomes and Measures: Measure of implicit bias derived from reaction times on the IATs and a measure of explicit bias asked directly to participants. Results: Almost 1 million IAT records from Project Implicit were reviewed, and 131 surgeons (64.9\% men; mean [SD] age, 42.3 [11.5] years) were recruited to complete the Gender-Specialty IAT. Healthcare professionals (nâ€‰=â€‰42â€¯991; 82.0\% women; mean [SD] age, 32.7 [11.8] years) held implicit (mean [SD] D score, 0.41 [0.36]; Cohen dâ€‰=â€‰1.14) and explicit (mean [SD], 1.43 [1.85]; Cohen dâ€‰=â€‰0.77) biases associating men with career and women with family. Similarly, surgeons implicitly (mean [SD] D score, 0.28 [0.37]; Cohen dâ€‰=â€‰0.76) and explicitly (men: mean [SD], 1.27 [0.39]; Cohen dâ€‰=â€‰0.93; women: mean [SD], 0.73 [0.35]; Cohen dâ€‰=â€‰0.53) associated men with surgery and women with family medicine. There was broad evidence of consensus across social groups in implicit and explicit biases with one exception. Women in healthcare (mean [SD], 1.43 [1.86]; Cohen dâ€‰=â€‰0.77) and surgery (mean [SD], 0.73 [0.35]; Cohen dâ€‰=â€‰0.53) were less likely than men to explicitly associate men with career (B coefficient, -0.10; 95\% CI, -0.15 to -0.04; Pâ€‰{\textless}â€‰.001) and surgery (B coefficient, -0.67; 95\% CI, -1.21 to -0.13; Pâ€‰=â€‰.001) and women with family and family medicine. Conclusions and Relevance: The main contribution of this work is an estimate of the extent of implicit gender bias within surgery. On both the Gender-Career IAT and the novel Gender-Specialty IAT, respondents had a tendency to associate men with career and surgery and women with family and family medicine. Awareness of the existence of implicit biases is an important first step toward minimizing their potential effect.",,,,
7,sampson_complementary_2016,"Sampson, Margaret; de Bruijn, Berry; Urquhart, Christine; Shojania, Kaveh",Complementary approaches to searching MEDLINE may be sufficient for updating systematic reviews,Journal of Clinical Epidemiology,October,2016,http://www.sciencedirect.com/science/article/pii/S0895435616300166,"Objectives To maximize the proportion of relevant studies identified for inclusion in systematic reviews (recall), complex time-consuming Boolean searches across multiple databases are common. Although MEDLINE provides excellent coverage of health science evidence, it has proved challenging to achieve high levels of recall through Boolean searches alone. Study Design and Setting Recall of one Boolean search method, the clinical query (CQ), combined with a ranking method, support vector machine (SVM), or PubMed-related articles, was tested against a gold standard of studies added to 6 updated Cochrane reviews and 10 Agency for Healthcare Research and Quality (AHRQ) evidence reviews. For the AHRQ sample, precision and temporal stability were examined for each method. Results Recall of new studies was 0.69 for the CQ, 0.66 for related articles, 0.50 for SVM, 0.91 for the combination of CQ and related articles, and 0.89 for the combination of CQ and SVM. Precision was 0.11 for CQ and related articles combined, and 0.11 for CQ and SVM combined. Related articles showed least stability over time. Conclusions The complementary combination of a Boolean search strategy and a ranking strategy appears to provide a robust method for identifying relevant studies in MEDLINE.",,"Clinical query, Information retrieval, MEDLINE, PubMed similar articles, Searches, Support vector machine, Systematic reviews, Updating",,
7,sanh_distilbert_2020,"Sanh, Victor; Debut, Lysandre; Chaumond, Julien; Wolf, Thomas","DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",arXiv:1910.01108 [cs],February,2020,http://arxiv.org/abs/1910.01108,"As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-theedge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller generalpurpose language representation model, called DistilBERT, which can then be ï¬Ånetuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-speciï¬Åc models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40\%, while retaining 97\% of its language understanding capabilities and being 60\% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.",,Computer Science - Computation and Language,,
7,sarker_data_2018,"Sarker, Abeed; Belousov, Maksim; Friedrichs, Jasper; Hakala, Kai; Kiritchenko, Svetlana; Mehryary, Farrokh; Han, Sifei; Tran, Tung; Rios, Anthony; Kavuluru, Ramakanth; de Bruijn, Berry; Ginter, Filip; Mahata, Debanjan; Mohammad, Saif M.; Nenadic, Goran; Gonzalez-Hernandez, Graciela",Data and systems for medication-related text classification and concept normalization from Twitter: insights from the Social Media Mining for Health (SMM4H)-2017 shared task,Journal of the American Medical Informatics Association,October,2018,https://academic.oup.com/jamia/article/25/10/1274/5113021,AbstractObjective.  We executed the Social Media Mining for Health (SMM4H) 2017 shared tasks to enable the community-driven development and large-scale evaluati,,,,
7,sayfullina_domain_2017,"Sayfullina, Luiza; Malmi, Eric; Liao, Yiping; Jung, Alex",Domain Adaptation for Resume Classification Using Convolutional Neural Networks,arXiv:1707.05576 [cs],July,2017,http://arxiv.org/abs/1707.05576,"We propose a novel method for classifying resume data of job applicants into 27 different job categories using convolutional neural networks. Since resume data is costly and hard to obtain due to its sensitive nature, we use domain adaptation. In particular, we train a classifier on a large number of freely available job description snippets and then use it to classify resume data. We empirically verify a reasonable classification performance of our approach despite having only a small amount of labeled resume data available.",,Computer Science - Computer Vision and Pattern Recognition,,
7,sayfullina_learning_2018,"Sayfullina, Luiza; Malmi, Eric; Kannala, Juho",Learning Representations for Soft Skill Matching,"arXiv:1807.07741 [cs, stat]",July,2018,http://arxiv.org/abs/1807.07741,"Employers actively look for talents having not only specific hard skills but also various soft skills. To analyze the soft skill demands on the job market, it is important to be able to detect soft skill phrases from job advertisements automatically. However, a naive matching of soft skill phrases can lead to false positive matches when a soft skill phrase, such as friendly, is used to describe a company, a team, or another entity, rather than a desired candidate. In this paper, we propose a phrase-matching-based approach which differentiates between soft skill phrases referring to a candidate vs. something else. The disambiguation is formulated as a binary text classification problem where the prediction is made for the potential soft skill based on the context where it occurs. To inform the model about the soft skill for which the prediction is made, we develop several approaches, including soft skill masking and soft skill tagging. We compare several neural network based approaches, including CNN, LSTM and Hierarchical Attention Model. The proposed tagging-based input representation using LSTM achieved the highest recall of 83.92\% on the job dataset when fixing a precision to 95\%.",,"Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning",,
7,schick_exploiting_2020,"Schick, Timo; SchÃ_tze, Hinrich",Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference,arXiv:2001.07676 [cs],April,2020,http://arxiv.org/abs/2001.07676,"Some NLP tasks can be solved in a fully unsupervised fashion by providing a pretrained language model with task descriptions"" in natural language (e.g.", Radford et al., 2019). While this approach underperforms its supervised counterpart, we show in this work that the two ideas can be combined: We introduce Pattern-Exploiting Training (PET), a semi-supervised training procedure that reformulates input examples as cloze-style phrases to help language models understand a given task. These phrases are then used to assign soft labels to a large set of unlabeled examples. Finally
7,schick_its_2020,"Schick, Timo; SchÃ_tze, Hinrich",It's Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners,arXiv:2009.07118 [cs],September,2020,http://arxiv.org/abs/2009.07118,"When scaled to hundreds of billions of parameters, pretrained language models such as GPT-3 (Brown et al., 2020) achieve remarkable few-shot performance on challenging natural language understanding benchmarks. In this work, we show that performance similar to GPT-3 can be obtained with language models whose parameter count is several orders of magnitude smaller. This is achieved by converting textual inputs into cloze questions that contain some form of task description, combined with gradient-based optimization; additionally exploiting unlabeled data gives further improvements. Based on our findings, we identify several key factors required for successful natural language understanding with small language models.",,"Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning",,
7,searle_medcattrainer_2019,"Searle, Thomas; Kraljevic, Zeljko; Bendayan, Rebecca; Bean, Daniel; Dobson, Richard",MedCATTrainer: A Biomedical Free Text Annotation Interface with Active Learning and Research Use Case Specific Customisation,arXiv:1907.07322 [cs.HC],July,2019,https://arxiv.org/abs/1907.07322,"We present MedCATTrainer an interface for building, improving and customising a given Named Entity Recognition and Linking (NER+L) model for biomedical domain text. NER+L is often used as a first step in deriving value from clinical text. Collecting labelled data for training models is difficult due to the need for specialist domain knowledge. MedCATTrainer offers an interactive web-interface to inspect and improve recognised entities from an underlying NER+L model via active learning. Secondary use of data for clinical research often has task and context specific criteria. MedCATTrainer provides a further interface to define and collect supervised learning training data for researcher specific use cases. Initial results suggest our approach allows for efficient and accurate collection of research use case specific training data.",,,,
7,sedlmair_design_2012,"Sedlmair, M.; Meyer, M.; Munzner, T.",Design Study Methodology: Reflections from the Trenches and the Stacks,IEEE Transactions on Visualization and Computer Graphics,December,2012,,"Design studies are an increasingly popular form of problem-driven visualization research, yet there is little guidance available about how to do them effectively. In this paper we reflect on our combined experience of conducting twenty-one design studies, as well as reading and reviewing many more, and on an extensive literature review of other field work methods and methodologies. Based on this foundation we provide definitions, propose a methodological framework, and provide practical guidance for conducting design studies. We define a design study as a project in which visualization researchers analyze a specific real-world problem faced by domain experts, design a visualization system that supports solving this problem, validate the design, and reflect about lessons learned in order to refine visualization design guidelines. We characterize two axes - a task clarity axis from fuzzy to crisp and an information location axis from the domain expert's head to the computer - and use these axes to reason about design study contributions, their suitability, and uniqueness from other approaches. The proposed methodological framework consists of 9 stages: learn, winnow, cast, discover, design, implement, deploy, reflect, and write. For each stage we provide practical guidance and outline potential pitfalls. We also conducted an extensive literature survey of related methodological approaches that involve a significant amount of qualitative field work, and compare design study methodology to that of ethnography, grounded theory, and action research.",,"Algorithm design and analysis, Collaboration, Data visualization, Design methodology, Design study, Logic gates, Visualization, data visualisation, design study methodology, framework, information location axis, methodology, problem-driven visualization, task clarity axis, visualization, visualization design guideline",,
7,sedlmair_visual_2014,"Sedlmair, M.; Heinzl, C.; Bruckner, S.; Piringer, H.; MÃ¶ller, T.",Visual Parameter Space Analysis: A Conceptual Framework,IEEE Transactions on Visualization and Computer Graphics,December,2014,,"Various case studies in different application domains have shown the great potential of visual parameter space analysis to support validating and using simulation models. In order to guide and systematize research endeavors in this area, we provide a conceptual framework for visual parameter space analysis problems. The framework is based on our own experience and a structured analysis of the visualization literature. It contains three major components: (1) a data flow model that helps to abstractly describe visual parameter space analysis problems independent of their application domain; (2) a set of four navigation strategies of how parameter space analysis can be supported by visualization tools; and (3) a characterization of six analysis tasks. Based on our framework, we analyze and classify the current body of literature, and identify three open research gaps in visual parameter space analysis. The framework and its discussion are meant to support visualization designers and researchers in characterizing parameter space analysis problems and to guide their design and evaluation processes.",,"Analytical models, Biological system modeling, Computational modeling, Data models, Image segmentation, Parameter space analysis, Predictive models, application domains, conceptual framework, data analysis, data flow model, data visualisation, input-output model, literature analysis, navigation strategy, simulation, simulation models, task characterization, visual parameter space analysis, visualization tools",,
7,senarath_evaluating_2020,"Senarath, Yasas; Chan, Jennifer; Purohit, Hemant; Uzuner, Ozlem",Evaluating the Relevance of UMLS Concepts for Public Health Informatics during Disasters using MetaMap,,,2020,https://ist.gmu.edu/~hpurohit/informatics-lab/papers/UMLS-metamap-evaluation-study.pdf,,,,,
7,senel_semantic_2018,"Senel, Lutfi Kerem; Utlu, Ihsan; Yucesoy, Veysel; Koc, Aykut; Cukur, Tolga",Semantic Structure and Interpretability of Word Embeddings,"IEEE/ACM Trans. Audio, Speech and Lang. Proc.",October,2018,https://doi.org/10.1109/TASLP.2018.2837384,"Dense word embeddings, which encode meanings of words to low-dimensional vector spaces, have become very popular in natural language processing NLP research due to their state-of-the-art performances in many NLP tasks. Word embeddings are substantially successful in capturing semantic relations among words, so a meaningful semantic structure must be present in the respective vector spaces. However, in many cases, this semantic structure is broadly and heterogeneously distributed across the embedding dimensions making interpretation of dimensions a big challenge. In this study, we propose a statistical method to uncover the underlying latent semantic structure in the dense word embeddings. To perform our analysis, we introduce a new dataset SEMCAT that contains more than 6500 words semantically grouped under 110 categories. We further propose a method to quantify the interpretability of the word embeddings. The proposed method is a practical alternative to the classical word intrusion test that requires human intervention.",,,,
7,shafieibavani_semantically_2017,"ShafieiBavani, Elaheh; Ebrahimi, Mohammad; Wong, Raymond; Chen, Fang",A Semantically Motivated Approach to Compute ROUGE Scores,arXiv:1710.07441 [cs],October,2017,http://arxiv.org/abs/1710.07441,"ROUGE is one of the first and most widely used evaluation metrics for text summarization. However, its assessment merely relies on surface similarities between peer and model summaries. Consequently, ROUGE is unable to fairly evaluate abstractive summaries including lexical variations and paraphrasing. Exploring the effectiveness of lexical resource-based models to address this issue, we adopt a graph-based algorithm into ROUGE to capture the semantic similarities between peer and model summaries. Our semantically motivated approach computes ROUGE scores based on both lexical and semantic similarities. Experiment results over TAC AESOP datasets indicate that exploiting the lexico-semantic similarity of the words used in summaries would significantly help ROUGE correlate better with human judgments.",,Computer Science - Computation and Language,,
7,shalaby_beyond_2018,"Shalaby, Walid; Zadrozny, Wlodek; Jin, Hongxia",Beyond Word Embeddings: Learning Entity and Concept Representations from Large Scale Knowledge Bases,Information Retrieval Journal,August,2018,http://arxiv.org/abs/1801.00388,"Text representations using neural word embeddings have proven effective in many NLP applications. Recent researches adapt the traditional word embedding models to learn vectors of multiword expressions (concepts/entities). However, these methods are limited to textual knowledge bases (e.g., Wikipedia). In this paper, we propose a novel and simple technique for integrating the knowledge about concepts from two large scale knowledge bases of different structure (Wikipedia and Probase) in order to learn concept representations. We adapt the efficient skip-gram model to seamlessly learn from the knowledge in Wikipedia text and Probase concept graph. We evaluate our concept embedding models on two tasks: (1) analogical reasoning, where we achieve a state-of-the-art performance of 91\% on semantic analogies, (2) concept categorization, where we achieve a state-of-the-art performance on two benchmark datasets achieving categorization accuracy of 100\% on one and 98\% on the other. Additionally, we present a case study to evaluate our model on unsupervised argument type identification for neural semantic parsing. We demonstrate the competitive accuracy of our unsupervised method and its ability to better generalize to out of vocabulary entity mentions compared to the tedious and error prone methods which depend on gazetteers and regular expressions.",,"Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Information Retrieval, Computer Science - Social and Information Networks",,
7,shi_benchmarking_2016,"Shi, Shaohuai; Wang, Qiang; Xu, Pengfei; Chu, Xiaowen",Benchmarking State-of-the-Art Deep Learning Software Tools,arXiv:1608.07249 [cs],August,2016,http://arxiv.org/abs/1608.07249,"Deep learning has been shown as a successful machine learning method for a variety of tasks, and its popularity results in numerous open-source deep learning software tools. Training a deep network is usually a very time-consuming process. To address the computational challenge in deep learning, many tools exploit hardware features such as multi-core CPUs and many-core GPUs to shorten the training time. However, different tools exhibit different features and running performance when training different types of deep networks on different hardware platforms, which makes it difficult for end users to select an appropriate pair of software and hardware. In this paper, we aim to make a comparative study of the state-of-the-art GPU-accelerated deep learning software tools, including Caffe, CNTK, MXNet, TensorFlow, and Torch. We first benchmark the running performance of these tools with three popular types of neural networks on two CPU platforms and three GPU platforms. We then benchmark some distributed versions on multiple GPUs. Our contribution is two-fold. First, for end users of deep learning tools, our benchmarking results can serve as a guide to selecting appropriate hardware platforms and software tools. Second, for software developers of deep learning tools, our in-depth analysis points out possible future directions to further optimize the running performance.",,"Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning",,
7,shi_neural_2020,"Shi, Tian; Keneshloo, Yaser; Ramakrishnan, Naren; Reddy, Chandan K.",Neural Abstractive Text Summarization with Sequence-to-Sequence Models: A Survey,"arXiv:1812.02303 [cs, stat]",January,2020,http://arxiv.org/abs/1812.02303,"In the past few years, neural abstractive text summarization with sequence-to-sequence (seq2seq) models have gained a lot of popularity. Many interesting techniques have been proposed to improve the seq2seq models, making them capable of handling different challenges, such as saliency, ï¬‚uency and human readability, and generate high-quality summaries. Generally speaking, most of these techniques differ in one of these three categories: network structure, parameter inference, and decoding/generation. There are also other concerns, such as efï¬Åciency and parallelism for training a model. In this paper, we provide a comprehensive literature and technical survey on different seq2seq models for abstractive text summarization from viewpoint of network structures, training strategies, and summary generation algorithms. Many models were ï¬Årst proposed for language modeling and generation tasks, such as machine translation, and later applied to abstractive text summarization. Therefore, we also provide a brief review of these models. As part of this survey, we also develop an open source library, namely Neural Abstractive Text Summarizer (NATS) toolkit, for the abstractive text summarization. An extensive set of experiments have been conducted on the widely used CNN/Daily Mail dataset to examine the effectiveness of several different neural network components. Finally, we benchmark two models implemented in NATS on two recently released datasets, i.e., Newsroom and Bytecup.",,"Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning",,
7,shin_classification_2017,"Shin, Bonggun; Chokshi, Falgun H.; Lee, Timothy; Choi, Jinho D.",Classification of Radiology Reports Using Neural Attention Models,arXiv:1708.06828 [cs],August,2017,http://arxiv.org/abs/1708.06828,"The electronic health record (EHR) contains a large amount of multi-dimensional and unstructured clinical data of signiï¬Åcant operational and research value. Distinguished from previous studies, our approach embraces a double-annotated dataset and strays away from obscure â€œblack-boxâ€ù models to comprehensive deep learning models. In this paper, we present a novel neural attention mechanism that not only classiï¬Åes clinically important ï¬Åndings. Speciï¬Åcally, convolutional neural networks (CNN) with attention analysis are used to classify radiology head computed tomography reports based on ï¬Åve categories that radiologists would account for in assessing acute and communicable ï¬Åndings in daily practice. The experiments show that our CNN attention models outperform non-neural models, especially when trained on a larger dataset. Our attention analysis demonstrates the intuition behind the classiï¬Åerâ€™s decision by generating a heatmap that highlights attended terms used by the CNN model; this is valuable when potential downstream medical decisions are to be performed by human experts or the classiï¬Åer information is to be used in cohort construction such as for epidemiological studies.",,"Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Information Retrieval",,
7,shu_small_2018,"Shu, Jun; Xu, Zongben; Meng, Deyu",Small Sample Learning in Big Data Era,"arXiv:1808.04572 [cs, stat]",August,2018,http://arxiv.org/abs/1808.04572,"As a promising area in artificial intelligence, a new learning paradigm, called Small Sample Learning (SSL), has been attracting prominent research attention in the recent years. In this paper, we aim to present a survey to comprehensively introduce the current techniques proposed on this topic. Specifically, current SSL techniques can be mainly divided into two categories. The first category of SSL approaches can be called concept learning""", which emphasizes learning new concepts from only few related observations. The purpose is mainly to simulate human learning behaviors like recognition, generation, imagination," synthesis and analysis. The second category is called ""experience learning"""
7,shui_deep_2019,"Shui, Changjian; Zhou, Fan; GagnÃ©, Christian; Wang, Boyu",Deep Active Learning: Unified and Principled Method for Query and Training,arXiv:1911.09162 [cs.LG],November,2019,https://arxiv.org/abs/1911.09162,"In this paper, we are proposing a unified and principled method for both the querying and training processes in deep batch active learning. We are providing theoretical insights from the intuition of modeling the interactive procedure in active learning as distribution matching, by adopting the Wasserstein distance. As a consequence, we derived a new training loss from the theoretical analysis, which is decomposed into optimizing deep neural network parameters and batch query selection through alternative optimization. In addition, the loss for training a deep neural network is naturally formulated as a min-max optimization problem through leveraging the unlabeled data information. Moreover, the proposed principles also indicate an explicit uncertainty-diversity trade-off in the query batch selection. Finally, we evaluate our proposed method on different benchmarks, consistently showing better empirical performances and a better time-efficient query strategy compared to the baselines.",,,,
7,siddhant_deep_2018,"Siddhant, Aditya; Lipton, Zachary C.",Deep Bayesian Active Learning for Natural Language Processing: Results of a Large-Scale Empirical Study,arXiv:1808.05697,September,2018,https://arxiv.org/abs/1808.05697,"Several recent papers investigate Active Learning (AL) for mitigating the datadependence of deep learning for natural language processing. However, the applicability of AL to real-world problems remains an open question. While in supervised learning, practitioners can try many different methods, evaluating each against a validation set before selecting a model, AL affords no such luxury. Over the course of one AL run, an agent annotates its dataset exhausting its labeling budget. Thus, given a new task, an active learner has no opportunity to compare models and acquisition functions. This paper provides a largescale empirical study of deep active learning, addressing multiple tasks and, for each, multiple datasets, multiple models, and a full suite of acquisition functions. We find that across all settings, Bayesian active learning by disagreement, using uncertainty estimates provided either by Dropout or Bayes-by-Backprop significantly improves over i.i.d. baselines and usually outperforms classic uncertainty sampling.",,,,
7,singh_deep_2018,"Singh, Ramandeep; Kalra, Mannudeep K.; Nitiwarangkul, Chayanin; Patti, John A.; Homayounieh, Fatemeh; Padole, Atul; Rao, Pooja; Putha, Preetham; Muse, Victorine V.; Sharma, Amita; Digumarthy, Subba R.",Deep learning in chest radiography: Detection of findings and presence of change,PLOS ONE,October,2018,https://dx.plos.org/10.1371/journal.pone.0204155,,,,,
7,sinoara_knowledge-enhanced_2019,"Sinoara, Roberta A.; Camacho-Collados, Jose; Rossi, Rafael G.; Navigli, Roberto; Rezende, Solange O.",Knowledge-enhanced document embeddings for text classification,Knowledge-Based Systems,January,2019,http://www.sciencedirect.com/science/article/pii/S0950705118305124,"Accurate semantic representation models are essential in text mining applications. For a successful application of the text mining process, the text representation adopted must keep the interesting patterns to be discovered. Although competitive results for automatic text classification may be achieved with traditional bag of words, such representation model cannot provide satisfactory classification performances on hard settings where richer text representations are required. In this paper, we present an approach to represent document collections based on embedded representations of words and word senses. We bring together the power of word sense disambiguation and the semantic richness of word- and word-sense embedded vectors to construct embedded representations of document collections. Our approach results in semantically enhanced and low-dimensional representations. We overcome the lack of interpretability of embedded vectors, which is a drawback of this kind of representation, with the use of word sense embedded vectors. Moreover, the experimental evaluation indicates that the use of the proposed representations provides stable classifiers with strong quantitative results, especially in semantically-complex classification scenarios.",,"Document embeddings, Semantic representation, Text classification, Text mining",,
7,siyam_mining_2020,"Siyam, Nur; Alqaryouti, Omar; Abdallah, Sherief",Mining government tweets to identify and predict citizens engagement,Technology in Society,February,2020,http://www.sciencedirect.com/science/article/pii/S0160791X19302040,"The rise of social media offered new channels of communication between a government and its citizens. The social media channels are interactive, inclusive, low-cost, and unconstrained by time or place. This two-way communication between governments and citizens is referred to as electronic citizen participation, or e-participation. E-participation in the age of technology is considered as a mean for citizens to express their opinions and as a new input to be integrated by policy makers to take decisions. Governments and policy makers always aim to increase such participation not only to utilize public expertise and experience, but also to increase the transparency, trust, and acceptability of government decisions. In this research we investigate how governments can increase citizens e-participation on social media. We collected 55,809 tweets over a period of one year from Twitter accounts of a progressive government in the Arab world. This was followed by statistical analysis of posts characteristics (Type, Day, Time) and their impact on citizens' engagement. Then, we evaluated how well can different machine learning techniques predict user engagement. Results of the statistical analysis confirmed that post type (video, image, link, and status) impacted citizens' engagement, with videos and images having the highest positive impact on engagement. Furthermore, posting government tweets on weekdays obtained higher citizensâ€™ engagement than weekends. Conversely, time of post had a weak effect on engagement. The results from the machine learning experiments show that two techniques (Random Forest and Adaboost) produced more accurate predictions, particularly when tweet textual contents were also used in the prediction. These results can help governments increase the engagement of their citizens.",,"Dubai government, Ensemble learning models, Machine learning, Mining government tweets, Post engagement, Twitter data",,
7,skrlj_attviz_2020,"Å krlj, BlaÅ_; ErÅ_en, Nika; Sheehan, Shane; Luz, Saturnino; Robnik-Å ikonja, Marko; Pollak, Senja",AttViz: Online exploration of self-attention for transparent neural language modeling,"arXiv:2005.05716 [cs, stat]",May,2020,http://arxiv.org/abs/2005.05716,"Neural language models are becoming the prevailing methodology for the tasks of query answering, text classiï¬Åcation, disambiguation, completion and translation. Commonly comprised of hundreds of millions of parameters, these neural network models offer state-of-the-art performance at the cost of interpretability; humans are no longer capable of tracing and understanding how decisions are being made. The attention mechanism, introduced initially for the task of translation, has been successfully adopted for other language-related tasks. We propose AttViz, an online toolkit for exploration of self-attentionâ€”real values associated with individual text tokens. We show how existing deep learning pipelines can produce outputs suitable for AttViz, offering novel visualizations of the attention heads and their aggregations with minimal effort, online. We show on examples of news segments how the proposed system can be used to inspect and potentially better understand what a model has learned (or emphasized).",,"Computer Science - Machine Learning, Statistics - Machine Learning",,
7,smit_chexbert_2020,"Smit, Akshay; Jain, Saahil; Rajpurkar, Pranav; Pareek, Anuj; Ng, Andrew Y.; Lungren, Matthew P.",CheXbert: Combining Automatic Labelers and Expert Annotations for Accurate Radiology Report Labeling Using BERT,arXiv:2004.09167 [cs],April,2020,http://arxiv.org/abs/2004.09167,"The extraction of labels from radiology text reports enables large-scale training of medical imaging models. Existing approaches to report labeling typically rely either on sophisticated feature engineering based on medical domain knowledge or manual annotations by experts. In this work, we introduce a BERTbased approach to medical image report labeling that exploits both the scale of available rule-based systems and the quality of expert annotations. We demonstrate superior performance of a biomedically pretrained BERT model ï¬Årst trained on annotations of a rulebased labeler and then ï¬Ånetuned on a small set of expert annotations augmented with automated backtranslation. We ï¬Ånd that our ï¬Ånal model, CheXbert, is able to outperform the previous best rules-based labeler with statistical signiï¬Åcance, setting a new SOTA for report labeling on one of the largest datasets of chest x-rays.",,"Computer Science - Computation and Language, Computer Science - Information Retrieval, Computer Science - Machine Learning, read",,
7,song_machine_2017,"Song, Yangqiu; Roth, Dan",Machine Learning with World Knowledge: The Position and Survey,"arXiv:1705.02908 [cs, stat]",May,2017,http://arxiv.org/abs/1705.02908,"Machine learning has become pervasive in multiple domains, impacting a wide variety of applications, such as knowledge discovery and data mining, natural language processing, information retrieval, computer vision, social and health informatics, ubiquitous computing, etc. Two essential problems of machine learning are how to generate features and how to acquire labels for machines to learn. Particularly, labeling large amount of data for each domain-specific problem can be very time consuming and costly. It has become a key obstacle in making learning protocols realistic in applications. In this paper, we will discuss how to use the existing general-purpose world knowledge to enhance machine learning processes, by enriching the features or reducing the labeling work. We start from the comparison of world knowledge with domain-specific knowledge, and then introduce three key problems in using world knowledge in learning processes, i.e., explicit and implicit feature representation, inference for knowledge linking and disambiguation, and learning with direct or indirect supervision. Finally we discuss the future directions of this research topic.",,"Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning",,
7,spasic_clinical_2020,"Spasic, Irena; Nenadic, Goran",Clinical Text Data in Machine Learning: Systematic Review,,March,2020,,"Background: Clinical narratives represent the main form of communication within health care, providing a personalized account of patient history and assessments, and offering rich information for clinical decision making. Natural language processing (NLP) has repeatedly demonstrated its feasibility to unlock evidence buried in clinical narratives. Machine learning can facilitate rapid development of NLP tools by leveraging large amounts of text data.",,Alan\_Katz,,
7,spinner_explainer_2020,"Spinner, Thilo; Schlegel, Udo; SchÃ_fer, Hanna; El-Assady, Mennatallah",explAIner: A Visual Analytics Framework for Interactive and Explainable Machine Learning,IEEE Transactions on Visualization and Computer Graphics,January,2020,,"We propose a framework for interactive and explainable machine learning that enables users to (1) understand machine learning models; (2) diagnose model limitations using different explainable AI methods; as well as (3) refine and optimize the models. Our framework combines an iterative XAI pipeline with eight global monitoring and steering mechanisms, including quality monitoring, provenance tracking, model comparison, and trust building. To operationalize the framework, we present explAIner, a visual analytics system for interactive and explainable machine learning that instantiates all phases of the suggested pipeline within the commonly used TensorBoard environment. We performed a user-study with nine participants across different expertise levels to examine their perception of our workflow and to collect suggestions to fill the gap between our system and framework. The evaluation confirms that our tightly integrated system leads to an informed machine learning process while disclosing opportunities for further extensions.",,"AI methods, Analytical models, Computational modeling, Data models, Deep Learning, Explainability, Explainable AI, Interactive Machine Learning, Interpretability, Machine learning, Monitoring, Pipelines, Visual Analytics, XAI pipeline, data analysis, data visualisation, explAIner, explainable machine learning, interactive machine learning, interactive systems, learning (artificial intelligence), machine learning process, visual analytics framework",,
7,spinner_explainer:_2019,"Spinner, Thilo; Schlegel, Udo; SchÃ_fer, Hanna; El-Assady, Mennatallah",explAIner: A Visual Analytics Framework for Interactive and Explainable Machine Learning,IEEE Transactions on Visualization and Computer Graphics,,2019,http://arxiv.org/abs/1908.00087,"We propose a framework for interactive and explainable machine learning that enables users to (1) understand machine learning models; (2) diagnose model limitations using different explainable AI methods; as well as (3) refine and optimize the models. Our framework combines an iterative XAI pipeline with eight global monitoring and steering mechanisms, including quality monitoring, provenance tracking, model comparison, and trust building. To operationalize the framework, we present explAIner, a visual analytics system for interactive and explainable machine learning that instantiates all phases of the suggested pipeline within the commonly used TensorBoard environment. We performed a user-study with nine participants across different expertise levels to examine their perception of our workflow and to collect suggestions to fill the gap between our system and framework. The evaluation confirms that our tightly integrated system leads to an informed machine learning process while disclosing opportunities for further extensions.",,"Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning",,
7,steinkamp_toward_2019,"Steinkamp, Jackson M.; Chambers, Charles; Lalevic, Darco; Zafar, Hanna M.; Cook, Tessa S.",Toward Complete Structured Information Extraction from Radiology Reports Using Machine Learning,Journal of Digital Imaging,August,2019,http://link.springer.com/10.1007/s10278-019-00234-y,"Unstructured and semi-structured radiology reports represent an underutilized trove of information for machine learning (ML)based clinical informatics applications, including abnormality tracking systems, research cohort identification, point-of-care summarization, semi-automated report writing, and as a source of weak data labels for training image processing systems. Clinical ML systems must be interpretable to ensure user trust. To create interpretable models applicable to all of these tasks, we can build general-purpose systems which extract all relevant human-level assertions or â€œfactsâ€ù documented in reports; identifying these facts is an information extraction (IE) task. Previous IE work in radiology has focused on a limited set of information, and extracts isolated entities (i.e., single words such as â€œlesionâ€ù or â€œcystâ€ù) rather than complete facts, which require the linking of multiple entities and modifiers. Here, we develop a prototype system to extract all useful information in abdominopelvic radiology reports (findings, recommendations, clinical history, procedures, imaging indications and limitations, etc.), in the form of complete, contextualized facts. We construct an information schema to capture the bulk of information in reports, develop real-time ML models to extract this information, and demonstrate the feasibility and performance of the system.",,,,
7,strobelt_seq2seq-vis:_2018,"Strobelt, Hendrik; Gehrmann, Sebastian; Behrisch, Michael; Perer, Adam; Pfister, Hanspeter; Rush, Alexander M.",Seq2Seq-Vis: A Visual Debugging Tool for Sequence-to-Sequence Models,arXiv:1804.09299 [cs],April,2018,http://arxiv.org/abs/1804.09299,"Neural Sequence-to-Sequence models have proven to be accurate and robust for many sequence prediction tasks, and have become the standard approach for automatic translation of text. The models work in a five stage blackbox process that involves encoding a source sequence to a vector space and then decoding out to a new target sequence. This process is now standard, but like many deep learning methods remains quite difficult to understand or debug. In this work, we present a visual analysis tool that allows interaction with a trained sequence-to-sequence model through each stage of the translation process. The aim is to identify which patterns have been learned and to detect model errors. We demonstrate the utility of our tool through several real-world large-scale sequence-to-sequence use cases.",,"Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing",,
7,strubell_energy_2019,"Strubell, Emma; Ganesh, Ananya; McCallum, Andrew",Energy and Policy Considerations for Deep Learning in NLP,arXiv:1906.02243 [cs],June,2019,http://arxiv.org/abs/1906.02243,"Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both ï¬Ånancially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate ï¬Ånancial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these ï¬Åndings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.",,Computer Science - Computation and Language,,
7,subramanian_extractive_2019,"Subramanian, Sandeep; Li, Raymond; Pilault, Jonathan; Pal, Christopher",On Extractive and Abstractive Neural Document Summarization with Transformer Language Models,arXiv:1909.03186 [cs],September,2019,http://arxiv.org/abs/1909.03186,"We present a method to produce abstractive summaries of long documents that exceed several thousand words via neural abstractive summarization. We perform a simple extractive step before generating a summary, which is then used to condition the transformer language model on relevant information before being tasked with generating a summary. We show that this extractive step significantly improves summarization results. We also show that this approach produces more abstractive summaries compared to prior work that employs a copy mechanism while still achieving higher rouge scores. Note: The abstract above was not written by the authors, it was generated by one of the models presented in this paper.",,Computer Science - Computation and Language,,
7,subramanian_extractive_2019,"Subramanian, Sandeep; Li, Raymond; Pilault, Jonathan; Pal, Christopher",On Extractive and Abstractive Neural Document Summarization with Transformer Language Models,arXiv:1909.03186 [cs],September,2019,http://arxiv.org/abs/1909.03186,"We present a method to produce abstractive summaries of long documents that exceed several thousand words via neural abstractive summarization. We perform a simple extractive step before generating a summary, which is then used to condition the transformer language model on relevant information before being tasked with generating a summary. We show that this extractive step signiï¬Åcantly improves summarization results. We also show that this approach produces more abstractive summaries compared to prior work that employs a copy mechanism while still achieving higher rouge scores. Note: The abstract above was not written by the authors, it was generated by one of the models presented in this paper.",,Computer Science - Computation and Language,,
7,sun_evaluating_2013,"Sun, Weiyi; Rumshisky, Anna; Uzuner, Ozlem",Evaluating temporal relations in clinical text: 2012 i2b2 Challenge,Journal of the American Medical Informatics Association,April,2013,https://academic.oup.com/jamia/article/20/5/806/726374,"Background The Sixth Informatics for Integrating Biology and the Bedside (i2b2) Natural Language Processing Challenge for Clinical Records focused on the temporal relations in clinical narratives. The organizers provided the research community with a corpus of discharge summaries annotated with temporal information, to be used for the development and evaluation of temporal reasoning systems. 18 teams from around the world participated in the challenge. During the workshop, participating teams presented comprehensive reviews and analysis of their systems, and outlined future research directions suggested by the challenge contributions.  Methods The challenge evaluated systems on the information extraction tasks that targeted: (1) clinically significant events, including both clinical concepts such as problems, tests, treatments, and clinical departments, and events relevant to the patient's clinical timeline, such as admissions, transfers between departments, etc; (2) temporal expressions, referring to the dates, times, durations, or frequencies phrases in the clinical text. The values of the extracted temporal expressions had to be normalized to an ISO specification standard; and (3) temporal relations, between the clinical events and temporal expressions. Participants determined pairs of events and temporal expressions that exhibited a temporal relation, and identified the temporal relation between them.  Results For event detection, statistical machine learning (ML) methods consistently showed superior performance. While ML and rule based methods seemed to detect temporal expressions equally well, the best systems overwhelmingly adopted a rule based approach for value normalization. For temporal relation classification, the systems using hybrid approaches that combined ML and heuristics based methods produced the best results.",,,,
7,sun_mitigating_2019,"Sun, Tony; Gaut, Andrew; Tang, Shirlyn; Huang, Yuxin; ElSherief, Mai; Zhao, Jieyu; Mirza, Diba; Belding, Elizabeth; Chang, Kai-Wei; Wang, William Yang",Mitigating Gender Bias in Natural Language Processing: Literature Review,arXiv:1906.08976 [cs],June,2019,http://arxiv.org/abs/1906.08976,"As Natural Language Processing (NLP) and Machine Learning (ML) tools rise in popularity, it becomes increasingly vital to recognize the role they play in shaping societal biases and stereotypes. Although NLP models have shown success in modeling various applications, they propagate and may even amplify gender bias found in text corpora. While the study of bias in artificial intelligence is not new, methods to mitigate gender bias in NLP are relatively nascent. In this paper, we review contemporary studies on recognizing and mitigating gender bias in NLP. We discuss gender bias based on four forms of representation bias and analyze methods recognizing gender bias. Furthermore, we discuss the advantages and drawbacks of existing gender debiasing methods. Finally, we discuss future studies for recognizing and mitigating gender bias in NLP.",,Computer Science - Computation and Language,,
7,sutskever_sequence_nodate,"Sutskever, Ilya; Vinyals, Oriol; Le, Quoc V.",Sequence to Sequence Learning with Neural Networks,,,,,"Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difï¬Åcult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a ï¬Åxed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTMâ€™s BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difï¬Åculty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTMâ€™s performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.",,sequence to sequence models,,
7,sze-to_searching_nodate,"Sze-To, Antonio; Tizhoosh, Hamid",Searching for Pneumothorax in Half a Million Chest X-Ray Images,,,,,"Pneumothorax, a collapsed or dropped lung, is a fatal condition typically detected on a chest X-ray by an experienced radiologist. Due to shortage of such experts, automated detection systems based on deep neural networks have been developed. Nevertheless, applying such systems in practice remains a challenge. These systems, mostly compute a single probability as output, may not be enough for diagnosis. On the contrary, content-based medical image retrieval (CBIR) systems, such as image search, can assist clinicians for diagnostic purposes by enabling them to compare the case they are examining with previous (already diagnosed) cases. However, there is a lack of study on such attempt. In this study, we explored the use of image search to classify pneumothorax among chest X-ray images. All chest X-ray images were ï¬Årst tagged with deep pretrained features, which were obtained from existing deep learning models. Given a query chest X-ray image, the majority voting of the top K retrieved images was then used as a classiï¬Åer, in which similar cases in the archive of past cases are provided besides the probability output. In our experiments, 551,383 chest X-ray images were obtained from three large recently released public datasets. Using 10-fold crossvalidation, it is shown that image search on deep pretrained features achieved promising results compared to those obtained by traditional classiï¬Åers trained on the same features. To the best of knowledge, it is the ï¬Årst study to demonstrate that deep pretrained features can be used for CBIR of pneumothorax in half a million chest X-ray images.",,,,
7,tan_assessing_2019,"Tan, Yi Chern; Celis, L. Elisa",Assessing Social and Intersectional Biases in Contextualized Word Representations,"arXiv:1911.01485 [cs, stat]",November,2019,http://arxiv.org/abs/1911.01485,"Social bias in machine learning has drawn significant attention, with work ranging from demonstrations of bias in a multitude of applications, curating definitions of fairness for different contexts, to developing algorithms to mitigate bias. In natural language processing, gender bias has been shown to exist in context-free word embeddings. Recently, contextual word representations have outperformed word embeddings in several downstream NLP tasks. These word representations are conditioned on their context within a sentence, and can also be used to encode the entire sentence. In this paper, we analyze the extent to which state-of-the-art models for contextual word representations, such as BERT and GPT-2, encode biases with respect to gender, race, and intersectional identities. Towards this, we propose assessing bias at the contextual word level. This novel approach captures the contextual effects of bias missing in context-free word embeddings, yet avoids confounding effects that underestimate bias at the sentence encoding level. We demonstrate evidence of bias at the corpus level, find varying evidence of bias in embedding association tests, show in particular that racial bias is strongly encoded in contextual word models, and observe that bias effects for intersectional minorities are exacerbated beyond their constituent minority identities. Further, evaluating bias effects at the contextual word level captures biases that are not captured at the sentence level, confirming the need for our novel approach.",,"Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society, Computer Science - Machine Learning, Statistics - Machine Learning",,
7,tang_automated_2020,"Tang, Yu-Xing; Tang, You-Bao; Peng, Yifan; Yan, Ke; Bagheri, Mohammadhadi; Redd, Bernadette A.; Brandon, Catherine J.; Lu, Zhiyong; Han, Mei; Xiao, Jing; Summers, Ronald M.",Automated abnormality classification of chest radiographs using deep convolutional neural networks,npj Digital Medicine,May,2020,https://www.nature.com/articles/s41746-020-0273-z,"As one of the most ubiquitous diagnostic imaging tests in medical practice, chest radiography requires timely reporting of potential findings and diagnosis of diseases in the images. Automated, fast, and reliable detection of diseases based on chest radiography is a critical step in radiology workflow. In this work, we developed and evaluated various deep convolutional neural networks (CNN) for differentiating between normal and abnormal frontal chest radiographs, in order to help alert radiologists and clinicians of potential abnormal findings as a means of work list triaging and reporting prioritization. A CNN-based model achieved an AUC of 0.9824â€‰Â±â€‰0.0043 (with an accuracy of 94.64â€‰Â±â€‰0.45\%, a sensitivity of 96.50â€‰Â±â€‰0.36\% and a specificity of 92.86â€‰Â±â€‰0.48\%) for normal versus abnormal chest radiograph classification. The CNN model obtained an AUC of 0.9804â€‰Â±â€‰0.0032 (with an accuracy of 94.71â€‰Â±â€‰0.32\%, a sensitivity of 92.20â€‰Â±â€‰0.34\% and a specificity of 96.34â€‰Â±â€‰0.31\%) for normal versus lung opacity classification. Classification performance on the external dataset showed that the CNN model is likely to be highly generalizable, with an AUC of 0.9444â€‰Â±â€‰0.0029. The CNN model pre-trained on cohorts of adult patients and fine-tuned on pediatric patients achieved an AUC of 0.9851â€‰Â±â€‰0.0046 for normal versus pneumonia classification. Pretraining with natural images demonstrates benefit for a moderate-sized training image set of about 8500 images. The remarkable performance in diagnostic accuracy observed in this study shows that deep CNNs can accurately and effectively differentiate normal and abnormal chest radiographs, thereby providing potential benefits to radiology workflow and patient care.",,,,
7,tang_distilling_2019,"Tang, Raphael; Lu, Yao; Liu, Linqing; Mou, Lili; Vechtomova, Olga; Lin, Jimmy",Distilling Task-Specific Knowledge from BERT into Simple Neural Networks,arXiv:1903.12136 [cs],March,2019,http://arxiv.org/abs/1903.12136,"In the natural language processing literature, neural networks are becoming increasingly deeper and complex. The recent poster child of this trend is the deep language representation model, which includes BERT, ELMo, and GPT. These developments have led to the conviction that previous-generation, shallower neural networks for language understanding are obsolete. In this paper, however, we demonstrate that rudimentary, lightweight neural networks can still be made competitive without architecture changes, external training data, or additional input features. We propose to distill knowledge from BERT, a state-ofthe-art language representation model, into a single-layer BiLSTM, as well as its siamese counterpart for sentence-pair tasks. Across multiple datasets in paraphrasing, natural language inference, and sentiment classiï¬Åcation, we achieve comparable results with ELMo, while using roughly 100 times fewer parameters and 15 times less inference time.",,"Computer Science - Computation and Language, Computer Science - Machine Learning",,
7,tarnpradab_toward_nodate,"Tarnpradab, Sansiri; Liu, Fei; Hua, Kien A.",Toward Extractive Summarization of Online Forum Discussions via Hierarchical Attention Networks,,,,,"Forum threads are lengthy and rich in content. Concise thread summaries will beneï¬Åt both newcomers seeking information and those who participate in the discussion. Few studies, however, have examined the task of forum thread summarization. In this work we make the ï¬Årst attempt to adapt the hierarchical attention networks for thread summarization. The model draws on the recent development of neural attention mechanisms to build sentence and thread representations and use them for summarization. Our results indicate that the proposed approach can outperform a range of competitive baselines. Further, a redundancy removal step is crucial for achieving outstanding results.",,,,
7,tenney_bert_2019,"Tenney, Ian; Das, Dipanjan; Pavlick, Ellie",BERT Rediscovers the Classical NLP Pipeline,arXiv:1905.05950 [cs],May,2019,http://arxiv.org/abs/1905.05950,"Pre-trained text encoders have rapidly advanced the state of the art on many NLP tasks. We focus on one such model, BERT, and aim to quantify where linguistic information is captured within the network. We find that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference. Qualitative analysis reveals that the model can and often does adjust this pipeline dynamically, revising lower-level decisions on the basis of disambiguating information from higher-level representations.",,Computer Science - Computation and Language,,
7,thelwall_microsoft_2018,"Thelwall, Mike",Microsoft Academic automatic document searches: Accuracy for journal articles and suitability for citation analysis,Journal of Informetrics,February,2018,http://www.sciencedirect.com/science/article/pii/S1751157717303346,,,"Article-level citation analysis, Citation analysis, Microsoft Academic, Scientometrics",,
7,tsai_small_2019,"Tsai, Henry; Riesa, Jason; Johnson, Melvin; Arivazhagan, Naveen; Li, Xin; Archer, Amelia",Small and Practical BERT Models for Sequence Labeling,arXiv:1909.00100 [cs],August,2019,http://arxiv.org/abs/1909.00100,"We propose a practical scheme to train a single multilingual sequence labeling model that yields state of the art results and is small and fast enough to run on a single CPU. Starting from a public multilingual BERT checkpoint, our ï¬Ånal model is 6x smaller and 27x faster, and has higher accuracy than a state-of-theart multilingual baseline. We show that our model especially outperforms on low-resource languages, and works on codemixed input text without being explicitly trained on codemixed examples. We showcase the effectiveness of our method by reporting on part-of-speech tagging and morphological prediction on 70 treebanks and 48 languages.",,Computer Science - Computation and Language,,
7,tshitoyan_unsupervised_2019,"Tshitoyan, Vahe; Dagdelen, John; Weston, Leigh; Dunn, Alexander; Rong, Ziqin; Kononova, Olga; Persson, Kristin A.; Ceder, Gerbrand; Jain, Anubhav",Unsupervised word embeddings capture latent knowledge from materials science literature,Nature,July,2019,https://www.nature.com/articles/s41586-019-1335-8,"Natural language processing algorithms applied to three million materials science abstracts uncover relationships between words, material compositions and properties, and predict potential new thermoelectric materials.  The overwhelming majority of scientific knowledge is published as text, which is difficult to analyse by either traditional statistical analysis or modern machine learning methods. By contrast, the main source of machine-interpretable data for the materials research community has come from structured property databases1,2, which encompass only a small fraction of the knowledge present in the research literature. Beyond property values, publications contain valuable knowledge regarding the connections and relationships between data items as interpreted by the authors. To improve the identification and use of this knowledge, several studies have focused on the retrieval of information from scientific literature  using supervised natural language processing, which requires large hand-labelled datasets for training. Here we show that materials science knowledge present in the published literature can be efficiently encoded as information-dense word embeddings (vector representations of words) without human labelling or supervision. Without any explicit insertion of chemical knowledge, these embeddings capture complex materials science concepts such as the underlying structure of the periodic table and structureâ€“property relationships in materials. Furthermore, we demonstrate  that an unsupervised method can recommend materials for functional applications several years before their discovery. This suggests that latent knowledge regarding future discoveries is to a large extent embedded in past publications. Our findings highlight the possibility of extracting knowledge and relationships from the massive body of scientific literature in a collective manner, and point towards a generalized approach to the mining of scientific literature.",,,,
7,tsoumakas_multi-label_2007,"Tsoumakas, Grigorios; Katakis, Ioannis",Multi-Label Classification: An Overview,International Journal of Data Warehousing and Mining (IJDWM),July,2007,https://www.igi-global.com/article/multi-label-classification/1786,,,,,
7,turc_well-read_2019,"Turc, Iulia; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina",Well-Read Students Learn Better: On the Importance of Pre-training Compact Models,arXiv:1908.08962 [cs],September,2019,http://arxiv.org/abs/1908.08962,"Recent developments in natural language representations have been accompanied by large and expensive models that leverage vast amounts of general-domain text through self-supervised pre-training. Due to the cost of applying such models to down-stream tasks, several model compression techniques on pre-trained language representations have been proposed (Sun et al., 2019a; Sanh, 2019). However, surprisingly, the simple baseline of just pre-training and ï¬Åne-tuning compact models has been overlooked. In this paper, we ï¬Årst show that pre-training remains important in the context of smaller architectures, and ï¬Åne-tuning pre-trained compact models can be competitive to more elaborate methods proposed in concurrent work. Starting with pre-trained compact models, we then explore transferring task knowledge from large ï¬Åne-tuned models through standard knowledge distillation. The resulting simple, yet effective and general algorithm, Pre-trained Distillation, brings further improvements. Through extensive experiments, we more generally explore the interaction between pre-training and distillation under two variables that have been under-studied: model size and properties of unlabeled task data. One surprising observation is that they have a compound effect even when sequentially applied on the same data. To accelerate future research, we will make our 24 pre-trained miniature BERT models publicly available.",,Computer Science - Computation and Language,,
7,varghese_active_2019,"Varghese, Arun; Hong, Tao; Hunter, Chelsea; Agyeman-Badu, George; Cawley, Michelle",Active learning in automated text classification: a case study exploring bias in predicted model performance metrics,Environment Systems and Decisions,January,2019,https://doi.org/10.1007/s10669-019-09717-3,"Machine learning has emerged as a cost-effective innovation to support systematic literature reviews in human health risk assessments and other contexts. Supervised machine learning approaches rely on a training dataset, a relatively small set of documents with human-annotated labels indicating their topic, to build models that automatically classify a larger set of unclassified documents. â€œActiveâ€ù machine learning has been proposed as an approach that limits the cost of creating a training dataset by interactively and sequentially focussing on training only the most informative documents. We simulate active learning using a dataset of approximately 7000 abstracts from the scientific literature related to the chemical arsenic. The dataset was previously annotated by subject matter experts with regard to relevance to two topics relating to toxicology and risk assessment. We examine the performance of alternative sampling approaches to sequentially expanding the training dataset, specifically looking at uncertainty-based sampling and probability-based sampling. We discover that while such active learning methods can potentially reduce training dataset size compared to random sampling, predictions of model performance in active learning are likely to suffer from statistical bias that negates the methodâ€™s potential benefits. We discuss approaches and the extent to which the bias resulting from skewed sampling can be compensated. We propose a useful role for active learning in contexts in which the accuracy of model performance metrics is not critical and/or where it is beneficial to rapidly create a class-balanced training dataset.",,"Active learning, Automated document classification, Literature review, Machine learning, Natural language processing, Systematic review",,
7,vaswani_attention_2017,"Vaswani, Ashish; Shazeer, Noam; Parmar, Niki; Uszkoreit, Jakob; Jones, Llion; Gomez, Aidan N.; Kaiser, Lukasz; Polosukhin, Illia",Attention Is All You Need,arXiv:1706.03762 [cs],December,2017,http://arxiv.org/abs/1706.03762,"The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",,"Computer Science - Computation and Language, Computer Science - Machine Learning",,
7,vaswani_attention_nodate,"Vaswani, Ashish; Shazeer, Noam; Parmar, Niki; Uszkoreit, Jakob; Jones, Llion; Gomez, Aidan N.; Kaiser, ÅÅukasz; Polosukhin, Illia",Attention is All you Need,,,,,"The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiï¬Åcantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.",,,,
7,VEGAOLIVEROS2019102063,"Vega-Oliveros, Didier A.; Gomes, Pedro Spoljaric; Milios, Evangelos E.; Berton, Lilian",A multi-centrality index for graph-based keyword extraction,Information Processing \& Management,,2019,http://www.sciencedirect.com/science/article/pii/S0306457319303267,"Keyword extraction aims to capture the main topics of a document and is an important step in natural language processing (NLP) applications. The use of different graph centrality measures has been proposed to extract automatic keywords. However, there is no consensus yet on how these measures compare in this task. Here, we present the multi-centrality index (MCI) approach, which aims to find the optimal combination of word rankings according to the selection of centrality measures. We analyze nine centrality measures (Betweenness, Clustering Coefficient, Closeness, Degree, Eccentricity, Eigenvector, K-Core, PageRank, Structural Holes) for identifying keywords in co-occurrence word-graphs representation of documents. We perform experiments on three datasets of documents and demonstrate that all individual centrality methods achieve similar statistical results, while the proposed MCI approach significantly outperforms the individual centralities, three clustering algorithms, and previously reported results in the literature.",,"Automatic keyword extraction, Centrality measures, Clustering, Complex networks, Network science, Text mining, Text networks",,
7,velickovic_graph_2018,"VeliÄçkoviÄ‡, Petar; Cucurull, Guillem; Casanova, Arantxa; Romero, Adriana; LiÃ_, Pietro; Bengio, Yoshua",Graph Attention Networks,"arXiv:1710.10903 [cs, stat]",February,2018,http://arxiv.org/abs/1710.10903,"We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoodsâ€™ features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-theart results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a proteinprotein interaction dataset (wherein test graphs remain unseen during training).",,"Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Social and Information Networks, Statistics - Machine Learning",,
7,vial_sense_2019,"Vial, LoÃ¯c; Lecouteux, Benjamin; Schwab, Didier",Sense Vocabulary Compression through the Semantic Knowledge of WordNet for Neural Word Sense Disambiguation,arXiv:1905.05677 [cs],May,2019,http://arxiv.org/abs/1905.05677,"In this article, we tackle the issue of the limited quantity of manually sense annotated corpora for the task of word sense disambiguation, by exploiting the semantic relationships between senses such as synonymy, hypernymy and hyponymy, in order to compress the sense vocabulary of Princeton WordNet, and thus reduce the number of different sense tags that must be observed to disambiguate all words of the lexical database. We propose two different methods that greatly reduces the size of neural WSD models, with the benefit of improving their coverage without additional training data, and without impacting their precision. In addition to our method, we present a new WSD system which relies on pre-trained BERT word vectors in order to achieve results that significantly outperform the state of the art on all WSD evaluation tasks.",,Computer Science - Computation and Language,,
7,vig_visualizing_2019,"Vig, Jesse",Visualizing Attention in Transformer-Based Language Representation Models,"arXiv:1904.02679 [cs, stat]",April,2019,http://arxiv.org/abs/1904.02679,"We present an open-source tool for visualizing multi-head self-attention in Transformer-based language representation models. The tool extends earlier work by visualizing attention at three levels of granularity: the attention-head level, the model level, and the neuron level. We describe how each of these views can help to interpret the model, and we demonstrate the tool on the BERT model and the OpenAI GPT-2 model. We also present three use cases for analyzing GPT-2: detecting model bias, identifying recurring patterns, and linking neurons to model behavior.",,"Computer Science - Human-Computer Interaction, Computer Science - Machine Learning, Statistics - Machine Learning",,
7,voita_analyzing_2019,"Voita, Elena; Talbot, David; Moiseev, Fedor; Sennrich, Rico; Titov, Ivan","Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned",arXiv:1905.09418 [cs],June,2019,http://arxiv.org/abs/1905.09418,"Multi-head self-attention is a key component of the Transformer, a state-of-the-art architecture for neural machine translation. In this work we evaluate the contribution made by individual attention heads in the encoder to the overall performance of the model and analyze the roles played by them. We find that the most important and confident heads play consistent and often linguistically-interpretable roles. When pruning heads using a method based on stochastic gates and a differentiable relaxation of the L0 penalty, we observe that specialized heads are last to be pruned. Our novel pruning method removes the vast majority of heads without seriously affecting performance. For example, on the English-Russian WMT dataset, pruning 38 out of 48 encoder heads results in a drop of only 0.15 BLEU.",,Computer Science - Computation and Language,,
7,volkovs_predicting_2020,"Volkovs, Maksims; Cheng, Zhaoyue; Ravaut, Mathieu; Yang, Hojin; Shen, Kevin; Zhou, Jin Peng; Wong, Anson; Zuberi, Saba; Zhang, Ivan; Frosst, Nick; Ngo, Helen; Chen, Carol; Venkitesh, Bharat; Gou, Stephen; Gomez, Aidan N.",Predicting Twitter Engagement With Deep Language Models,,,2020,,"Twitter has become one of the main information sharing platforms for millions of users world-wide. Numerous tweets are created daily, many with highly time sensitive content such as breaking news, new multimedia content or personal updates. Consequently, accurately recommending relevant tweets to users in a timely manner is a highly important and challenging problem. The 2020 ACM RecSys Challenge is aimed at benchmarking leading recommendation models for this task. The challenge is based on a large and recent dataset of over 200M tweet engagements released by Twitter with content in over 50 languages. In this work we present our approach where we leverage recent advances in deep language modeling and attention architectures, to combine information from extracted features, user engagement history and target tweet content. We first finetune leading multilingual language models M-BERT and XLM-R for Twitter data. Embeddings from these models are used to extract tweet and user history representations. We then combine all components together and jointly train them to maximize engagement prediction accuracy. Our approach achieves highly competitive performance placing 2â€™nd on the final private leaderboard. Full code is available here: https://github.com/layer6ai-labs/RecSys2020.",,,,
7,wang_comparison_2018,"Wang, Yanshan; Liu, Sijia; Afzal, Naveed; Rastegar-Mojarad, Majid; Wang, Liwei; Shen, Feichen; Kingsbury, Paul; Liu, Hongfang",A comparison of word embeddings for the biomedical natural language processing,Journal of Biomedical Informatics,November,2018,http://www.sciencedirect.com/science/article/pii/S1532046418301825,"Background Word embeddings have been prevalently used in biomedical Natural Language Processing (NLP) applications due to the ability of the vector representations being able to capture useful semantic properties and linguistic relationships between words. Different textual resources (e.g., Wikipedia and biomedical literature corpus) have been utilized in biomedical NLP to train word embeddings and these word embeddings have been commonly leveraged as feature input to downstream machine learning models. However, there has been little work on evaluating the word embeddings trained from different textual resources. Methods In this study, we empirically evaluated word embeddings trained from four different corpora, namely clinical notes, biomedical publications, Wikipedia, and news. For the former two resources, we trained word embeddings using unstructured electronic health record (EHR) data available at Mayo Clinic and articles (MedLit) from PubMed Central, respectively. For the latter two resources, we used publicly available pre-trained word embeddings, GloVe and Google News. The evaluation was done qualitatively and quantitatively. For the qualitative evaluation, we randomly selected medical terms from three categories (i.e., disorder, symptom, and drug), and manually inspected the five most similar words computed by embeddings for each term. We also analyzed the word embeddings through a 2-dimensional visualization plot of 377 medical terms. For the quantitative evaluation, we conducted both intrinsic and extrinsic evaluation. For the intrinsic evaluation, we evaluated the word embeddingsâ€™ ability to capture medical semantics by measruing the semantic similarity between medical terms using four published datasets: Pedersenâ€™s dataset, Hliaoutakisâ€™s dataset, MayoSRS, and UMNSRS. For the extrinsic evaluation, we applied word embeddings to multiple downstream biomedical NLP applications, including clinical information extraction (IE), biomedical information retrieval (IR), and relation extraction (RE), with data from shared tasks. Results The qualitative evaluation shows that the word embeddings trained from EHR and MedLit can find more similar medical terms than those trained from GloVe and Google News. The intrinsic quantitative evaluation verifies that the semantic similarity captured by the word embeddings trained from EHR is closer to human expertsâ€™ judgments on all four tested datasets. The extrinsic quantitative evaluation shows that the word embeddings trained on EHR achieved the best F1 score of 0.900 for the clinical IE task; no word embeddings improved the performance for the biomedical IR task; and the word embeddings trained on Google News had the best overall F1 score of 0.790 for the RE task. Conclusion Based on the evaluation results, we can draw the following conclusions. First, the word embeddings trained from EHR and MedLit can capture the semantics of medical terms better, and find semantically relevant medical terms closer to human expertsâ€™ judgments than those trained from GloVe and Google News. Second, there does not exist a consistent global ranking of word embeddings for all downstream biomedical NLP applications. However, adding word embeddings as extra features will improve results on most downstream tasks. Finally, the word embeddings trained from the biomedical domain corpora do not necessarily have better performance than those trained from the general domain corpora for any downstream biomedical NLP task.",,"Biomedical Text, Information extraction, Information retrieval, Machine learning, Natural language processing, Word embeddings",,
7,wang_dqnviz:_2019,"Wang, Junpeng; Gou, Liang; Shen, Han-Wei; Yang, Hao",DQNViz: A Visual Analytics Approach to Understand Deep Q-Networks,IEEE Transactions on Visualization and Computer Graphics,January,2019,https://ieeexplore.ieee.org/document/8454905/,"Deep Q-Network (DQN), as one type of deep reinforcement learning model, targets to train an intelligent agent that acquires optimal actions while interacting with an environment. The model is well known for its ability to surpass professional human players across many Atari 2600 games. Despite the superhuman performance, in-depth understanding of the model and interpreting the sophisticated behaviors of the DQN agent remain to be challenging tasks, due to the long-time model training process and the large number of experiences dynamically generated by the agent. In this work, we propose DQNViz, a visual analytics system to expose details of the blind training process in four levels, and enable users to dive into the large experience space of the agent for comprehensive analysis. As an initial attempt in visualizing DQN models, our work focuses more on Atari games with a simple action space, most notably the Breakout game. From our visual analytics of the agentâ€™s experiences, we extract useful action/reward patterns that help to interpret the model and control the training. Through multiple case studies conducted together with deep learning experts, we demonstrate that DQNViz can effectively help domain experts to understand, diagnose, and potentially improve DQN models. Index Termsâ€”Deep Q-Network (DQN), reinforcement learning, model interpretation, visual analytics.",,,,
7,wang_irgan:_2017,"Wang, Jun; Yu, Lantao; Zhang, Weinan; Gong, Yu; Xu, Yinghui; Wang, Benyou; Zhang, Peng; Zhang, Dell",IRGAN: A Minimax Game for Unifying Generative and Discriminative Information Retrieval Models,Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval - SIGIR '17,,2017,http://arxiv.org/abs/1705.10513,"This paper provides a unified account of two schools of thinking in information retrieval modelling: the generative retrieval focusing on predicting relevant documents given a query, and the discriminative retrieval focusing on predicting relevancy given a query-document pair. We propose a game theoretical minimax game to iteratively optimise both models. On one hand, the discriminative model, aiming to mine signals from labelled and unlabelled data, provides guidance to train the generative model towards fitting the underlying relevance distribution over documents given the query. On the other hand, the generative model, acting as an attacker to the current discriminative model, generates difficult examples for the discriminative model in an adversarial way by minimising its discrimination objective. With the competition between these two models, we show that the unified framework takes advantage of both schools of thinking: (i) the generative model learns to fit the relevance distribution over documents via the signals from the discriminative model, and (ii) the discriminative model is able to exploit the unlabelled data selected by the generative model to achieve a better estimation for document ranking. Our experimental results have demonstrated significant performance gains as much as 23.96\% on Precision@5 and 15.50\% on MAP over strong baselines in a variety of applications including web search, item recommendation, and question answering.",,"Computer Science - Information Retrieval, Computer Science - Machine Learning",,
7,wang_natural_2019,"Wang, Yanshan; Mehrabi, Saeed; Sohn, Sunghwan; Atkinson, Elizabeth J.; Amin, Shreyasee; Liu, Hongfang",Natural language processing of radiology reports for identification of skeletal site-specific fractures,BMC Medical Informatics and Decision Making,April,2019,https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-019-0780-5,"Background: Osteoporosis has become an important public health issue. Most of the population, particularly elderly people, are at some degree of risk of osteoporosis-related fractures. Accurate identification and surveillance of patient populations with fractures has a significant impact on reduction of cost of care by preventing future fractures and its corresponding complications. Methods: In this study, we developed a rule-based natural language processing (NLP) algorithm for identification of twenty skeletal site-specific fractures from radiology reports. The rule-based NLP algorithm was based on regular expressions developed using MedTagger, an NLP tool of the Apache Unstructured Information Management Architecture (UIMA) pipeline to facilitate information extraction from clinical narratives. Radiology notes were retrieved from the Mayo Clinic electronic health records data warehouse. We developed rules for identifying each fracture type according to physiciansâ€™ knowledge and experience, and refined these rules via verification with physicians. This study was approved by the institutional review board (IRB) for human subject research. Results: We validated the NLP algorithm using the radiology reports of a community-based cohort at Mayo Clinic with the gold standard constructed by medical experts. The micro-averaged results of sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV), and F1-score of the proposed NLP algorithm are 0.930, 1.0, 1.0, 0.941, 0.961, respectively. The F1-score is 1.0 for 8 fractures, and above 0.9 for a total of 17 out of 20 fractures (85\%). Conclusions: The results verified the effectiveness of the proposed rule-based NLP algorithm in automatic identification of osteoporosis-related skeletal site-specific fractures from radiology reports. The NLP algorithm could be utilized to accurately identify the patients with fractures and those who are also at high risk of future fractures due to osteoporosis. Appropriate care interventions to those patients, not only the most at-risk patients but also those with emerging risk, would significantly reduce future fractures.",,,,
7,wang_structbert_2019,"Wang, Wei; Bi, Bin; Yan, Ming; Wu, Chen; Bao, Zuyi; Xia, Jiangnan; Peng, Liwei; Si, Luo",StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding,arXiv:1908.04577 [cs],September,2019,http://arxiv.org/abs/1908.04577,"Recently, the pre-trained language model, BERT (and its robustly optimized version RoBERTa), has attracted a lot of attention in natural language understanding (NLU), and achieved state-of-the-art accuracy in various NLU tasks, such as sentiment classification, natural language inference, semantic textual similarity and question answering. Inspired by the linearization exploration work of Elman [8], we extend BERT to a new model, StructBERT, by incorporating language structures into pre-training. Specifically, we pre-train StructBERT with two auxiliary tasks to make the most of the sequential order of words and sentences, which leverage language structures at the word and sentence levels, respectively. As a result, the new model is adapted to different levels of language understanding required by downstream tasks. The StructBERT with structural pre-training gives surprisingly good empirical results on a variety of downstream tasks, including pushing the state-of-the-art on the GLUE benchmark to 89.0 (outperforming all published models), the F1 score on SQuAD v1.1 question answering to 93.0, the accuracy on SNLI to 91.7.",,Computer Science - Computation and Language,,
7,wang_structured_2019,"Wang, Ziheng; Wohlwend, Jeremy; Lei, Tao",Structured Pruning of Large Language Models,"arXiv:1910.04732 [cs, stat]",October,2019,http://arxiv.org/abs/1910.04732,"Large language models have recently achieved state of the art performance across a wide variety of natural language tasks. Meanwhile, the size of these models and their latency have significantly increased, which makes their usage costly, and raises an interesting question: do language models need to be large? We study this question through the lens of model compression. We present a novel, structured pruning approach based on low rank factorization and augmented Lagrangian L0 norm regularization. Our structured approach achieves significant inference speedups while matching or outperforming our unstructured pruning baseline at various sparsity levels. We apply our method to state of the art models on the enwiki8 dataset and obtain a 1.19 perplexity score with just 5M parameters, vastly outperforming a model of the same size trained from scratch. We also demonstrate that our method can be applied to language model fine-tuning by pruning the BERT model on several downstream classification benchmarks.",,"Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning",,
7,wood_automated_2020,"Wood, David A.; Lynch, Jeremy; Kafiabadi, Sina; Guilhem, Emily; Busaidi, Aisha Al; Montvila, Antanas; Varsavsky, Thomas; Siddiqui, Juveria; Gadapa, Naveen; Townend, Matthew; Kiik, Martin; Patel, Keena; Barker, Gareth; Ourselin, Sebastian; Cole, James H.; Booth, Thomas C.",Automated Labelling using an Attention model for Radiology reports of MRI scans (ALARM),arXiv:2002.06588 [cs],February,2020,http://arxiv.org/abs/2002.06588,"Labelling large datasets for training high-capacity neural networks is a major obstacle to the development of deep learning-based medical imaging applications. Here we present a transformer-based network for magnetic resonance imaging (MRI) radiology report classiï¬Åcation which automates this task by assigning image labels on the basis of free-text expert radiology reports. Our modelâ€™s performance is comparable to that of an expert radiologist, and better than that of an expert physician, demonstrating the feasibility of this approach. We make code available online for researchers to label their own MRI datasets for medical imaging applications.",,Computer Science - Computer Vision and Pattern Recognition,,
7,wu_googles_2016,"Wu, Yonghui; Schuster, Mike; Chen, Zhifeng; Le, Quoc V.; Norouzi, Mohammad; Macherey, Wolfgang; Krikun, Maxim; Cao, Yuan; Gao, Qin; Macherey, Klaus; Klingner, Jeff; Shah, Apurva; Johnson, Melvin; Liu, Xiaobing; Kaiser, ÅÅukasz; Gouws, Stephan; Kato, Yoshikiyo; Kudo, Taku; Kazawa, Hideto; Stevens, Keith; Kurian, George; Patil, Nishant; Wang, Wei; Young, Cliff; Smith, Jason; Riesa, Jason; Rudnick, Alex; Vinyals, Oriol; Corrado, Greg; Hughes, Macduff; Dean, Jeffrey",Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation,arXiv:1609.08144 [cs],October,2016,http://arxiv.org/abs/1609.08144,"Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference â€“ sometimes prohibitively so in the case of very large data sets and large models. Several authors have also charged that NMT systems lack robustness, particularly when input sentences contain rare words. These issues have hindered NMTâ€™s use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Googleâ€™s Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using residual connections as well as attention connections from the decoder network to the encoder. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the ï¬Ånal translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units (â€œwordpiecesâ€ù) for both input and output. This method provides a good balance between the ï¬‚exibility of â€œcharacterâ€ù-delimited models and the eï¬ƒciency of â€œwordâ€ù-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. To directly optimize the translation BLEU scores, we consider reï¬Åning the models by using reinforcement learning, but we found that the improvement in the BLEU scores did not reï¬‚ect in the human evaluation. On the WMTâ€™14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60\% compared to Googleâ€™s phrase-based production system.",,"Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning",,
7,xhonneux_continuous_nodate,"Xhonneux, Louis-Pascal A. C.; Qu, Meng; Tang, Jian",Continuous Graph Neural Networks,,,,,"This paper builds on the connection between graph neural networks and traditional dynamical systems. We propose continuous graph neural networks (CGNN), which generalise existing graph neural networks with discrete dynamics in that they can be viewed as a speciï¬Åc discretisation scheme. The key idea is how to characterise the continuous dynamics of node representations, i.e. the derivatives of node representations, w.r.t. time. Inspired by existing diffusion-based methods on graphs (e.g. PageRank and epidemic models on social networks), we deï¬Åne the derivatives as a combination of the current node representations, the representations of neighbors, and the initial values of the nodes. We propose and analyse two possible dynamics on graphsâ€”including each dimension of node representations (a.k.a. the feature channel) change independently or interact with each otherâ€”both with theoretical justiï¬Åcation. The proposed continuous graph neural networks are robust to over-smoothing and hence allow us to build deeper networks, which in turn are able to capture the long-range dependencies between nodes. Experimental results on the task of node classiï¬Åcation demonstrate the effectiveness of our proposed approach over competitive baselines.",,,,
7,xie_introducing_2019,"Xie, Zhe; Yang, Yuanyuan; Wang, Mingqing; Li, Ming; Huang, Haozhe; Zheng, Dezhong; Shu, Rong; Ling, Tonghui",Introducing Information Extraction to Radiology Information Systems to Improve the Efficiency on Reading Reports,Methods of Information in Medicine,,2019,,"BACKGROUND: Radiology reports are a permanent record of patient's health information often used in clinical practice and research. Reading radiology reports is common for clinicians and radiologists. However, it is laborious and time-consuming when the amount of reports to be read is large. Assisting clinicians to locate and assimilate the key information of reports is of great significance for improving the efficiency of reading reports. There are few studies on information extraction from Chinese medical texts and its application in radiology information systems (RIS) for efficiency improvement. OBJECTIVES: The purpose of this study was to explore methods for extracting, grouping, ranking, delivering, and displaying medical-named entities in radiology reports which can yield efficiency improvement in RISs. METHODS: A total of 5,000 reports were obtained from two medical institutions for this study. We proposed a neural network model called Multi-Embedding-BGRU-CRF (bidirectional gated recurrent unit-conditional random field) for medical-named entity recognition and rule-based methods for entity grouping and ranking. Furthermore, a methodology for delivering and displaying entities in RISs was presented. RESULTS: The proposed neural named entity recognition model has achieved a good F1 score of 95.88\%. Entity ranking achieved a very high accuracy of 99.23\%. The weakness of the system is the entity grouping approach which yield accuracy of 91.03\%. The effectiveness of the overall solution was proved by an evaluation task performed by two clinicians based on the setup of actual clinical practice. CONCLUSIONS: The neural model shows great potential in extracting medical-named entities from radiology reports, especially for languages, that lack lexicons and natural language processing tools. The pipeline of extracting, grouping, ranking, delivering, and displaying medical-named entities could be a feasible solution to enhance RIS functionality by information extraction. The integration of information extraction and RIS has been demonstrated to be effective in improving the efficiency of reading radiology reports.",,"Data Mining, Hospitals, Humans, Models, Theoretical, Radiology Information Systems, Reading, Research Report",,
7,xie_unsupervised_2019,"Xie, Qizhe; Dai, Zihang; Hovy, Eduard; Luong, Minh-Thang; Le, Quoc V.",Unsupervised Data Augmentation for Consistency Training,"arXiv:1904.12848 [cs, stat]",September,2019,http://arxiv.org/abs/1904.12848,"Semi-supervised learning lately has shown much promise in improving deep learning models when labeled data is scarce. Common among recent approaches is the use of consistency training on a large amount of unlabeled data to constrain model predictions to be invariant to input noise. In this work, we present a new perspective on how to effectively noise unlabeled examples and argue that the quality of noising, specifically those produced by advanced data augmentation methods, plays a crucial role in semi-supervised learning. By substituting simple noising operations with advanced data augmentation methods, our method brings substantial improvements across six language and three vision tasks under the same consistency training framework. On the IMDb text classification dataset, with only 20 labeled examples, our method achieves an error rate of 4.20, outperforming the state-of-the-art model trained on 25,000 labeled examples. On a standard semi-supervised learning benchmark, CIFAR-10, our method outperforms all previous approaches and achieves an error rate of 2.7\% with only 4,000 examples, nearly matching the performance of models trained on 50,000 labeled examples. Our method also combines well with transfer learning, e.g., when finetuning from BERT, and yields improvements in high-data regime, such as ImageNet, whether when there is only 10\% labeled data or when a full labeled set with 1.3M extra unlabeled examples is used. Code is available at https://github.com/google-research/uda.",,"Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning",,
7,xin_accelerating_2018,"Xin, Doris; Ma, Litian; Liu, Jialin; Macke, Stephen; Song, Shuchen; Parameswaran, Aditya",Accelerating Human-in-the-loop Machine Learning Challenges and Opportunities,arXiv:2001.03554 [cs],April,2018,https://arxiv.org/pdf/1804.05892.pdf,"Development of machine learning (ML) workflows is a tedious process of iterative experimentation: developers repeatedly make changes to workflows until the desired accuracy is attained. We describe our vision for a â€œhuman-in-the-loopâ€ù ML system that accelerates this process: by intelligently tracking changes and intermediate results over time, such a system can enable rapid iteration, quick responsive feedback, introspection and debugging, and background execution and automation. We finally describe Helix, our preliminary attempt at such a system that has already led to speedups of upto 10Ã— on typical iterative workflows against competing systems.",,,,
7,yang_leveraging_2018,"Yang, Jie; Drake, Thomas; Damianou, Andreas; Maarek, Yoelle",Leveraging Crowdsourcing Data For Deep Active Learning An Application: Learning Intents in Alexa,Crowdsourcing and Human Computation for the Web,April,2018,https://dl.acm.org/doi/abs/10.1145/3178876.3186033,"This paper presents a generic Bayesian framework that enables any deep learning model to actively learn from targeted crowds. Our framework inherits from recent advances in Bayesian deep learning, and extends existing work by considering the targeted crowdsourcing approach, where multiple annotators with unknown expertise contribute an uncontrolled amount (often limited) of annotations. Our framework leverages the low-rank structure in annotations to learn individual annotator expertise, which then helps to infer the true labels from noisy and sparse annotations. It provides a unified Bayesian model to simultaneously infer the true labels and train the deep learning model in order to reach an optimal learning efficacy. Finally, our framework exploits the uncertainty of the deep learning model during prediction as well as the annotatorsâ€™ estimated expertise to minimize the number of required annotations and annotators for optimally training the deep learning model. We evaluate the effectiveness of our framework for intent classification in Alexa (Amazonâ€™s personal assistant), using both synthetic and real-world datasets. Experiments show that our framework can accurately learn annotator expertise, infer true labels, and effectively reduce the amount of annotations in model training as compared to state-of-the-art approaches. We further discuss the potential of our proposed framework in bridging machine learning and crowdsourcing towards improved human-in-the-loop systems.",,,,
7,yang_libact_nodate,"Yang, Yao-Yuan; Lee, Shao-Chuan; Chung, Yu-An; Wu, Tung-En; Chen, Si-An; Lin, Hsuan-Tien",libact: Pool-based Active Learning in Python,arXiv:1710.00379 [cs.LG],,,https://arxiv.org/abs/1710.00379,"libact is a Python package designed to make active learning easier for general users. The package not only implements several popular active learning strategies, but also features the active-learning-by-learning meta-algorithm that assists the users to automatically select the best strategy on the fly. Furthermore, the package provides a unified interface for implementing more strategies, models and application-specific labelers. The package is open-source on Github, and can be easily installed from Python Package Index repository.",,,,
7,yang_simple_2019,"Yang, Wei; Zhang, Haotian; Lin, Jimmy",Simple Applications of BERT for Ad Hoc Document Retrieval,arXiv:1903.10972 [cs],March,2019,http://arxiv.org/abs/1903.10972,"Following recent successes in applying BERT to question answering, we explore simple applications to ad hoc document retrieval. This required confronting the challenge posed by documents that are typically longer than the length of input BERT was designed to handle. We address this issue by applying inference on sentences individually, and then aggregating sentence scores to produce document scores. Experiments on TREC microblog and newswire test collections show that our approach is simple yet effective, as we report the highest average precision on these datasets by neural approaches that we are aware of.",,"Computer Science - Computation and Language, Computer Science - Information Retrieval",,
7,yang_xlnet_2020,"Yang, Zhilin; Dai, Zihang; Yang, Yiming; Carbonell, Jaime; Salakhutdinov, Ruslan; Le, Quoc V.",XLNet: Generalized Autoregressive Pretraining for Language Understanding,arXiv:1906.08237 [cs],January,2020,http://arxiv.org/abs/1906.08237,"With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.",,"Computer Science - Computation and Language, Computer Science - Machine Learning",,
7,yang_xlnet:_2019,"Yang, Zhilin; Dai, Zihang; Yang, Yiming; Carbonell, Jaime; Salakhutdinov, Ruslan; Le, Quoc V.",XLNet: Generalized Autoregressive Pretraining for Language Understanding,arXiv:1906.08237 [cs],June,2019,http://arxiv.org/abs/1906.08237,"With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, XLNet outperforms BERT on 20 tasks, often by a large margin, and achieves state-of-the-art results on 18 tasks including question answering, natural language inference, sentiment analysis, and document ranking.",,"Computer Science - Computation and Language, Computer Science - Machine Learning",,
7,yetisgen-yildiz_text_2013,"Yetisgen-Yildiz, Meliha; Gunn, Martin L.; Xia, Fei; Payne, Thomas H.",A text processing pipeline to extract recommendations from radiology reports,Journal of Biomedical Informatics,April,2013,https://linkinghub.elsevier.com/retrieve/pii/S1532046413000038,"Communication of follow-up recommendations when abnormalities are identiï¬Åed on imaging studies is prone to error. The absence of an automated system to identify and track radiology recommendations is an important barrier to ensuring timely follow-up of patients especially with non-acute incidental ï¬Åndings on imaging examinations. In this paper, we present a text processing pipeline to automatically identify clinically important recommendation sentences in radiology reports. Our extraction pipeline is based on natural language processing (NLP) and supervised text classiï¬Åcation methods. To develop and test the pipeline, we created a corpus of 800 radiology reports double annotated for recommendation sentences by a radiologist and an internist. We ran several experiments to measure the impact of different feature types and the data imbalance between positive and negative recommendation sentences. Our fully statistical approach achieved the best f-score 0.758 in identifying the critical recommendation sentences in radiology reports.",,,,
7,ying_towards_nodate,"Ying, Ding; Jiang, Jing",Towards Opinion Summarization from Online Forums,,,,,"Summarizing opinions expressed in online forums can potentially beneï¬Åt many people. However, special characteristics of this problem may require changes to standard text summarization techniques. In this work, we present our initial attempt at extractive summarization of opinionated online forum threads. Given the nature of user generated content in online discussion forums, we hypothesize that besides relevance, text quality and subjectivity also play important roles in deciding which sentences are good summary sentences. We therefore construct an annotated corpus to facilitate our study of extractive summarization of online discussion forums. We deï¬Åne a set of features to capture relevance, text quality and subjectivity, and empirically test their usefulness in choosing summary sentences. Using unpaired Studentâ€™s t-test, we ï¬Ånd that sentence length and number of sentiment words have high correlations with good summary sentences. Finally we propose some simple modiï¬Åcations to a standard Integer Linear Programming based summarization framework to incorporate these features.",,,,
7,yoo_learning_2019,"Yoo, Donggeun; Kweon, In So",Learning Loss for Active Learning,IEEE Conference on Computer Vision and Pattern Recognition (CVPR),May,2019,http://openaccess.thecvf.com/content_CVPR_2019/html/Yoo_Learning_Loss_for_Active_Learning_CVPR_2019_paper.html,"The performance of deep neural networks improves with more annotated data. The problem is that the budget for annotation is limited. One solution to this is active learning, where a model asks human to annotate data that it perceived as uncertain. A variety of recent methods have been proposed to apply active learning to deep networks but most of them are either designed specific for their target tasks or computationally inefficient for large networks. In this paper, we propose a novel active learning method that is simple but task-agnostic, and works efficiently with the deep networks. We attach a small parametric module, named â€œloss prediction module,â€ù to a target network, and learn it to predict target losses of unlabeled inputs. Then, this module can suggest data that the target model is likely to produce a wrong prediction. This method is task-agnostic as networks are learned from a single loss regardless of target tasks. We rigorously validate our method through image classification, object detection, and human pose estimation, with the recent network architectures. The results demonstrate that our method consistently outperforms the previous methods over the tasks.",,,,
7,yoshida_analysis_2018,"Yoshida, Mitsuo; Toriumi, Fujio",Analysis of Political Party Twitter Accounts' Retweeters During Japan's 2017 Election,2018 IEEE/WIC/ACM International Conference on Web Intelligence (WI),December,2018,http://arxiv.org/abs/1812.06698,"In modern election campaigns, political parties utilize social media to advertise their policies and candidates and to communicate to the electorate. In Japanâ€™s latest general election in 2017, the 48th general election for the Lower House, social media, especially Twitter, was actively used. In this paper, we analyze the users who retweeted tweets of political parties on Twitter during the election. Our aim is to clarify what kinds of users are diffusing (retweeting) tweets of political parties. The results indicate that the characteristics of retweeters of the largest ruling party (Liberal Democratic Party of Japan) and the largest opposition party (The Constitutional Democratic Party of Japan) were similar, even though the retweeters did not overlap each other. We also found that a particular opposition party (Japanese Communist Party) had quite different characteristics from other political parties.",,Computer Science - Social and Information Networks,,
7,zamani_neural_2017,"Zamani, Hamed; Mitra, Bhaskar; Song, Xia; Craswell, Nick; Tiwary, Saurabh",Neural Ranking Models with Multiple Document Fields,arXiv:1711.09174 [cs],November,2017,http://arxiv.org/abs/1711.09174,"Deep neural networks have recently shown promise in the ad-hoc retrieval task. However, such models have often been based on one field of the document, for example considering document title only or document body only. Since in practice documents typically have multiple fields, and given that non-neural ranking models such as BM25F have been developed to take advantage of document structure, this paper investigates how neural models can deal with multiple document fields. We introduce a model that can consume short text fields such as document title and long text fields such as document body. It can also handle multi-instance fields with variable number of instances, for example where each document has zero or more instances of incoming anchor text. Since fields vary in coverage and quality, we introduce a masking method to handle missing field instances, as well as a field-level dropout method to avoid relying too much on any one field. As in the studies of non-neural field weighting, we find it is better for the ranker to score the whole document jointly, rather than generate a per-field score and aggregate. We find that different document fields may match different aspects of the query and therefore benefit from comparing with separate representations of the query text. The combination of techniques introduced here leads to a neural ranker that can take advantage of full document structure, including multiple instance and missing instance data, of variable length. The techniques significantly enhance the performance of the ranker, and also outperform a learning to rank baseline with hand-crafted features.",,Computer Science - Information Retrieval,,
7,zaragoza_bayesian_nodate,"Zaragoza, Hugo; Hiemstra, Djoerd; Tipping, Michael",Bayesian Extension to the Language Model for Ad Hoc,,,,,"We propose a Bayesian extension to the ad-hoc Language Model. Many smoothed estimators used for the multinomial query model in ad-hoc Language Models (including Laplace and Bayes-smoothing) are approximations to the Bayesian predictive distribution. In this paper we derive the full predictive distribution in a form amenable to implementation by classical IR models, and then compare it to other currently used estimators. In our experiments the proposed model outperforms Bayes-smoothing, and its combination with linear interpolation smoothing outperforms all other estimators.",,,,
7,zech_natural_2018,"Zech, John; Pain, Margaret; Titano, Joseph; Badgeley, Marcus; Schefflein, Javin; Su, Andres; Costa, Anthony; Bederson, Joshua; Lehar, Joseph; Oermann, Eric Karl",Natural Languageâ€“based Machine Learning Models for the Annotation of Clinical Radiology Reports,Radiology,May,2018,http://pubs.rsna.org/doi/10.1148/radiol.2018171093,,,,,
7,zhang_biowordvec_2019,"Zhang, Yijia; Chen, Qingyu; Yang, Zhihao; Lin, Hongfei; Lu, Zhiyong","BioWordVec, improving biomedical word embeddings with subword information and MeSH",Scientific Data,May,2019,https://www.nature.com/articles/s41597-019-0055-0,Design Type(s)   data transformation objective â€¢ data integration objective â€¢ text processing and analysis objective    Measurement Type(s)   word representation    Technology Type(s)   Text\_Mining    Factor Type(s)     Sample Characteristic(s)             Machine-accessible metadata file describing the reported data (ISA-Tab format),,"Biomedical Text, embeddings",,
7,zhang_how_2019,"Zhang, Shanshan; He, Lihong; Dragut, Eduard; Vucetic, Slobodan",How to Invest my Time: Lessons from Human-in-the-Loop Entity Extraction,KDD '19: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining,July,2019,https://dl.acm.org/doi/10.1145/3292500.3330773,"Recognizing entities that follow or closely resemble a regular expression (regex) pattern is an important task in information extraction. Common approaches for extraction of such entities require humans to either write a regex recognizing an entity or manually label entity mentions in a document corpus. While human effort is critical to build an entity recognition model, surprisingly little is known about how to best invest that effort given a limited time budget. To get an answer, we consider an iterative human-in-theloop (HIL) framework that allows users to write a regex or manually label entity mentions, followed by training and refining a classifier based on the provided information. We demonstrate on 5 entity recognition tasks that classification accuracy improves over time with either approach. When a user is allowed to choose between regex construction and manual labeling, we discover that (1) if the time budget is low, spending all time for regex construction is often advantageous, (2) if the time budget is high, spending all time for manual labeling seems to be superior, and (3) between those two extremes, writing regexes followed by manual labeling is typically the best approach. Our code and data can be found through https://github.com/nymph332088/HILRecognizer.",,,,
7,zhang_keywords_2020,"Zhang, Yu; Tuo, Mingxiang; Yin, Qingyu",Keywords extraction with deep neural network model,Neurocomputing,March,2020,https://www.sciencedirect.com/science/article/abs/pii/S092523121931687X,,,,,
7,zhang_pretraining-based_2019,"Zhang, Haoyu; Xu, Jianjun; Wang, Ji",Pretraining-Based Natural Language Generation for Text Summarization,arXiv:1902.09243 [cs],April,2019,http://arxiv.org/abs/1902.09243,"In this paper, we propose a novel pretraining-based encoder-decoder framework, which can generate the output sequence based on the input sequence in a two-stage manner. For the encoder of our model, we encode the input sequence into context representations using BERT. For the decoder, there are two stages in our model, in the ï¬Årst stage, we use a Transformer-based decoder to generate a draft output sequence. In the second stage, we mask each word of the draft sequence and feed it to BERT, then by combining the input sequence and the draft representation generated by BERT, we use a Transformer-based decoder to predict the reï¬Åned word for each masked position. To the best of our knowledge, our approach is the ï¬Årst method which applies the BERT into text generation tasks. As the ï¬Årst step in this direction, we evaluate our proposed method on the text summarization task. Experimental results show that our model achieves new state-of-the-art on both CNN/Daily Mail and New York Times datasets.",,"Computer Science - Artificial Intelligence, Computer Science - Computation and Language",,
7,zhao_gender_2018,"Zhao, Jieyu; Wang, Tianlu; Yatskar, Mark; Ordonez, Vicente; Chang, Kai-Wei",Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods,arXiv:1804.06876 [cs],April,2018,http://arxiv.org/abs/1804.06876,"We introduce a new benchmark, WinoBias, for coreference resolution focused on gender bias. Our corpus contains Winograd-schema style sentences with entities corresponding to people referred by their occupation (e.g. the nurse, the doctor, the carpenter). We demonstrate that a rule-based, a feature-rich, and a neural coreference system all link gendered pronouns to pro-stereotypical entities with higher accuracy than anti-stereotypical entities, by an average difference of 21.1 in F1 score. Finally, we demonstrate a data-augmentation approach that, in combination with existing word-embedding debiasing techniques, removes the bias demonstrated by these systems in WinoBias without significantly affecting their performance on existing coreference benchmark datasets. Our dataset and code are available at http://winobias.org.",,"Computer Science - Artificial Intelligence, Computer Science - Computation and Language",,
7,zhao_using_2009,"Zhao, Lin; Wu, Lide; Huang, Xuanjing",Using query expansion in graph-based approach for query-focused multi-document summarization,Information Processing \& Management,January,2009,http://www.sciencedirect.com/science/article/pii/S030645730800071X,"This paper presents a novel query expansion method, which is combined in the graph-based algorithm for query-focused multi-document summarization, so as to resolve the problem of information limit in the original query. Our approach makes use of both the sentence-to-sentence relations and the sentence-to-word relations to select the query biased informative words from the document set and use them as query expansions to improve the sentence ranking result. Compared to previous query expansion approaches, our approach can capture more relevant information with less noise. We performed experiments on the data of document understanding conference (DUC) 2005 and DUC 2006, and the evaluation results show that the proposed query expansion method can significantly improve the system performance and make our system comparable to the state-of-the-art systems.",,"Graph-based ranking, Query expansion, Query-focused summarization, summarization",,
7,zhdanov_diverse_2019,"Zhdanov, Fedor",Diverse mini-batch Active Learning,arXiv:1901.05954,January,2019,https://arxiv.org/abs/1901.05954,"We study the problem of reducing the amount of labeled training data required to train supervised classification models. We approach it by leveraging Active Learning, through sequential selection of examples which benefit the model most. Selecting examples one by one is not practical for the amount of training examples required by the modern Deep Learning models. We consider the mini-batch Active Learning setting, where several examples are selected at once. We present an approach which takes into account both informativeness of the examples for the model, as well as the diversity of the examples in a mini-batch. By using the well studied K-means clustering algorithm, this approach scales better than the previously proposed approaches, and achieves comparable or better performance.",,,,
7,zheng_identifying_2019,"Zheng, Shuangjia; Yan, Xin; Yang, Yuedong; Xu, Jun",Identifying Structureâ€“Property Relationships through SMILES Syntax Analysis with Self-Attention Mechanism,Journal of Chemical Information and Modeling,February,2019,http://pubs.acs.org/doi/10.1021/acs.jcim.8b00803,,,,,
7,zhou_summarization_nodate,"Zhou, Liang",On the Summarization of Dynamically Introduced Information: Online Discussions and Blogs,,,,,"In this paper we describe computational approaches to summarizing dynamically introduced information: online discussions and blogs, and their evaluations. Research in the past has been mainly focused on text-based summarization where the input data is predominantly newswire data. When branching into these newly emerged data types, we face number of difficulties that are discussed here.",,Old,,
7,zhu_introducing_2012,"Zhu, Y.",Introducing Google Chart Tools and Google Maps API in Data Visualization Courses,IEEE Computer Graphics and Applications,November,2012,,,,"Computer Graphics, Data visualization, Georgia State University, Google, Google Chart Tools, Google Fusion Tables, Google Maps API, Google chart tools, Humans, Information Science, Internet, Maps as Topic, Search Engine, Software, Students, Web sites, application program interfaces, cartography, charts, computer aided instruction, computer graphics, data visualisation, data visualization, data visualization courses, interactive data visualizations, interactive systems, processing, visualization education, visualization toolkits",,
7,zhu_real-time_2017,"Zhu, Xiang; Huang, Jiuming; Zhou, Bin; Li, Aiping; Jia, Yan",Real-time personalized twitter search based on semantic expansion and quality model,Neurocomputing,September,2017,http://www.sciencedirect.com/science/article/pii/S0925231217304034,"The vast amount of information in social networks makes it difficult for users to find what they want, users may get drowned in the information flood. It is a challenging problem to retrieve the high quality and relevant information according to a userâ€™s searching query. Traditional methods for personalized search become insufficient in social networks due to the high velocity, topic variety, data sparseness and high sociability. To overcome those difficulties, we propose a novel framework for real-time personalized twitter search for twitter stream in this paper. Firstly, we develop a boolean logic keyword filter to enhance the accuracy. Then a tweet quality model is built to distinguish high quality tweets, it could improve the ranking performance. After that, we utilize an external search engine to implement query expansion, which could understand user preferences and interests properly. Our framework integrates the semantic features and social attributes which are utilized to make a comprehensive rank for a tweet. In addition, we adopt a dynamic strategy to push high quality and relevant tweets to a user automatically to avoid information overload. A thorough evaluation is conducted using real twitter stream data in TREC 2015, demonstrating a superior performance against competitive baselines in a variety of metrics.",,"Personalized search, Quality model, Semantic computing, Social network",,
7,ziegler_fine-tuning_2020,"Ziegler, Daniel M.; Stiennon, Nisan; Wu, Jeffrey; Brown, Tom B.; Radford, Alec; Amodei, Dario; Christiano, Paul; Irving, Geoffrey",Fine-Tuning Language Models from Human Preferences,"arXiv:1909.08593 [cs, stat]",January,2020,http://arxiv.org/abs/1909.08593,"Reward learning enables the application of reinforcement learning (RL) to tasks where reward is defined by human judgment, building a model of reward by asking humans questions. Most work on reward learning has used simulated environments, but complex information about values is often expressed in natural language, and we believe reward learning for language is a key to making RL practical and safe for real-world tasks. In this paper, we build on advances in generative pretraining of language models to apply reward learning to four natural language tasks: continuing text with positive sentiment or physically descriptive language, and summarization tasks on the TL;DR and CNN/Daily Mail datasets. For stylistic continuation we achieve good results with only 5,000 comparisons evaluated by humans. For summarization, models trained with 60,000 comparisons copy whole sentences from the input but skip irrelevant preamble; this leads to reasonable ROUGE scores and very good performance according to our human labelers, but may be exploiting the fact that labelers rely on simple heuristics.",,"Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning",,
1,gelbukh_computational_2015,,Computational Linguistics and Intelligent Text Processing,, Proceedings,,Google-Books-ID: 8lcSCAAAQBAJ,,"The two volumes LNCS 9041 and 9042 constitute the proceedings of the 16th International Conference on Computational Linguistics and Intelligent Text Processing, CICLing 2015, held in Cairo, Egypt, in April 2015. The total of 95 full papers presented was carefully reviewed and selected from 329 submissions. They were organized in topical sections on grammar formalisms and lexical resources; morphology and chunking; syntax and parsing; anaphora resolution and word sense disambiguation; semantics and dialogue; machine translation and multilingualism; sentiment analysis and emotion detection; opinion mining and social network analysis; natural language generation and text summarization; information retrieval, question answering, and information extraction; text classification; speech processing; and applications.",,"Computers / Databases / Data Mining, Computers / Information Technology, Computers / Intelligence (AI) \& Semantics, Computers / Natural Language Processing, Computers / Speech \& Audio Processing, Computers / System Administration / Storage \& Retrieval, Language Arts \& Disciplines / Linguistics / General",
1,herawan_proceedings_2014,,Proceedings of the First International Conference on Advanced Data and Information Engineering (DaEng-2013),,,2014,http://link.springer.com/10.1007/978-981-4585-18-7,,,,,
1,molnar_interpretable_nodate,"Molnar, Christoph",Interpretable Machine Learning,,,,https://christophm.github.io/interpretable-ml-book/,Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable.,,,,
1,peter_gardenfors_conceptual_2000,Peter Gardenfors,Conceptual Spaces: The Geometry of Thought,,,2000,https://mitpress.mit.edu/books/conceptual-spaces,"Within cognitive science, two approaches currently dominate the problem of modeling representations. The symbolic approach views cognition as computation involving symbolic manipulation. Connectionism, a special case of associationism, models associations using artificial neuron networks. Peter GÃ_rdenfors offers his theory of conceptual representations as a bridge between the symbolic and connectionist approaches.Symbolic representation is particularly weak at modeling concept learning, which is paramount for understanding many cognitive phenomena. Concept learning is closely tied to the notion of similarity, which is also poorly served by the symbolic approach. GÃ_rdenfors's theory of conceptual spaces presents a framework for representing information on the conceptual level. A conceptual space is built up from geometrical structures based on a number of quality dimensions. The main applications of the theory are on the constructive side of cognitive science: as a constructive model the theory can be applied to the development of artificial systems capable of solving cognitive tasks. GÃ_rdenfors also shows how conceptual spaces can serve as an explanatory framework for a number of empirical theories, in particular those concerning concept formation, induction, and semantics. His aim is to present a coherent research program that can be used as a basis for more detailed investigations.",,,,
1,zhao_deep_2017,"Zhao, Wenquan",Deep Active Learning for Short-Text Classification,,,2017,http://urn.kb.se/resolve?urn=urn:nbn:se:kth:diva-212577,"In this paper, we propose a novel active learning algorithm for short-text (Chinese) classification applied to a deep learning architecture. This topic thus belongs to a cross research area between active learning and deep learning. One of the bottlenecks of deep learning for classiï¬Åcation is that it relies on large number of labeled samples, which is expensive and time consuming to obtain. Active learning aims to overcome this disad- vantage through asking the most useful queries in the form of unlabeled samples to be labeled. In other words, active learning intends to achieve precise classiï¬Åcation accuracy using as few labeled samples as possible. Such ideas have been investigated in conventional machine learning algorithms, such as support vector machine (SVM) for image classiï¬Åcation, and in deep neural networks, including convolutional neural networks (CNN) and deep belief networks (DBN) for image classiï¬Åcation. Yet the research on combining active learning with recurrent neural networks (RNNs) for short-text classiï¬Åcation is rare. We demonstrate results for short-text classiï¬Åcation on datasets from Zhuiyi Inc. Importantly, to achieve better classiï¬Åcation accuracy with less computational overhead, the proposed algorithm shows large reductions in the number of labeled training samples compared to random sampling. Moreover, the proposed algorithm is a little bit better than the conventional sampling method, uncertainty sampling. The proposed active learning algorithm dramatically decreases the amount of labeled samples without significantly inï¬‚uencing the test classiï¬Åcation accuracy of the original RNNs classiï¬Åer, trained on the whole data set. In some cases, the proposed algorithm even achieves better classiï¬Åcation accuracy than the original RNNs classiï¬Åer.",,,,
5,cellier_aueb_2020,"Pappas, Dimitris; McDonald, Ryan; Brokos, Georgios-Ioannis; Androutsopoulos, Ion",AUEB at BioASQ 7: Document and Snippet Retrieval,,,2020,http://link.springer.com/10.1007/978-3-030-43887-6_55,"We present the submissions of aueb to the bioasq 7 document and snippet retrieval tasks (parts of Task 7b, Phase A). Our systems build upon the methods we used in bioasq 6. This year we also experimented with models that jointly learn to retrieve documents and snippets, as opposed to using separate pipelined models for document and snippet retrieval. We also experimented with models based on bert [5]. Our systems obtained the best document and snippet retrieval results for all batches of the challenge that we participated in.",,to review,,
5,desolneux_helmholtz_2008,,The Helmholtz Principle,,,2008,https://doi.org/10.1007/978-0-387-74378-3_3,,,"Bibliographic Note, Black Pixel, Exceptional Event, Markov Inequality, Universal Variable",,
5,gelbukh_supervised_2018,"Dermouche, Mohamed; Velcin, Julien; Flicoteaux, RÃ©mi; Chevret, Sylvie; Taright, Namik",Supervised Topic Models for Diagnosis Code Assignment to Discharge Summaries,,,2018,http://link.springer.com/10.1007/978-3-319-75487-1_38,,,,,
5,goos_information_2003,"Setchi, Rossitza; Tang, Qiao; Cheng, Lixin",Information Retrieval Using Deep Natural Language Processing,,,2003,http://link.springer.com/10.1007/978-3-540-45224-9_117,"This paper addresses some problems of the conventional information retrieval (IR) systems, by suggesting an approach to information retrieval that uses deep natural language processing (NLP). The proposed client-side IR system employs a Head-Driven Phrase Grammar (HPSG) formalism and uses Attribute Values Matrices (AVMs) for information storage, representation and communication. The paper describes the architecture and the main processes of the system. The initial experimental results following the implementation of the HPSG processor show that the extraction of semantic information using the HPSG formalism is feasible.",,,,
5,jose_rethinking_2020,"Padaki, Ramith; Dai, Zhuyun; Callan, Jamie",Rethinking Query Expansion for BERT Reranking,,,2020,http://link.springer.com/10.1007/978-3-030-45442-5_37,"Recent studies have shown promising results of using BERT for Information Retrieval with its advantages in understanding the text content of documents and queries. Compared to short, keywords queries, higher accuracy of BERT were observed on long, natural language queries, demonstrating BERTâ€™s ability in extracting rich information from complex queries. These results show the potential of using query expansion to generate better queries for BERT-based rankers. In this work, we explore BERTâ€™s sensitivity to the addition of structure and concepts. We ï¬Ånd that traditional word-based query expansion is not entirely applicable, and provide insight into methods that produce better experimental results.",,to review,,
5,montavon_layer-wise_2019,"Montavon, GrÃ©goire; Binder, Alexander; Lapuschkin, Sebastian; Samek, Wojciech; MÃ_ller, Klaus-Robert",Layer-Wise Relevance Propagation: An Overview,,,2019,https://doi.org/10.1007/978-3-030-28954-6_10,"For a machine learning model to generalize well, one needs to ensure that its decisions are supported by meaningful patterns in the input data. A prerequisite is however for the model to be able to explain itself, e.g. by highlighting which input features it uses to support its prediction. Layer-wise Relevance Propagation (LRP) is a technique that brings such explainability and scales to potentially highly complex deep neural networks. It operates by propagating the prediction backward in the neural network, using a set of purposely designed propagation rules. In this chapter, we give a concise introduction to LRP with a discussion of (1) how to implement propagation rules easily and efficiently, (2) how the propagation procedure can be theoretically justified as a â€˜deep Taylor decompositionâ€™, (3) how to choose the propagation rules at each layer to deliver high explanation quality, and (4) how LRP can be extended to handle a variety of machine learning scenarios beyond deep neural networks.",,"Deep Neural Networks, Deep Taylor Decomposition, Explanations, Layer-wise Relevance Propagation",,
5,pasi_neural_2018,"Ai, Qingyao; Oâ€™Connor, Brendan; Croft, W. Bruce",A Neural Passage Model for Ad-hoc Document Retrieval,,,2018,http://link.springer.com/10.1007/978-3-319-76941-7_41,"Traditional statistical retrieval models often treat each document as a whole. In many cases, however, a document is relevant to a query only because a small part of it contain the targeted information. In this work, we propose a neural passage model (NPM) that uses passagelevel information to improve the performance of ad-hoc retrieval. Instead of using a single window to extract passages, our model automatically learns to weight passages with diï¬€erent granularities in the training process. We show that the passage-based document ranking paradigm from previous studies can be directly derived from our neural framework. Also, our experiments on a TREC collection showed that the NPM can significantly outperform the existing passage-based retrieval models.",,,,
5,robertson_simple_1994,"Robertson, S. E.; Walker, S.",Some Simple Effective Approximations to the 2-Poisson Model for Probabilistic Weighted Retrieval,,,1994,http://link.springer.com/10.1007/978-1-4471-2099-5_24,"The 2-Poisson model for term frequencies is used to suggest ways of incorporating certain variables in probabilistic models for information retrieval. The variables concerned are within-document term tkequency, document length, and within-query term frequency. Simple weighting functions are developed, and tested on the TREC test collection. Considerable performance improvements (over simple inverse collection frequency weighting) are demonstrated.",,,,
5,sep-concepts,"Margolis, Eric; Laurence, Stephen",Stanford Encyclopedia of Philosophy,,,2014,https://plato.stanford.edu/archives/spr2014/entries/concepts/,,,,,
5,stefanakis_google_2008,"Stefanakis, Emmanuel; Patroumpas, Kostas",Google Earth and XML: Advanced Visualization and Publishing of Geographic Information,,,2008,https://doi.org/10.1007/978-3-540-72029-4_10,,,"Digital Terrain Model, Geography Markup Language, Geospatial Data, Open Geospatial Consortium, Virtual Globe",,
5,zhang_deep_2018,"Yang, Zhou; Lan, Qingfeng; Guo, Jiafeng; Fan, Yixing; Zhu, Xiaofei; Lan, Yanyan; Wang, Yue; Cheng, Xueqi",A Deep Top-K Relevance Matching Model for Ad-hoc Retrieval,,,2018,http://link.springer.com/10.1007/978-3-030-01012-6_2,"In this paper, we propose a novel model named DTMM, which is speciï¬Åcally designed for ad-hoc retrieval. Given a query and a document, DTMM ï¬Årstly builds an word-level interaction matrix based on word embeddings from query and document. At the same time, we also compress the embeddings of both document word and query word into a small dimension, to learn the importance of each word. Speciï¬Åcally, the compressed query word embedding is projected into the term gating network, and the compressed document word embedding is concatenated into the interaction matrix. Then, we apply the top-k pooling layer (i.e., ordered k-max pooling) on the interaction matrix, and get the essential top relevance signals. The top relevance signals is associated with each query term, and projected into a multi-layer perceptron neural network to obtain the query term level matching score. Finally, the query term level matching scores are aggregated with the term gating network to produce the ï¬Ånal relevance score. We have tested our model on two representative benchmark datasets. Experimental results show that our model can signiï¬Åcantly outperform existing baseline models.",,,,
6,10.1007/978-3-030-18305-9_35,"Huyghues-Despointes, Charles; Khouas, Leila; Velcin, Julien; Loudcher, Sabine",Weaving information propagation: modeling the way information spreads in document collections,,,2019,,"Information usually spreads between people by the mean of textual documents. During such propagation, a piece of information can either remain the same or mutate. We propose to formulate information spread with a set of time-ordered document chains along which some information has likely been transmitted. This formulation is different from the usual graph view of a transmission process as it integrates a notion of lineage of the information. We also propose a way to construct a candidate set of document chains for the information propagation in a corpus of documents. We show that most of the chains have been judged as plausible by human experts.",,,,
6,acs_evaluating_2016,"ÃÅcs, Judit; Kornai, AndrÃ¡s",Evaluating embeddings on dictionary-based similarity,,August,2016,https://www.aclweb.org/anthology/W16-2514,,,,,
6,agirre_study_2009,"Agirre, Eneko; Alfonseca, Enrique; Hall, Keith; Kravalova, Jana; Pasca, Marius; Soroa, Aitor",A Study on Similarity and Relatedness Using Distributional and WordNet-based Approaches,,June,2009,https://www.aclweb.org/anthology/N09-1003,,,,,
6,ai_learning_2017,"Ai, Qingyao; Zhang, Yongfeng; Bi, Keping; Chen, Xu; Croft, W. Bruce",Learning a Hierarchical Embedding Model for Personalized Product Search,,,2017,http://doi.acm.org/10.1145/3077136.3080813,Loading...,,"latent space model, personalization, product search, representation learning",,
6,akkalyoncu_yilmaz_applying_2019,"Akkalyoncu Yilmaz, Zeynep; Wang, Shengjin; Yang, Wei; Zhang, Haotian; Lin, Jimmy",Applying BERT to Document Retrieval with Birch,,,2019,https://www.aclweb.org/anthology/D19-3004,"We present Birch, a system that applies BERT to document retrieval via integration with the open-source Anserini information retrieval toolkit to demonstrate end-to-end search over large document collections. Birch implements simple ranking models that achieve stateof-the-art effectiveness on standard TREC newswire and social media test collections. This demonstration focuses on technical challenges in the integration of NLP and IR capabilities, along with the design rationale behind our approach to tightly-coupled integration between Python (to support neural networks) and the Java Virtual Machine (to support document retrieval using the open-source Lucene search library). We demonstrate integration of Birch with an existing search interface as well as interactive notebooks that highlight its capabilities in an easy-to-understand manner.",,,,
6,alexandrescu_data-driven_2007,"Alexandrescu, Andrei; Kirchhoff, Katrin",Data-Driven Graph Construction for Semi-Supervised Graph-Based Learning in NLP,,April,2007,https://www.aclweb.org/anthology/N07-1026,,,,,
6,an_deep_2018,"An, Bang; Wu, Wenjun; Han, Huimin",Deep Active Learning for Text Classification,,,2018,http://doi.acm.org/10.1145/3271553.3271578,"In recent years, Active Learning (AL) has been applied in the domain of text classification successfully. However, traditional methods need researchers to pay attention to feature extraction of datasets and different features will influence the final accuracy seriously. In this paper, we propose a new method that uses Recurrent Neutral Network (RNN) as the acquisition function in Active Learning called Deep Active Learning (DAL). For DAL, there is no need to consider how to extract features because RNN can use its internal state to process sequences of inputs. We have proved that DAL can achieve the accuracy that cannot be reached by traditional Active Learning methods when dealing with text classification. What's more, DAL can decrease the need of the great number of labeled instances for Deep Learning (DL). At the same time, we design a strategy to distribute label work to different workers. We have proved by using a proper batch size of instance, we can save much time but not decrease the model's accuracy. Based on this, we provide batch of instances for different workers and the size of batch is determined by worker's ability and scale of dataset, meanwhile, it can be updated with the performance of the workers.",,"Active Learning, Artificial Intelligence, Deep Learning, Machine Learning, Text Classification",,
6,ancona_towards_2018,"Ancona, Marco; Ceolini, Enea; \&\#xD6, Cengiz; ztireli; Gross, Markus",Towards better understanding of gradient-based attribution methods for Deep Neural Networks,,February,2018,https://openreview.net/forum?id=Sy21R9JAW,Understanding the flow of information in Deep Neural Networks (DNNs) is a challenging problem that has gain increasing attention over the last few years. While several methods have been proposed to...,,,,
6,balaneshinkordan_attentive_2018,"Balaneshinkordan, Saeid; Kotov, Alexander; Nikolaev, Fedor",Attentive Neural Architecture for Ad-hoc Structured Document Retrieval,,October,2018,https://dl.acm.org/doi/10.1145/3269206.3271801,"The problem of ad-hoc structured document retrieval arises in many information access scenarios, from Web to product search. Yet neither deep neural networks, which have been successfully applied to ad-hoc information retrieval and Web search, nor the attention mechanism, which has been shown to signi cantly improve the performance of deep neural networks on natural language processing tasks, have been explored in the context of this problem. In this paper, we propose a deep neural architecture for ad-hoc structured document retrieval, which utilizes attention mechanism to determine important phrases in keyword queries as well as the relative importance of matching those phrases in di erent elds of structured documents. Experimental evaluation on publicly available collections for Web document, product and entity retrieval from knowledge graphs indicates superior retrieval accuracy of the proposed neural architecture relative to both state-of-the-art neural architectures for ad-hoc document retrieval and probabilistic models for ad-hoc structured document retrieval.",,,,
6,bohne_efficient_2011,"Bohne, Thomas; RÃ¶nnau, Sebastian; Borghoff, Uwe",Efficient keyword extraction for meaningful document perception,,September,2011,,,,,,
6,butnarua_image_2017,"Butnarua, Andrei",From Image to Text Classification: A Novel Approach based on Clustering Word Embeddings,,,2017,,"In this paper, we propose a novel approach for text classification based on clustering word embeddings, inspired by the bag of visual words model, which is widely used in computer vision. After each word in a collection of documents is represented as word vector using a pre-trained word embeddings model, a k-means algorithm is applied on the word vectors in order to obtain a fixed-size set of clusters. The centroid of each cluster is interpreted as a super word embedding that embodies all the semantically related word vectors in a certain region of the embedding space. Every embedded word in the collection of documents is then assigned to the nearest cluster centroid. In the end, each document is represented as a bag of super word embeddings by computing the frequency of each super word embedding in the respective document. We also diverge from the idea of building a single vocabulary for the entire collection of documents, and propose to build class-specific vocabularies for better performance. Using this kind of representation, we report results on two text mining tasks, namely text categorization by topic and polarity classification. On both tasks, our model yields better performance than the standard bag of words. c 2017 The Authors. Published by Elsevier B.V. Peer-review under responsibility of KES International. Keywords: word embeddings, bag of super word embeddings, clustering word embeddings, bag of word embeddings, kernel methods, polarity classification, text categorization, bag of words, k-means.  1. Introduction With the recent exponential growth of the Internet, there is more and more data that requires efficient processing methods for storing and extracting relevant information. This data is usually unstructured or semi-structured, and comes in different forms, such as images or texts. In order to process larger and larger amounts of data, researchers  need to develop new techniques that can extract relevant information and infer some kind of structure from the avail- able data. One of the principal domains that study such methods is machine learning, but there are many other related  domains that aim to extract useful information from data. There are plenty of research studies 1,2,3,4,5 in this direction. One of the main scopes of machine learning is to define a good representation of the data in order to build accurate  classifiers. In text processing, implementing a simple bag of words (BOW) model to represent a collection of docu- ments can prove to be useful in tasks such as sentiment analysis 6, text categorization7 or information retrieval 4. On  âˆ— Corresponding author. E-mail address: raducu.ionescu@gmail.com 1877-0509 c 2017 The Authors. Published by Elsevier B.V. Peer-review under responsibility of KES International.  Available online at www.sciencedirect.com  Procedia Computer Science 00 (2017) 000â€“000  www.elsevier.com/locate/procedia 21st International Conference on Knowledge Based and Intelligent Information and Engineering  Systems  From Image to Text Classification: A Novel Approach based on  Clustering Word Embeddings Andrei M. Butnarua  , Radu Tudor Ionescua,âˆ—  aFaculty of Mathematics and Computer Science, University of Bucharest, 14 Academiei, Bucharest, Romania  Abstract In this paper, we propose a novel approach for text classification based on clustering word embeddings, inspired by the bag of visual words model, which is widely used in computer vision. After each word in a collection of documents is represented as word vector using a pre-trained word embeddings model, a k-means algorithm is applied on the word vectors in order to obtain a fixed-size set of clusters. The centroid of each cluster is interpreted as a super word embedding that embodies all the semantically related word vectors in a certain region of the embedding space. Every embedded word in the collection of documents is then assigned to the nearest cluster centroid. In the end, each document is represented as a bag of super word embeddings by computing the frequency of each super word embedding in the respective document. We also diverge from the idea of building a single vocabulary for the entire collection of documents, and propose to build class-specific vocabularies for better performance. Using this kind of representation, we report results on two text mining tasks, namely text categorization by topic and polarity classification. On both tasks, our model yields better performance than the standard bag of words.",,,,
6,cao_including_2018,"Cao, Kai; Li, Xiang; Ma, Weicheng; Grishman, Ralph",Including New Patterns to Improve Event Extraction Systems,,,2018,https://aaai.org/ocs/index.php/FLAIRS/FLAIRS18/paper/view/17676,"Event Extraction (EE) is a challenging Information Extraction task which aims to discover event triggers of speciï¬Åc types along with their arguments. Most recent research on Event Extraction relies on pattern-based or feature-based approaches, trained on annotated corpora, to recognize combinations of event triggers, arguments, and other contextual information. However, as the event instances in the ACE corpus are not evenly distributed, some frequent expressions involving ACE event triggers do not appear in the training data, adversely affecting the performance. In this paper, we demonstrate the effectiveness of systematically importing expertlevel patterns from TABARI to boost EE performance. The experimental results demonstrate that our pattern-based system with the expanded patterns can achieve 69.8\% (with 1.9\% absolute improvement) F-measure over the baseline, an advance over current state-of-the-art systems.",,,,
6,cer_semeval-2017_2017,"Cer, Daniel; Diab, Mona; Agirre, Eneko; Lopez-Gazpio, Inigo; Specia, Lucia",SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation,,,2017,http://aclweb.org/anthology/S17-2001,"Semantic Textual Similarity (STS) measures the meaning similarity of sentences. Applications include machine translation (MT), summarization, generation, question answering (QA), short answer grading, semantic search, dialog and conversational systems. The STS shared task is a venue for assessing the current state-of-the-art. The 2017 task focuses on multilingual and cross-lingual pairs with one sub-track exploring MT quality estimation (MTQE) data. The task obtained strong participation from 31 teams, with 17 participating in all language tracks. We summarize performance and review a selection of well performing methods. Analysis highlights common errors, providing insight into the limitations of existing models. To support ongoing work on semantic representations, the STS Benchmark is introduced as a new shared training and evaluation set carefully selected from the corpus of English STS shared task data (2012-2017).",,,,
6,chaplot_knowledge-based_2018,"Chaplot, Devendra Singh; Salakhutdinov, Ruslan",Knowledge-based Word Sense Disambiguation using Topic Models,,,2018,https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17415,"Word Sense Disambiguation is an open problem in Natural Language Processing which is particularly challenging and useful in the unsupervised setting where all the words in any given text need to be disambiguated without using any labeled data. Typically WSD systems use the sentence or a small window of words around the target word as the context for disambiguation because their computational complexity scales exponentially with the size of the context. In this paper, we leverage the formalism of topic model to design a WSD system that scales linearly with the number of words in the context. As a result, our system is able to utilize the whole document as the context for a word to be disambiguated. The proposed method is a variant of Latent Dirichlet Allocation in which the topic proportions for a document are replaced by synset proportions. We further utilize the information in the WordNet by assigning a non-uniform prior to synset distribution over words and a logistic-normal prior for document distribution over synsets. We evaluate the proposed method on Senseval-2, Senseval-3, SemEval-2007, SemEval2013 and SemEval-2015 English All-Word WSD datasets and show that it outperforms the state-of-the-art unsupervised knowledge-based WSD system by a signiï¬Åcant margin.",,,,
6,cho_learning_2014,"Cho, Kyunghyun; van Merrienboer, Bart; Gulcehre, Caglar; Bahdanau, Dzmitry; Bougares, Fethi; Schwenk, Holger; Bengio, Yoshua",Learning Phrase Representations using RNN Encoderâ€“Decoder for Statistical Machine Translation,,,2014,http://aclweb.org/anthology/D14-1179,"In this paper, we propose a novel neural network model called RNN Encoderâ€“Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a ï¬Åxedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoderâ€“Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.",,seq to seq models,,
6,clinchant_information-based_2010,"Clinchant, StÃ©phane; Gaussier, Eric",Information-based models for ad hoc IR,,,2010,http://portal.acm.org/citation.cfm?doid=1835449.1835490,"We introduce in this paper the family of information-based models for ad hoc information retrieval. These models draw their inspiration from a long-standing hypothesis in IR, namely the fact that the diï¬€erence in the behaviors of a word at the document and collection levels brings information on the signiï¬Åcance of the word for the document. This hypothesis has been exploited in the 2-Poisson mixture models, in the notion of eliteness in BM25, and more recently in DFR models. We show here that, combined with notions related to burstiness, it can lead to simpler and better models.",,,,
6,cohen_cross_2018,"Cohen, Daniel; Mitra, Bhaskar; Hofmann, Katja; Croft, W. Bruce",Cross Domain Regularization for Neural Ranking Models Using Adversarial Learning,,,2018,http://doi.acm.org/10.1145/3209978.3210141,"Unlike traditional learning to rank models that depend on hand-crafted features, neural representation learning models learn higher level features for the ranking task by training on large datasets. Their ability to learn new features directly from the data, however, may come at a price. Without any special supervision, these models learn relationships that may hold only in the domain from which the training data is sampled, and generalize poorly to domains not observed during training. We study the effectiveness of adversarial learning as a cross domain regularizer in the context of the ranking task. We use an adversarial discriminator and train our neural ranking model on a small set of domains. The discriminator provides a negative feedback signal to discourage the model from learning domain specific representations. Our experiments show consistently better performance on held out domains in the presence of the adversarial discriminator---sometimes up to 30\% on precision\$@1\$.",,"adversarial learning, deep learning, information retrieval",,
6,conneau_senteval_2018,"Conneau, Alexis; Kiela, Douwe",SentEval: An Evaluation Toolkit for Universal Sentence Representations,,May,2018,http://arxiv.org/abs/1803.05449,"We introduce SentEval, a toolkit for evaluating the quality of universal sentence representations. SentEval encompasses a variety of tasks, including binary and multi-class classification, natural language inference and sentence similarity. The set of tasks was selected based on what appears to be the community consensus regarding the appropriate evaluations for universal sentence representations. The toolkit comes with scripts to download and preprocess datasets, and an easy interface to evaluate sentence encoders. The aim is to provide a fairer, less cumbersome and more centralized way for evaluating sentence representations.",,Computer Science - Computation and Language,,
6,cui_adaptive_2018,"Cui, Wei; Li, Bing; Yang, Xiaochun; Wang, Wei; Wang, Bin; Zhang, Xianchao",An Adaptive Hierarchical Compositional Model for Phrase Embedding,,,2018,https://www.ijcai.org/proceedings/2018/576,"Phrase embedding aims at representing phrases in a vector space and it is important for the per- formance of many NLP tasks. Existing models only regard a phrase as either full-compositional or non-compositional, while ignoring the hybrid- compositionality that widely exists, especially in long phrases. This drawback prevents them from having a deeper insight into the semantic structure for long phrases and as a consequence, weakens the accuracy of the embeddings. In this paper, we present a novel method for jointly learning com- positionality and phrase embedding by adaptively weighting different compositions using an implic- it hierarchical structure. Our model has the ability of adaptively adjusting among different composi- tions without entailing too much model complex- ity and time cost. To the best of our knowledge, our work is the ï¬Årst effort that considers hybrid- compositionality in phrase embedding. The experi- mental evaluation demonstrates that our model out- performs state-of-the-art methods in both similarity tasks and analogy tasks.",,,,
6,cummins_constrained_2016,"Cummins, Ronan; Zhang, Meng; Briscoe, Ted",Constrained Multi-Task Learning for Automated Essay Scoring,,August,2016,https://www.aclweb.org/anthology/P16-1075,,,,,
6,dai_deeper_2019,"Dai, Zhuyun; Callan, Jamie",Deeper Text Understanding for IR with Contextual Neural Language Modeling,,July,2019,https://doi.org/10.1145/3331184.3331303,"Identifying which terms are important with respect to a specific query or document is crucial to Information Retrieval, but context is difficult to capture with traditional term frequency signals. This paper proposes a Deep Contextualized Term Weighting framework (DeepCT) that identifies import terms by taking into consideration the meaning of the term and the role it plays in a specific context. The DeepCT framework is built upon the neural contextualized text representations of BERT[3]. It learns to map the contextualized word representations onto the target term weights in a supervised manner. In DeepCT, a specific term's weight may vary in different textual contexts, reflecting its relations to other words. The contextualized term weights from DeepCT are beneficial to both document and query understanding. On the document side, we propose DeepCT-Index, which uses DeepCT to estimate the importance of each document term, and then uses those estimations to generate term weights that are stored in the index. On the query side, we propose DeepCT-Query, which estimates the importance of each query term and use the weights to generate new query representations. Both the new index and the new query can be used directly by bag-of-words retrieval models such as BM25 and QL, making these methods efficient and easy to incorporate into existing search systems. Experiments demonstrate that DeepCT can greatly improve retrieval accuracy by providing a deeper understanding of a term's importance in a specific document/query context.",,"neural-IR, text understanding",,
6,diaz_query_2016,"Diaz, Fernando; Mitra, Bhaskar; Craswell, Nick",Query Expansion with Locally-Trained Word Embeddings,,August,2016,https://www.aclweb.org/anthology/P16-1035,,,,,
6,dietz_utilizing_2018,"Dietz, Laura; Kotov, Alexander; Meij, Edgar",Utilizing Knowledge Graphs for Text-Centric Information Retrieval,,,2018,http://doi.acm.org/10.1145/3209978.3210187,"The past decade has witnessed the emergence of several publicly available and proprietary knowledge graphs (KGs). The depth and breadth of content in these KGs made them not only rich sources of structured knowledge by themselves, but also valuable resources for search systems. A surge of recent developments in entity linking and entity retrieval methods gave rise to a new line of research that aims at utilizing KGs for text-centric retrieval applications. This tutorial is the first to summarize and disseminate the progress in this emerging area to industry practitioners and researchers.",,"entity linking, entity retrieval, information retrieval, knowledge graphs",,
6,dinakar_modeling_2011,"Dinakar, Karthik; Reichart, Roi; Lieberman, Henry",Modeling the Detection of Textual Cyberbullying,,July,2011,https://www.aaai.org/ocs/index.php/ICWSM/ICWSM11/paper/view/3841,,,,,
6,dixon_measuring_2018,"Dixon, Lucas; Li, John; Sorensen, Jeffrey; Thain, Nithum; Vasserman, Lucy",Measuring and Mitigating Unintended Bias in Text Classification,,,2018,http://dl.acm.org/citation.cfm?doid=3278721.3278729,"We introduce and illustrate a new approach to measuring and mitigating unintended bias in machine learning models. Our deï¬Ånition of unintended bias is parameterized by a test set and a subset of input features. We illustrate how this can be used to evaluate text classiï¬Åers using a synthetic test set and a public corpus of comments annotated for toxicity from Wikipedia Talk pages. We also demonstrate how imbalances in training data can lead to unintended bias in the resulting models, and therefore potentially unfair applications. We use a set of common demographic identity terms as the subset of input features on which we measure bias. This technique permits analysis in the common scenario where demographic information on authors and readers is unavailable, so that bias mitigation must focus on the content of the text itself. The mitigation method we introduce is an unsupervised approach based on balancing the training dataset. We demonstrate that this approach reduces the unintended bias without compromising overall model quality.",,,,
6,el-assady_lingvis.io_2019,"El-Assady, Mennatallah; Jentner, Wolfgang; Sperrle, Fabian; Sevastjanova, Rita; Hautli-Janisz, Annette; Butt, Miriam; Keim, Daniel",lingvis.io - A Linguistic Visual Analytics Framework,,July,2019,https://www.aclweb.org/anthology/P19-3003,"We present a modular framework for the rapid-prototyping of linguistic, web-based, visual analytics applications. Our framework gives developers access to a rich set of machine learning and natural language processing steps, through encapsulating them into micro-services and combining them into a computational pipeline. This processing pipeline is auto-configured based on the requirements of the visualization front-end, making the linguistic processing and visualization design, detached independent development tasks. This paper describes the constellation and modality of our framework, which continues to support the efficient development of various human-in-the-loop, linguistic visual analytics research techniques and applications.",,,,
6,el-assady_lingvisio_2019,"El-Assady, Mennatallah; Jentner, Wolfgang; Sperrle, Fabian; Sevastjanova, Rita; Hautli-Janisz, Annette; Butt, Miriam; Keim, Daniel",lingvis.io - A Linguistic Visual Analytics Framework,,July,2019,https://www.aclweb.org/anthology/P19-3003,"We present a modular framework for the rapid-prototyping of linguistic, web-based, visual analytics applications. Our framework gives developers access to a rich set of machine learning and natural language processing steps, through encapsulating them into micro-services and combining them into a computational pipeline. This processing pipeline is auto-configured based on the requirements of the visualization front-end, making the linguistic processing and visualization design, detached independent development tasks. This paper describes the constellation and modality of our framework, which continues to support the efficient development of various human-in-the-loop, linguistic visual analytics research techniques and applications.",,,,
6,fadaee_learning_2017,"Fadaee, Marzieh; Bisazza, Arianna; Monz, Christof",Learning Topic-Sensitive Word Representations,,July,2017,https://www.aclweb.org/anthology/papers/P/P17/P17-2070/,,,,,
6,fan_pkuicst_2015,"Fan, Feifan; Fei, Yue; Lv, Chao; Yao, Lili; Yang, Jianwu; Zhao, Dongyan",PKUICST at TREC 2015 Microblog Track: Query-biased Adaptive Filtering in Real-time Microblog Stream,,,2015,,"This paper describes our approaches to real-time filtering task including push notifications on a mobile phone scenario and periodic email digest scenario in the TREC 2015 Microblog track. In the push notifications on a mobile phone scenario, we apply an adaptive timely query-biased filtering framework which utilizes two effective scores to estimate the relevance of tweets. External evidences are well incorporated in our approach with Web-based query expansion technique. In the periodic email digest scenario, we apply pseudo-relevance feedback using language model and similarly we adopt an adaptive dynamic query-biased filtering method to choose the novel representative tweets. Besides, the results of scenario periodic email digest can promote the performance of scenario push notifications since we utilize shared global relevance threshold. Experimental results show that our adaptive query-biased filtering methods achieve good performance with respect to ELG and nCG metrics for push notifications scenario. In addition, our systems for scenario periodic email digest also obtain convincing nDCG scores.",,,,
6,faruqui_problems_2016,"Faruqui, Manaal; Tsvetkov, Yulia; Rastogi, Pushpendre; Dyer, Chris",Problems With Evaluation of Word Embeddings Using Word Similarity Tasks,,August,2016,https://www.aclweb.org/anthology/W16-2506,,,,,
6,gabrilovich_computing_2007,"Gabrilovich, Evgeniy; Markovitch, Shaul",Computing Semantic Relatedness Using Wikipedia-based Explicit Semantic Analysis,,,2007,http://dl.acm.org/citation.cfm?id=1625275.1625535,"Computing semantic relatedness of natural language texts requires access to vast amounts of common-sense and domain-specific world knowledge. We propose Explicit Semantic Analysis (ESA), a novel method that represents the meaning of texts in a high-dimensional space of concepts derived from Wikipedia. We use machine learning techniques to explicitly represent the meaning of any text as a weighted vector of Wikipedia-based concepts. Assessing the relatedness of texts in this space amounts to comparing the corresponding vectors using conventional metrics (e.g., cosine). Compared with the previous state of the art, using ESA results in substantial improvements in correlation of computed relatedness scores with human judgments: from r = 0.56 to 0.75 for individual words and from r = 0.60 to 0.72 for texts. Importantly, due to the use of natural concepts, the ESA model is easy to explain to human users.",,,,
6,ghaddar_winer_2017,"Ghaddar, Abbas; Langlais, Phillippe",WiNER: A Wikipedia Annotated Corpus for Named Entity Recognition,,November,2017,https://www.aclweb.org/anthology/I17-1042,"We revisit the idea of mining Wikipedia in order to generate named-entity annotations. We propose a new methodology that we applied to English Wikipedia to build WiNER, a large, high quality, annotated corpus. We evaluate its usefulness on 6 NER tasks, comparing 4 popular state-of-the art approaches. We show that LSTM-CRF is the approach that benefits the most from our corpus. We report impressive gains with this model when using a small portion of WiNER on top of the CONLL training material. Last, we propose a simple but efficient method for exploiting the full range of WiNER, leading to further improvements.",,,,
6,ghosal_dialoguegcn_2019,"Ghosal, Deepanway; Majumder, Navonil; Poria, Soujanya; Chhaya, Niyati; Gelbukh, Alexander",DialogueGCN: A Graph Convolutional Neural Network for Emotion Recognition in Conversation,,,2019,https://www.aclweb.org/anthology/D19-1015,"Emotion recognition in conversation (ERC) has received much attention, lately, from researchers due to its potential widespread applications in diverse areas, such as health-care, education, and human resources. In this paper, we present Dialogue Graph Convolutional Network (DialogueGCN), a graph neural network based approach to ERC. We leverage self and inter-speaker dependency of the interlocutors to model conversational context for emotion recognition. Through the graph network, DialogueGCN addresses context propagation issues present in the current RNN-based methods. We empirically show that this method alleviates such issues, while outperforming the current state of the art on a number of benchmark emotion classiï¬Åcation datasets.",,,,
6,giaquinto_topic_2018,"Giaquinto, Robert; Banerjee, Arindam",Topic Modeling on Health Journals With Regularized Variational Inference,,April,2018,https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16932,"Topic modeling enables exploration and compact representation of a corpus. The CaringBridge (CB) dataset is a massive collection of journals written by patients and caregivers during a health crisis. Topic modeling on the CB dataset, however, is challenging due to the asynchronous nature of multiple authors writing about their health journeys. To overcome this challenge we introduce the Dynamic Author-Persona topic model (DAP), a probabilistic graphical model designed for temporal corpora with multiple authors. The novelty of the DAP model lies in its representation of authors by a persona---where personas capture the propensity to write about certain topics over time. Further, we present a regularized variational inference (RVI) algorithm, which we use to encourage the DAP model's personas to be distinct. Our results show significant improvements over competing topic models---particularly after regularization, and highlight the DAP model's unique ability to capture common journeys shared by different authors.",,,,
6,gil_towards_2019,"Gil, Yolanda; Honaker, James; Gupta, Shikhar; Ma, Yibo; D'Orazio, Vito; Garijo, Daniel; Gadewar, Shruti; Yang, Qifan; Jahanshad, Neda",Towards human-guided machine learning,,March,2019,https://doi.org/10.1145/3301275.3302324,"Automated Machine Learning (AutoML) systems are emerging that automatically search for possible solutions from a large space of possible kinds of models. Although fully automated machine learning is appropriate for many applications, users often have knowledge that supplements and constraints the available data and solutions. This paper proposes human-guided machine learning (HGML) as a hybrid approach where a user interacts with an AutoML system and tasks it to explore different problem settings that reflect the user's knowledge about the data available. We present: 1) a task analysis of HGML that shows the tasks that a user would want to carry out, 2) a characterization of two scientific publications, one in neuroscience and one in political science, in terms of how the authors would search for solutions using an AutoML system, 3) requirements for HGML based on those characterizations, and 4) an assessment of existing AutoML systems in terms of those requirements.",,"automated machine learning (AutoML), human-guided machine learning, scientific workflows, task analysis",,
6,gladkova_intrinsic_2016,"Gladkova, Anna; Drozd, Aleksandr",Intrinsic Evaluations of Word Embeddings: What Can We Do Better?,,August,2016,https://www.aclweb.org/anthology/W16-2507,,,,,
6,hazem_word_2018,"Hazem, Amir; Daille, BÃ©atrice",Word Embedding Approach for Synonym Extraction of Multi-Word Terms,,May,2018,https://www.aclweb.org/anthology/L18-1045,,,,,
6,hoffart_kore:_2012,"Hoffart, Johannes; Seufert, Stephan; Nguyen, Dat Ba; Theobald, Martin; Weikum, Gerhard",KORE: Keyphrase Overlap Relatedness for Entity Disambiguation,,,2012,http://doi.acm.org/10.1145/2396761.2396832,"Measuring the semantic relatedness between two entities is the basis for numerous tasks in IR, NLP, and Web-based knowledge extraction. This paper focuses on disambiguating names in a Web or text document by jointly mapping all names onto semantically related entities registered in a knowledge base. To this end, we have developed a novel notion of semantic relatedness between two entities represented as sets of weighted (multi-word) keyphrases, with consideration of partially overlapping phrases. This measure improves the quality of prior link-based models, and also eliminates the need for (usually Wikipedia-centric) explicit interlinkage between entities. Thus, our method is more versatile and can cope with long-tail and newly emerging entities that have few or no links associated with them. For efficiency, we have developed approximation techniques based on min-hash sketches and locality-sensitive hashing. Our experiments on semantic relatedness and on named entity disambiguation demonstrate the superiority of our method compared to state-of-the-art baselines.",,"entity disambiguation, entity relatedness, locality-sensitive hashing, semantic relatedness",,
6,hsiao_topic_2015,"Hsiao, I.-Han; Awasthi, Piyush",Topic Facet Modeling: Semantic Visual Analytics for Online Discussion Forums,,,2015,http://doi.acm.org/10.1145/2723576.2723613,"In this paper, we propose a novel Topic Facet Model (TFM), a probabilistic topic model that assumes all words in single sentence are generated from one topic facet. The model is applied to automatically extract forum posts semantics for uncovering the content latent structures. We further prototype a visual analytics interface to present online discussion forum semantics. We hypothesize that the semantic modeling through analytics on open online discussion forums can help users examine the post content by viewing the summarized topic facets. Our preliminary results demonstrated that TFM can be a promising method to extract topic specificity from conversational and relatively short texts in online programming discussion forums.",,"LDA, SLDA, TFM, automated assessment, discourse analytics, discussion forums, learning analytics, programming, topic modeling",,
6,huang_holes_2019,"Huang, Chien-yu; Casey, Arlene; G{\textbackslash}lowacka, Dorota; Medlar, Alan",Holes in the Outline: Subject-dependent Abstract Quality and Its Implications for Scientific Literature Search,,,2019,http://doi.acm.org/10.1145/3295750.3298953,"Scientific literature search engines typically index abstracts instead of the full-text of publications. The expectation is that the abstract provides a comprehensive summary of the article, enumerating key points for the reader to assess whether their information needs could be satisfied by reading the full-text. Furthermore, from a practical standpoint, obtaining the full-text is more complicated due to licensing issues, in the case of commercial publishers, and resource limitations of public repositories and pre-print servers. In this article, we use topic modelling to represent content in abstracts and full-text articles. Using Computer Science as a case study, we demonstrate that how well the abstract summarises the full-text is subfield-dependent. Indeed, we show that abstract representativeness has a direct impact on retrieval performance, with poorer abstracts leading to degraded performance. Finally, we present evidence that how well an abstract represents the full-text of an article is not random, but is a consequence of style and writing conventions in different subdisciplines and can be used to infer an evolutionary"" tree of subfields within Computer Science.""",,"scientific literature search, term taxonomy, topic models",,
6,jian_simple_2016,"Jian, Fanghong; Huang, Jimmy Xiangji; Zhao, Jiashu; He, Tingting; Hu, Po",A Simple Enhancement for Ad-hoc Information Retrieval via Topic Modelling,,,2016,http://dl.acm.org/citation.cfm?doid=2911451.2914748,"Traditional information retrieval (IR) models, in which a document is normally represented as a bag of words and their frequencies, capture the term-level and document-level information. Topic models, on the other hand, discover semantic topic-based information among words. In this paper, we consider term-based information and semantic information as two features of query terms and propose a simple enhancement for ad-hoc IR via topic modeling. In particular, three topic-based hybrid models, LDA-BM25, LDA-MATF and LDA-LM, are proposed. A series of experiments on eight standard datasets show that our proposed models can always outperform signiï¬Åcantly the corresponding strong baselines over all datasets in terms of MAP and most of datasets in terms of P@5 and P@20. A direct comparison on eight standard datasets also indicates our proposed models are at least comparable to the state-of-the-art approaches.",,,,
6,jin_probing_2019,"Jin, Qiao; Dhingra, Bhuwan; Cohen, William; Lu, Xinghua",Probing Biomedical Embeddings from Language Models,,June,2019,https://www.aclweb.org/anthology/W19-2011,"Contextualized word embeddings derived from pre-trained language models (LMs) show significant improvements on downstream NLP tasks. Pre-training on domain-specific corpora, such as biomedical articles, further improves their performance. In this paper, we conduct probing experiments to determine what additional information is carried intrinsically by the in-domain trained contextualized embeddings. For this we use the pre-trained LMs as fixed feature extractors and restrict the downstream task models to not have additional sequence modeling layers. We compare BERT (Devlin et al. 2018), ELMo (Peters et al., 2018), BioBERT (Lee et al., 2019) and BioELMo, a biomedical version of ELMo trained on 10M PubMed abstracts. Surprisingly, while fine-tuned BioBERT is better than BioELMo in biomedical NER and NLI tasks, as a fixed feature extractor BioELMo outperforms BioBERT in our probing tasks. We use visualization and nearest neighbor analysis to show that better encoding of entity-type and relational information leads to this superiority.",,,,
6,k_m_learning_2018,"Annervaz, K. M.; Basu Roy Chowdhury, Somnath; Dukkipati, Ambedkar",Learning beyond Datasets: Knowledge Graph Augmented Neural Networks for Natural Language Processing,,June,2018,https://www.aclweb.org/anthology/N18-1029,"Machine Learning has been the quintessential solution for many AI problems, but learning models are heavily dependent on specific training data. Some learning models can be incorporated with prior knowledge using a Bayesian setup, but these learning models do not have the ability to access any organized world knowledge on demand. In this work, we propose to enhance learning models with world knowledge in the form of Knowledge Graph (KG) fact triples for Natural Language Processing (NLP) tasks. Our aim is to develop a deep learning model that can extract relevant prior support facts from knowledge graphs depending on the task using attention mechanism. We introduce a convolution-based model for learning representations of knowledge graph entity and relation clusters in order to reduce the attention space. We show that the proposed method is highly scalable to the amount of prior information that has to be processed and can be applied to any generic NLP task. Using this method we show significant improvement in performance for text classification with 20Newsgroups (News20) \& DBPedia datasets, and natural language inference with Stanford Natural Language Inference (SNLI) dataset. We also demonstrate that a deep learning model can be trained with substantially less amount of labeled training data, when it has access to organized world knowledge in the form of a knowledge base.",,,,
6,kiela_specializing_2015,"Kiela, Douwe; Hill, Felix; Clark, Stephen",Specializing Word Embeddings for Similarity or Relatedness,,September,2015,https://www.aclweb.org/anthology/D15-1242,,,,,
6,klein_opennmt:_2017,"Klein, Guillaume; Kim, Yoon; Deng, Yuntian; Senellart, Jean; Rush, Alexander",OpenNMT: Open-Source Toolkit for Neural Machine Translation,,,2017,http://aclweb.org/anthology/P17-4012,"We describe an open-source toolkit for neural machine translation (NMT). The toolkit prioritizes efficiency, modularity, and extensibility with the goal of supporting NMT research into model architectures, feature representations, and source modalities, while maintaining competitive performance and reasonable training requirements. The toolkit consists of modeling and translation support, as well as detailed pedagogical documentation about the underlying techniques.",,,,
6,kryscinski_neural_2019,"Kryscinski, Wojciech; Keskar, Nitish Shirish; McCann, Bryan; Xiong, Caiming; Socher, Richard",Neural Text Summarization: A Critical Evaluation,,,2019,https://www.aclweb.org/anthology/D19-1051,"Text summarization aims at compressing long documents into a shorter form that conveys the most important parts of the original document. Despite increased interest in the community and notable research effort, progress on benchmark datasets has stagnated. We critically evaluate key ingredients of the current research setup: datasets, evaluation metrics, and models, and highlight three primary shortcomings: 1) automatically collected datasets leave the task underconstrained and may contain noise detrimental to training and evaluation, 2) current evaluation protocol is weakly correlated with human judgment and does not account for important characteristics such as factual correctness, 3) models overï¬Åt to layout biases of current datasets and offer limited diversity in their outputs.",,,,
6,krzeminski_helmholtz_2018,"KrzemiÅ„ski, Dominik; Balinsky, Helen; Balinsky, Alexander",Helmholtz Principle on Word Embeddings for Automatic Document Segmentation,,,2018,http://doi.acm.org/10.1145/3209280.3229103,,,"Helmholtz principle, automatic document segmentation, natural language processing, word embeddings",,
6,laden_rdocer_2020,"Laden, Daniel; Jayasundara, Shyaman; Kahanda, Indika",RDoCer: Information Retrieval and Sentence Extraction for Mental Health using Research Domain Criteria,,February,2020,,"Research Domain Criteria (RDoC) is a classification framework for mental illness, recently introduced by the National Institute of Mental Health. The RDoC Task at this years' BioNLP Open Shared Tasks 2019 workshop is a competition for inviting text mining groups around the world for developing informatics models for two subtasks: information retrieval and sentence extraction for mental health using RDoC. For competing in the RDoC task, we developed RDoCer, which uses a Term Frequency Inverse Document Frequency-based similarity measure for information retrieval and supervised machine learning models for sentence extraction. In comparison to the other models competed at the RDoC task, RDoCer performs competitively across the two subtasks while notably ranking first for the Sustained Threat RDoC construct in the sentence extraction subtask. The findings of this study have implications for mental health informaticians as well as researchers, curators, clinicians working in this domain.",,"BioNLP Open Shared Tasks 2019 workshop, Circadian rhythm, Data mining, Feature extraction, Information retrieval, Machine learning, National Institute of Mental Health, RDoC, RDoC Task, RDoC task, RDoCer, Task analysis, Training data, bionlp, classification framework, data mining, informatics models, information extraction, information retrieval, machine learning, mental health informaticians, mental health informatics, mental illness, pattern classification, psychology, sentence extraction subtask, supervised learning, supervised machine learning models, sustained threat RDoC, term frequency inverse document frequency-based similarity measure, text analysis, text mining groups",,
6,lan_albert_2019,"Lan, Zhenzhong; Chen, Mingda; Goodman, Sebastian; Gimpel, Kevin; Sharma, Piyush; Soricut, Radu",ALBERT: A Lite BERT for Self-supervised Learning of Language Representations,,September,2019,https://openreview.net/forum?id=H1eA7AEtvS,"Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due...",,,,
6,le_deep_2018,"Le, Minh; Postma, Marten; Urbani, Jacopo; Vossen, Piek",A Deep Dive into Word Sense Disambiguation with LSTM,,August,2018,https://www.aclweb.org/anthology/C18-1030,"LSTM-based language models have been shown effective in Word Sense Disambiguation (WSD). In particular, the technique proposed by Yuan et al. (2016) returned state-of-the-art performance in several benchmarks, but neither the training data nor the source code was released. This paper presents the results of a reproduction study and analysis of this technique using only openly available datasets (GigaWord, SemCor, OMSTI) and software (TensorFlow). Our study showed that similar results can be obtained with much less data than hinted at by Yuan et al. (2016). Detailed analyses shed light on the strengths and weaknesses of this method. First, adding more unannotated training data is useful, but is subject to diminishing returns. Second, the model can correctly identify both popular and unpopular meanings. Finally, the limited sense coverage in the annotated datasets is a major limitation. All code and trained models are made freely available.",,,,
6,lee_assorted_2016,"Lee, Kathy; Qadir, Ashequl; Datla, V.; Hasan, Sadid A.; Liu, J.; Prakash, Aaditya; Farri, Oladimeji",Assorted Textual Features and Dynamic Push Strategies for Real-time Tweet Notification,,,2016,,"In this paper, we describe our systems and corresponding results submitted to the RealTime Summarization (RTS) track at the 2016 Text Retrieval Conference (TREC). The task involved identifying relevant tweets based on a userâ€™s interest profile. In Scenario A of the task, tweets relevant to an interest profile were pushed to a live user in real-time. In Scenario B, a daily digest of relevant tweets was sent to a user. We submitted three automatic runs for each scenario. Our overall method for identifying relevant tweets was based on 1) automatically identifying key textual features from a set of interest profiles provided by the Track organizers, 2) expanding the textual phrases with their paraphrases, and 3) exploiting the features for message filtering and relevance measurement after novelty recognition. We experimented with different push strategies to decide when to deliver a message to a user. The evaluation results (by mobile and NIST assessors) show that our system ranked 3rd for Scenario A and 6th for Scenario B.",,,,
6,lee_empirical_2002,"Lee, Yoong Keok; Ng, Hwee Tou",An Empirical Evaluation of Knowledge Sources and Learning Algorithms for Word Sense Disambiguation,,,2002,https://doi.org/10.3115/1118693.1118699,"In this paper, we evaluate a variety of knowledge sources and supervised learning algorithms for word sense disambiguation on SENSEVAL-2 and SENSEVAL-1 data. Our knowledge sources include the part-of-speech of neighboring words, single words in the surrounding context, local collocations, and syntactic relations. The learning algorithms evaluated include Support Vector Machines (SVM), Naive Bayes, AdaBoost, and decision tree algorithms. We present empirical results showing the relative contribution of the component knowledge sources and the different learning algorithms. In particular, using all of these knowledge sources and SVM (i.e., a single learning algorithm) achieves accuracy higher than the best official scores on both SENSEVAL-2 and SENSEVAL-1 test data.",,,,
6,li_data_2017,"Li, Quanzhi; Shah, Sameena; Liu, Xiaomo; Nourbakhsh, Armineh",Data Sets: Word Embeddings Learned from Tweets and General Data,,May,2017,https://aaai.org/ocs/index.php/ICWSM/ICWSM17/paper/view/15672,"A word embedding is a low-dimensional, dense and real-valued vector representation of a word. Word embeddings have been used in many NLP tasks. They are usually generated from a large text corpus. The embedding of a word captures both its syntactic and semantic aspects. Tweets are short, noisy and have unique lexical and semantic features that are different from other types of text. Therefore, it is necessary to have word embeddings learned specifically from tweets. In this paper, we present ten word embedding data sets. In addition to the data sets learned from just tweet data, we also built embedding sets from the general data and the combination of tweets and the general data. The general data consist of news articles, Wikipedia data and other web data. These ten embedding models were learned from about 400 million tweets and 7 billion words from the general data. In this paper, we also present two experiments demonstrating how to use the data sets in some NLP tasks, such as tweet sentiment analysis and tweet topic classification tasks.",,,,
6,li_deep_2016,"Li, Hang; Lu, Zhengdong",Deep Learning for Information Retrieval,,,2016,http://dl.acm.org/citation.cfm?doid=2911451.2914800,"Recent years have observed a signiï¬Åcant progress in information retrieval and natural language processing with deep learning technologies being successfully applied into almost all of their major tasks. The key to the success of deep learning is its capability of accurately learning distributed representations (vector representations or structured arrangement of them) of natural language expressions such as sentences, and eï¬€ectively utilizing the representations in the tasks. This tutorial aims at summarizing and introducing the results of recent research on deep learning for information retrieval, in order to stimulate and foster more significant research and development work on the topic in the future.",,,,
6,li_joint_2016,"Li, Yuezhang; Zheng, Ronghuo; Tian, Tian; Hu, Zhiting; Iyer, Rahul; Sycara, Katia",Joint Embedding of Hierarchical Categories and Entities for Concept Categorization and Dataless Classification,,December,2016,https://www.aclweb.org/anthology/C16-1252,"Existing work learning distributed representations of knowledge base entities has largely failed to incorporate rich categorical structure, and is unable to induce category representations. We propose a new framework that embeds entities and categories jointly into a semantic space, by integrating structured knowledge and taxonomy hierarchy from large knowledge bases. Our framework enables to compute meaningful semantic relatedness between entities and categories in a principled way, and can handle both single-word and multiple-word concepts. Our method shows significant improvement on the tasks of concept categorization and dataless hierarchical classification.",,,,
6,li_subword-level_2018,"Li, Bofang; Drozd, Aleksandr; Liu, Tao; Du, Xiaoyong",Subword-level Composition Functions for Learning Word Embeddings,,June,2018,https://www.aclweb.org/anthology/W18-1205,"Subword-level information is crucial for capturing the meaning and morphology of words, especially for out-of-vocabulary entries. We propose CNN- and RNN-based subword-level composition functions for learning word embeddings, and systematically compare them with popular word-level and subword-level models (Skip-Gram and FastText). Additionally, we propose a hybrid training scheme in which a pure subword-level model is trained jointly with a conventional word-level embedding model based on lookup-tables. This increases the fitness of all types of subword-level word embeddings; the word-level embeddings can be discarded after training, leaving only compact subword-level representation with much smaller data volume. We evaluate these embeddings on a set of intrinsic and extrinsic tasks, showing that subword-level models have advantage on tasks related to morphology and datasets with high OOV rate, and can be combined with other types of embeddings.",,,,
6,li_user_2015,"Li, Hua; J. T. Powell, Daniel; Clark, Mark; O'Brien, Tifani; Alonso, Rafael",User Modeling of Skills and Expertise from Resumes,,,2015,https://doi.org/10.5220/0005622202290233,"Job applicants describe their skills and expertise in resumes and curriculum vitaes (CVs). These biographic data are often evaluated by human resource personnel or a search committee. This manual approach works well when the number of resumes is small. However, in this information age, the volume of available resumes can be overwhelming and there is a need for automatic evaluation of applicant skills and expertise. In this paper, we describe a user modeling algorithm to quantitatively identify skills and expertise from biographic data. This algorithm is called REMA (Resume Expertise Modeling Algorithm). REMA takes data from a resume document as input and produces an expertise model. The expertise model details the expertise topics for which the resume owner has claimed competency. Each topic carries a weight indicating the level of competency. There are two key insights for this algorithm. First, oneÃ¢Â€Â™s expertise is the cumulative result of the various Ã¢Â€Âœlearning eventsÃ¢Â€Âù in oneÃ¢Â€Â™s career. These learning events are mentioned in various sections of the resume, such as earning a degree, writing a paper, or getting a patent. Second, oneÃ¢Â€Â™s knowledge and skills can become outdated or forgotten over time if not reinforced by learning. We have developed a prototype resume evaluation system based on REMA and are in the process of evaluating REMAÃ¢Â€Â™s performance.",,"Expertise Modeling, Profile, Resume, Skill., User Modeling",,
6,liang_exploiting_2012,"Liang, Feng; Qiang, Runwei; Yang, Jianwu",Exploiting real-time information retrieval in the microblogosphere,,,2012,http://dl.acm.org/citation.cfm?doid=2232817.2232867,"Information seeking behavior in microblogging environments such as Twitter diï¬€ers from traditional web search. The best performing microblog retrieval techniques attempt to utilize both semantic and temporal aspects of documents. In this paper, we present an eï¬€ective approach, including the query modeling, the document modeling and the temporal re-ranking, to discover the most recent but relevant information to the query. For the query modeling, we introduce a two-stage pseudo-relevance feedback query expansion to overcome the severe vocabulary-mismatch problem of short message retrieval in microblog. For the document modeling, we propose two ways to expand document with the help of the shortened URL. For the temporal re-ranking, we suggest several methods to evaluate the temporal aspects of documents. Experimental results demonstrate that our approach obtains signiï¬Åcant improvements compared with baseline systems. Speciï¬Åcally, the proposed system gives 26.37\% and 9.94\% further increases in P@30 and MAP over the best performing result on highrel in the TRECâ€™11 RealTime Search Task.",,,,
6,lin_enhanced_2019,"Lin, Hsiao-Yun; Lo, Tien-Hong; Chen, Berlin",Enhanced Bert-Based Ranking Models for Spoken Document Retrieval,,December,2019,,"The Bidirectional Encoder Representations from Transformers (BERT) model has recently achieved record-breaking success on many natural language processing (NLP) tasks such as question answering and language understanding. However, relatively little work has been done on ad-hoc information retrieval (IR), especially for spoken document retrieval (SDR). This paper adopts and extends BERT for SDR, while its contributions are at least three-fold. First, we augment BERT with extra language features such as unigram and inverse document frequency (IDF) statistics to make it more applicable to SDR. Second, we also explore the incorporation of confidence scores into document representations to see if they could help alleviate the negative effects resulting from imperfect automatic speech recognition (ASR). Third, we conduct a comprehensive set of experiments to compare our BERT-based ranking methods with other state-of-the-art ones and investigate the synergy effect of them as well.",,"ASR, BERT, BERT-based ranking methods, Bit error rate, Context modeling, Data models, Natural language processing, SDR, Solid modeling, Spoken document retrieval, Task analysis, Training, ad-hoc information retrieval, bidirectional encoder representations, document handling, document representations, enhanced bert-based ranking models, extra language features, information retrieval, inverse document frequency, language understanding, model augmentation, natural language processing, natural language processing tasks, question answering, record-breaking success, speech recognition, spoken document retrieval, transformers model, unigram",,
6,lipczak_tulip:_2014,"Lipczak, Marek; Koushkestani, Arash; Milios, Evangelos",Tulip: Lightweight Entity Recognition and Disambiguation Using Wikipedia-based Topic Centroids,,,2014,http://doi.acm.org/10.1145/2633211.2634351,,,"entity recognition and disambiguation, erd 2014 - long track, linked open data, term centroids, text annotation, wikification",,
6,liu_visual_2018,"Liu, Shusen; Li, Tao; Li, Zhimin; Srikumar, Vivek; Pascucci, Valerio; Bremer, Peer-Timo",Visual Interrogation of Attention-Based Models for Natural Language Inference and Machine Comprehension,,,2018,http://aclweb.org/anthology/D18-2007,"Neural networks models have gained unprecedented popularity in natural language processing due to their state-of-the-art performance and the ï¬‚exible end-to-end training scheme. Despite their advantages, the lack of interpretability hinders the deployment and reï¬Ånement of the models. In this work, we present a ï¬‚exible visualization library for creating customized visual analytic environments, in which the user can investigate and interrogate the relationships among the input, the model internals (i.e., attention), and the output predictions, which in turn shed light on the model decision-making process.",,,,
6,m_information_2019,"Bhavadharani, M.; Ramkumar, M. P.; G S R, Emil Selvan",Information Retrieval in Search Engines Using Pseudo Relevance Feedback Mechanism,,March,2019,,"Online data generation is increasing rapidly due to technological development. In order to handle the huge volume of data, big data methodology is used widely. Though big data can able to process huge data, it has its own issues including information storage, data management, and retrieving data. In big data, data retrieval is an issue where the method of data retrieval and implementation environment differs. Though data mining has become evolving technology, it requires an optimal retrieval mechanism to extract data from the hugerepository.Therefore, an information retrieval method which in turn should effectively perform data retrieval in terms of both speed and accuracy need to be implemented. Topic relevance mechanism is used to retrieve an appropriate document from the repository, these documents are then ranked based on user preference using pseudo-relevance feedback mechanism. And hence the accuracy of the retrieved document is improved.",,"Big Data, Big Data methodology, Information Retrieval (IR), Information retrieval, Market research, Mathematical model, Probabilistic logic, Probabilistic model, Pseudo Relevance Feedback (PRF), Query expansion, Query processing, Query score, Ranking algorithms, Semantics, Term Frequency-Inverse Document Frequency (TF-IDF), data management, data mining, data retrieval, information retrieval, information retrieval method, information storage, online data generation, optimal retrieval mechanism, pseudorelevance feedback mechanism, relevance feedback, retrieved document, search engines",,
6,macavaney_cedr_2019,"MacAvaney, Sean; Yates, Andrew; Cohan, Arman; Goharian, Nazli",CEDR: Contextualized Embeddings for Document Ranking,,July,2019,https://doi.org/10.1145/3331184.3331317,"Although considerable attention has been given to neural ranking architectures recently, far less attention has been paid to the term representations that are used as input to these models. In this work, we investigate how two pretrained contextualized language models (ELMo and BERT) can be utilized for ad-hoc document ranking. Through experiments on TREC benchmarks, we find that several ex-sting neural ranking architectures can benefit from the additional context provided by contextualized language models. Furthermore, we propose a joint approach that incorporates BERT's classification vector into existing neural models and show that it outperforms state-of-the-art ad-hoc ranking baselines. We call this joint approach CEDR (Contextualized Embeddings for Document Ranking). We also address practical challenges in using these models for ranking, including the maximum input length imposed by BERT and runtime performance impacts of contextualized language models.",,"contextualized word embeddings, neural ranking",,
6,macavaney_cedr:_2019,"MacAvaney, Sean; Yates, Andrew; Cohan, Arman; Goharian, Nazli",CEDR: Contextualized Embeddings for Document Ranking,,July,2019,http://arxiv.org/abs/1904.07094,"Although considerable attention has been given to neural ranking architectures recently, far less attention has been paid to the term representations that are used as input to these models. In this work, we investigate how two pretrained contextualized language models (ELMo and BERT) can be utilized for ad-hoc document ranking. Through experiments on TREC benchmarks, we find that several existing neural ranking architectures can benefit from the additional context provided by contextualized language models. Furthermore, we propose a joint approach that incorporates BERT's classification vector into existing neural models and show that it outperforms state-of-the-art ad-hoc ranking baselines. We call this joint approach CEDR (Contextualized Embeddings for Document Ranking). We also address practical challenges in using these models for ranking, including the maximum input length imposed by BERT and runtime performance impacts of contextualized language models.",,"Computer Science - Computation and Language, Computer Science - Information Retrieval",,
6,macavaney_opennir_2020,"MacAvaney, Sean",OpenNIR: A Complete Neural Ad-Hoc Ranking Pipeline,,January,2020,https://dl.acm.org/doi/10.1145/3336191.3371864,,,,,
6,maheshwary_matching_2018,"Maheshwary, Saket; Misra, Hemant",Matching Resumes to Jobs via Deep Siamese Network,,,2018,https://doi.org/10.1145/3184558.3186942,"In this paper we investigate the important and challenging task of recommending appropriate jobs for job seeking candidates by matching semi structured resumes of candidates to job descriptions. To perform this task, we propose to use a siamese adaptation of convolutional neural network. The proposed approach effectively captures the underlying semantics thus enabling to project similar resumes and job descriptions closer to each other, and make dissimilar resumes and job descriptions distant from each other in the semantic space. Our experimental results on a set of 1314 resumes and a set of 3809 job descriptions (5,005,026 resume-job description pairs) demonstrate that our approach is better than the current state-of-the-art approaches.",,"deep learning, job recommendation, natural language processing, siamese network",,
6,makki_twitter_2016,"Makki, Raheleh; Soto, Axel J.; Brooks, Stephen; Milios, Evangelos E.",Twitter message recommendation based on user interest profiles,,August,2016,,"Twitter has become one of the most important platforms for gathering information, where users follow breaking news, track ongoing events and learn about their topics of interest. Considering the sheer volume of Twitter data and the ever-growing number of users, it is of great importance to have real-time systems that can monitor and recommend relevant and non-redundant tweets with respect to users' interests. In this paper, we propose a framework using language models as a basis for analyzing strategies and techniques for tweet recommendation based on user interest profiles. Results show that identifying named entities in profiles has a major impact on the accuracy of the recommender. We also performed a thorough comparison to investigate whether state-of-the-art semantic relatedness techniques have a positive impact on the precision of the recommended tweets. The TREC 2015 Microblog track dataset is used for comparison and evaluation throughout this paper.",,"Computer science, Mathematical model, Measurement, Recommender systems, Semantics, TREC 2015 Microblog track dataset, Twitter, Twitter message recommendation, breaking news, information gathering, information retrieval, interest topic learning, language models, learning (artificial intelligence), named entity identification, natural language processing, ongoing event tracking, real-time filtering, real-time systems, recommender systems, semantic relatedness techniques, social networking (online), social recommenders, tweet recommendation, user interest profiles",,
6,manali_sharma_active_2015,Manali Sharma; Di Zhuang; Mustafa Bilgic,Active Learning with Rationales for Text Classification,,June,2015,https://www.aclweb.org/anthology/N15-1047,"We present a simple and yet effective approach that can incorporate rationales elicited from annotators into the training of any off-the-shelf classifier. We show that our simple approach is effective for multinomial naÂ¨Ä±ve Bayes, logistic regression, and support vector machines. We additionally present an active learning method tailored specifically for the learning with rationales framework.",,,,
6,mcdonald_deep_2018,"McDonald, Ryan; Brokos, George; Androutsopoulos, Ion",Deep Relevance Ranking Using Enhanced Document-Query Interactions,,,2018,http://aclweb.org/anthology/D18-1211,"We explore several new models for document relevance ranking, building upon the Deep Relevance Matching Model (DRMM) of Guo et al. (2016). Unlike DRMM, which uses context-insensitive encodings of terms and query-document term interactions, we inject rich context-sensitive encodings throughout our models, inspired by PACRRâ€™s (Hui et al., 2017) convolutional n-gram matching features, but extended in several ways including multiple views of query and document inputs. We test our models on datasets from the BIOASQ question answering challenge (Tsatsaronis et al., 2015) and TREC ROBUST 2004 (Voorhees, 2005), showing they outperform BM25-based baselines, DRMM, and PACRR.",,,,
6,medlar_how_2018,"Medlar, Alan; Glowacka, Dorota",How Consistent is Relevance Feedback in Exploratory Search?,,,2018,http://doi.acm.org/10.1145/3269206.3269297,"Search activities involving knowledge acquisition, investigation and synthesis are collectively known as exploratory search. Exploratory search is challenging for users, who may be unable to formulate search queries, have ill-defined search goals or may even struggle to understand search results. To ameliorate these difficulties, reinforcement learning-based information retrieval systems were developed to provide adaptive support to users. Reinforcement learning is used to build a model of user intent based on relevance feedback provided by the user. But how reliable is relevance feedback in this context? To answer this question, we developed a novel permutation-based metric for scoring the consistency of relevance feedback. We used this metric to perform a retrospective analysis of interaction data from lookup and exploratory search experiments. Our analysis shows that for lookup search relevance judgments are highly consistent, supporting previous findings that relevance feedback improves retrieval performance. For exploratory search, however, the distribution of consistency scores shows considerable inconsistency.",,"exploratory search, lookup search, relevance feedback",,
6,melamud_combining_2019,"Melamud, Oren; Bornea, Mihaela; Barker, Ken",Combining Unsupervised Pre-training and Annotator Rationales to Improve Low-shot Text Classification,,November,2019,https://www.aclweb.org/anthology/D19-1401,"Supervised learning models often perform poorly at low-shot tasks, i.e. tasks for which little labeled data is available for training. One prominent approach for improving low-shot learning is to use unsupervised pre-trained neural models. Another approach is to obtain richer supervision by collecting annotator rationales (explanations supporting label annotations). In this work, we combine these two approaches to improve low-shot text classification with two novel methods: a simple bag-of-words embedding approach; and a more complex context-aware method, based on the BERT model. In experiments with two English text classification datasets, we demonstrate substantial performance gains from combining pre-training with rationales. Furthermore, our investigation of a range of train-set sizes reveals that the simple bag-of-words approach is the clear top performer when there are only a few dozen training instances or less, while more complex models, such as BERT or CNN, require more training data to shine.",,,,
6,mendoza_ad-hoc_2018,"Mendoza, Marcelo; Ormeno, Pablo; Valle, Carlos",Ad-hoc Information Retrieval based on Boosted Latent Dirichlet Allocated Topics,,November,2018,,"Latent Dirichlet Allocation (LDA) is a fundamental method in the text mining field. We propose strategies for topic and model selection based on LDA that exploits the semantic coherence of the topics inferred, boosting the quality of the models found. Then we study how our boosted topic models perform in ad-hoc information retrieval tasks. Experimental results in four datasets show that our proposal improves the quality of the topics found favoring document retrieval tasks. Our method outperforms traditional LDA-based methods showing that model selection based on semantic coherence is useful for document modeling and information retrieval tasks.",,"Boosted aggregation of topics, Coherence, Data models, Hidden Markov models, Information retrieval, LDA-based methods, Semantics, Smoothing methods, Task analysis, ad-hoc information retrieval, ad-hoc information retrieval tasks, boosted latent Dirichlet allocated topics, boosted topic models, data mining, document modeling, document retrieval tasks, information retrieval, latent Dirichlet allocation, model selection, semantic coherence, statistical analysis, text analysis, text mining field, topic selection",,
6,metzler_similarity_2007,"Metzler, Donald; Dumais, Susan; Meek, Christopher",Similarity Measures for Short Segments of Text,,,2007,,"Measuring the similarity between documents and queries has been extensively studied in information retrieval. However, there are a growing number of tasks that require computing the similarity between two very short segments of text. These tasks include query reformulation, sponsored search, and image retrieval. Standard text similarity measures perform poorly on such tasks because of data sparseness and the lack of context. In this work, we study this problem from an information retrieval perspective, focusing on text representations and similarity measures. We examine a range of similarity measures, including purely lexical measures, stemming, and language modeling-based measures. We formally evaluate and analyze the methods on a query-query similarity task using 363,822 queries from a web search log. Our analysis provides insights into the strengths and weaknesses of each method, including important tradeoffs between effectiveness and efficiency.",,"Excellent Match, Expanded Representation, Query Expansion, Short Segment, Similarity Measure",,
6,mihalcea_pagerank_2004,"Mihalcea, Rada; Tarau, Paul; Figa, Elizabeth","PageRank on Semantic Networks, with Application to Word Sense Disambiguation",,,2004,https://doi.org/10.3115/1220355.1220517,"This paper presents a new open text word sense disambiguation method that combines the use of logical inferences with PageRank-style algorithms applied on graphs extracted from natural language documents. We evaluate the accuracy of the proposed algorithm on several sense-annotated texts, and show that it consistently outperforms the accuracy of other previously proposed knowledge-based word sense disambiguation methods. We also explore and evaluate methods that combine several open-text word sense disambiguation algorithms.",,,,
6,mihalcea_using_2007,"Mihalcea, Rada",Using Wikipedia for Automatic Word Sense Disambiguation,,April,2007,https://digital.library.unt.edu/ark:/67531/metadc31000/,"This paper describes a method for generating sense-tagged data using Wikipedia as a source of sense annotations. Through word sense disambiguation experiments, the authors show that the Wikipedia-based sense annotations are reliable and can be used to construct accurate sense classifiers.",,,,
6,mihalcea_wikify!:_2007,"Mihalcea, Rada; Csomai, Andras",Wikify!: Linking Documents to Encyclopedic Knowledge,,,2007,http://doi.acm.org/10.1145/1321440.1321475,"This paper introduces the use of Wikipedia as a resource for automatic keyword extraction and word sense disambiguation, and shows how this online encyclopedia can be used to achieve state-of-the-art results on both these tasks. The paper also shows how the two methods can be combined into a system able to automatically enrich a text with links to encyclopedic knowledge. Given an input document, the system identifies the important concepts in the text and automatically links these concepts to the corresponding Wikipedia pages. Evaluations of the system show that the automatic annotations are reliable and hardly distinguishable from manual annotations.",,"keyword extraction, semantic annotation, wikipedia, word sense disambiguation",,
6,mikolov_distributed_nodate,Mikolov,Distributed Representations of Sentences and Documents,,,,,"Many  machine  learning  algorithms  require  theinput to be represented as a fixed-length featurevector.  When it comes to texts, one of the mostcommon  fixed-length  features  is  bag-of-words.Despite  their  popularity,  bag-of-words  featureshave two major weaknesses: they lose the order-ing of the words and they also ignore semanticsof the words. For example, â€œpowerful,â€ù â€œstrongâ€ùand â€œParisâ€ù are equally distant. In this paper, weproposeParagraph Vector, an unsupervised algo-rithm that learns fixed-length feature representa-tions from variable-length pieces of texts, such assentences, paragraphs, and documents. Our algo-rithm represents each document by a dense vec-tor which is trained to predict words in the doc-ument.  Its construction gives our algorithm thepotential to overcome the weaknesses of bag-of-words models. Empirical results show that Para-graph Vectors outperform bag-of-words modelsas  well  as  other  techniques  for  text  representa-tions. Finally, we achieve new state-of-the-art re-sults on several text classification and sentimentanalysis tasks.",,,,
6,milne_effective_2008,"Milne, David; Witten, Ian H.","An Effective, Low-Cost Measure of Semantic Relatedness Obtained From Wikipedia Links",,,2008,https://www.aaai.org/Papers/Workshops/2008/WS-08-15/WS08-15-005.pdf,"relatedness between terms using the links found within their corresponding Wikipedia articles. Unlike other techniques based on Wikipedia, WLM is able to provide accurate measures efficiently, using only the links between articles rather than their textual content. Before describing the details, we first outline the other systems to which it can be compared. This is followed by a description of the algorithm, and its evaluation against manually-defined ground truth. The paper concludes with a discussion of the strengths and weaknesses of the new approach. This paper describes a new technique for obtaining measures of semantic relatedness. Like other recent approaches, it uses Wikipedia to provide structured world knowledge about the terms of interest. Our approach is",,,,
6,nandhakumar_clinically_2017,"Nandhakumar, Nidhin; Sherkat, Ehsan; Milios, Evangelos E.; Gu, Hong; Butler, Michael",Clinically Significant Information Extraction from Radiology Reports,,,2017,http://dl.acm.org/citation.cfm?doid=3103010.3103023,"Radiology reports are one of the most important medical documents that a diagnostician looks into, especially in the emergency context. ey provide the emergency physicians with critical information regarding the condition of the patient and help the physicians take immediate action on urgent conditions. However, the reports are in the form of unstructured text, which makes them time consuming for humans to interpret. We have developed a machine learning system to (a) e ciently extract the clinically signi cant parts and their level of importance in radiology reports, and (b) to classi es the overall report into critical or non-critical categories which help doctors to identify potential high priority reports. As a starting point, the system uses anonymized chest X-RAY reports of adults and provides three levels of importance for medical phrases. We used the Conditional Random Field (CRF) model to identify clinically signi cant phrases with an average f1-score of 0.75. e proposed system includes a web-based interface which highlights the medical phrases, and their level of importance to the emergency physician. e overall classi cation of the report is performed using the phrases extracted from the CRF model as features for the classi er. Average accuracy achieved is 85\%.",,"Active Learning, Biomedical Text, Radiology Report",,
6,nayak_evaluating_2016,"Nayak, Neha; Angeli, Gabor; Manning, Christopher D.",Evaluating Word Embeddings Using a Representative Suite of Practical Tasks,,August,2016,https://www.aclweb.org/anthology/W16-2504,,,,,
6,nejadgholi_recognizing_2019,"Nejadgholi, Isar; Fraser, Kathleen C.; De Bruijn, Berry; Li, Muqun; LaPlante, Astha; Zine El Abidine, Khaldoun",Recognizing UMLS Semantic Types with Deep Learning,,November,2019,https://www.aclweb.org/anthology/D19-6219,"Entity recognition is a critical first step to a number of clinical NLP applications, such as entity linking and relation extraction. We present the first attempt to apply state-of-the-art entity recognition approaches on a newly released dataset, MedMentions. This dataset contains over 4000 biomedical abstracts, annotated for UMLS semantic types. In comparison to existing datasets, MedMentions contains a far greater number of entity types, and thus represents a more challenging but realistic scenario in a real-world setting. We explore a number of relevant dimensions, including the use of contextual versus non-contextual word embeddings, general versus domain-specific unsupervised pre-training, and different deep learning architectures. We contrast our results against the well-known i2b2 2010 entity recognition dataset, and propose a new method to combine general and domain-specific information. While producing a state-of-the-art result for the i2b2 2010 task (F1 = 0.90), our results on MedMentions are significantly lower (F1 = 0.63), suggesting there is still plenty of opportunity for improvement on this new data.",,,,
6,ng_better_2015,"Ng, Jun-Ping; Abrecht, Viktoria",Better Summarization Evaluation with Word Embeddings for ROUGE,,,2015,http://aclweb.org/anthology/D15-1222,"ROUGE is a widely adopted, automatic evaluation measure for text summarization. While it has been shown to correlate well with human judgements, it is biased towards surface lexical similarities. This makes it unsuitable for the evaluation of abstractive summarization, or summaries with substantial paraphrasing. We study the effectiveness of word embeddings to overcome this disadvantage of ROUGE. Speciï¬Åcally, instead of measuring lexical overlaps, word embeddings are used to compute the semantic similarity of the words used in summaries instead. Our experimental results show that our proposal is able to achieve better correlations with human judgements when measured with the Spearman and Kendall rank coefï¬Åcients.",,,,
6,nguyen_event_2015,"Nguyen, Thien Huu; Grishman, Ralph",Event Detection and Domain Adaptation with Convolutional Neural Networks,,,2015,http://aclweb.org/anthology/P15-2060,We study the event detection problem using convolutional neural networks (CNNs) that overcome the two fundamental limitations of the traditional feature-based approaches to this task: complicated feature engineering for rich feature sets and error propagation from the preceding stages which generate these features. The experimental results show that the CNNs outperform the best reported feature-based systems in the general setting as well as the domain adaptation setting without resorting to extensive external resources.,,,,
6,nieto_pina_training_2017,"Nieto PiÃ±a, Luis; Johansson, Richard",Training Word Sense Embeddings With Lexicon-based Regularization,,November,2017,https://www.aclweb.org/anthology/I17-1029,"We propose to improve word sense embeddings by enriching an automaticcorpus-based method with lexicographic data. Information from a lexicon isintroduced into the learning algorithm's objective function through aregularizer. The incorporation of lexicographic data yields embeddings that areable to reflect expert-defined word senses, while retaining the robustness,high quality, and coverage of automatic corpus-based methods. These propertiesare observed in a manual inspection of the semantic clusters that differentdegrees of regularizer strength create in the vector space. Moreover, weevaluate the sense embeddings in two downstream applications: word sensedisambiguation and semantic frame prediction, where they outperform simplerapproaches. Our results show that a corpus-based model balanced withlexicographic data learns better representations and improve their performancein downstream tasks.",,,,
6,niu_word_2005,"Niu, Zheng-Yu; Ji, Dong-Hong; Tan, Chew Lim",Word Sense Disambiguation Using Label Propagation Based Semi-supervised Learning,,,2005,https://doi.org/10.3115/1219840.1219889,"Shortage of manually sense-tagged data is an obstacle to supervised word sense disambiguation methods. In this paper we investigate a label propagation based semi-supervised learning algorithm for WSD, which combines labeled and unlabeled data in learning process to fully realize a global consistency assumption: similar examples should have similar labels. Our experimental results on benchmark corpora indicate that it consistently outperforms SVM when only very few labeled examples are available, and its performance is also better than monolingual bootstrapping, and comparable to bilingual bootstrapping.",,,,
6,noauthor_language_2017,,Language Modeling by Clustering with Word Embeddings for Text Readability Assessment,,,2017,http://www.eecs.harvard.edu/~htk/publication/2017-cikm-cha-gwon-kung.pdf,"We present a clustering-based language model using word em- beddings for text readability prediction. Presumably, an Euclidean  semantic space hypothesis holds true for word embeddings whose training is done by observing word co-occurrences. We argue that clustering with word embeddings in the metric space should yield feature representations in a higher semantic space appropriate  for text regression. Also, by representing features in terms of his- tograms, our approach can naturally address documents of varying  lengths. An empirical evaluation using the Common Core Standards corpus reveals that the features formed on our clustering-based language model signi!cantly improve the previously known results for the same corpus in readability prediction. We also evaluate the task of sentence matching based on semantic relatedness using the Wiki-SimpleWiki corpus and !nd that our features lead to superior matching performance.",,,,
6,ostrowski_using_2015,"Ostrowski, D. A.",Using latent dirichlet allocation for topic modelling in twitter,,February,2015,,,,"Analytical models, Bayes methods, Market research, Semantics, Twitter messages, customer relationship management, generative probabilistic model, large-scale corpora, latent Dirichlet allocation, natural language processing, social media, social networking (online), social networks, subtopics identification, topic modelling",,
6,pang_text_2016,"Pang, Liang; Lan, Yanyan; Guo, Jiafeng; Xu, Jun; Wan, Shengxian; Cheng, Xueqi",Text Matching As Image Recognition,,,2016,http://dl.acm.org/citation.cfm?id=3016100.3016292,"Matching two texts is a fundamental problem in many natural language processing tasks. An effective way is to extract meaningful matching patterns from words, phrases, and sentences to produce the matching score. Inspired by the success of convolutional neural network in image recognition, where neurons can capture many complicated patterns based on the extracted elementary visual patterns such as oriented edges and corners, we propose to model text matching as the problem of image recognition. Firstly, a matching matrix whose entries represent the similarities between words is constructed and viewed as an image. Then a convolutional neural network is utilized to capture rich matching patterns in a layer-by-layer way. We show that by resembling the compositional hierarchies of patterns in image recognition, our model can successfully identify salient signals such as n-gram and n-term matchings. Experimental results demonstrate its superiority against the baselines.",,,,
6,papineni_bleu_2001,"Papineni, Kishore; Roukos, Salim; Ward, Todd; Zhu, Wei-Jing",BLEU: a method for automatic evaluation of machine translation,,,2001,http://portal.acm.org/citation.cfm?doid=1073083.1073135,,,,,
6,pappu_lightweight_2017,"Pappu, Aasish; Blanco, Roi; Mehdad, Yashar; Stent, Amanda; Thadani, Kapil",Lightweight Multilingual Entity Extraction and Linking,,,2017,http://doi.acm.org/10.1145/3018661.3018724,"Text analytics systems often rely heavily on detecting and linking entity mentions in documents to knowledge bases for downstream applications such as sentiment analysis, question answering and recommender systems. A major challenge for this task is to be able to accurately detect entities in new languages with limited labeled resources. In this paper we present an accurate and lightweight, multilingual named entity recognition (NER) and linking (NEL) system. The contributions of this paper are three-fold: 1) Lightweight named entity recognition with competitive accuracy; 2) Candidate entity retrieval that uses search click-log data and entity embeddings to achieve high precision with a low memory footprint; and 3) efficient entity disambiguation. Our system achieves state-of-the-art performance on TAC KBP 2013 multilingual data and on English AIDA CONLL data.",,"clustering entities, document processing, entity extraction, entity linking, natural language processing, unsupervised learning",,
6,paranyushkin_infranodus_2019,"Paranyushkin, Dmitry",InfraNodus: Generating Insight Using Text Network Analysis,,,2019,http://dl.acm.org/citation.cfm?doid=3308558.3314123,"In this paper we present a web-based open source tool and a method for generating insight from any text or discourse using text network analysis. The tool (InfraNodus) can be used by researchers and writers to organize and to better understand their notes, to measure the level of bias in discourse, and to identify the parts of the discourse where there is a potential for insight and new ideas. The method is based on text network analysis algorithm, which represents any text as a network and identifies the most influential words in a discourse based on the terms' co-occurrence. Graph community detection algorithm is then applied in order to identify the different topical clusters, which represent the main topics in the text as well as the relations between them. The community structure is used in conjunction with other measures to identify the level of bias or cognitive diversity of the discourse. Finally, the structural gaps in the graph can indicate the parts of the discourse where the connections are lacking, therefore highlighting the areas where thereâ€™s a potential for new ideas. The tool can be used as standalone software by end users as well as implemented via an API into other tools. Another interesting application is in the field of recommendation systems: structural gaps could indicate potentially interesting non-trivial connections to any connected datasets.",,,,
6,park_word_nodate,"Park, Hosung",Word Clustering Using Word Embedding Generated by Neural Net-based Skip Gram,,,,https://pdfs.semanticscholar.org/36f2/1f3602a6825341a4008b1052682d306a9432.pdf?_ga=2.249745350.1169744153.1558973111-1321972634.1555903103,Abstractâ€”This paper proposes word clustering using word embedding. We used a neural net-based continuous skip-gram method for generating word embedding in continuous space. The proposed word clustering method represents each word in the vector space using a neural network. The K-means clustering method partitions word embedding into predetermined K-word clusters.,,,,
6,parveen_topical_2015,"Parveen, Daraksha; Ramsl, Hans-Martin; Strube, Michael",Topical Coherence for Graph-based Extractive Summarization,,,2015,http://aclweb.org/anthology/D15-1226,"We present an approach for extractive single-document summarization. Our approach is based on a weighted graphical representation of documents obtained by topic modeling. We optimize importance, coherence and non-redundancy simultaneously using ILP. We compare ROUGE scores of our system with state-of-the-art results on scientiï¬Åc articles from PLOS Medicine and on DUC 2002 data. Human judges evaluate the coherence of summaries generated by our system in comparision to two baselines. Our approach obtains competitive performance.",,,,
6,pauls_faster_2011,"Pauls, Adam; Klein, Dan",Faster and Smaller N-gram Language Models,,,2011,http://dl.acm.org/citation.cfm?id=2002472.2002506,"N-gram language models are a major resource bottleneck in machine translation. In this paper, we present several language model implementations that are both highly compact and fast to query. Our fastest implementation is as fast as the widely used SRILM while requiring only 25\% of the storage. Our most compact representation can store all 4 billion n-grams and associated counts for the Google n-gram corpus in 23 bits per n-gram, the most compact lossless representation to date, and even more compact than recent lossy compression techniques. We also discuss techniques for improving query speed during decoding, including a simple but novel language model caching technique that improves the query speed of our language models (and SRILM) by up to 300\%.",,,,
6,pelevina_making_2016,"Pelevina, Maria; Arefiev, Nikolay; Biemann, Chris; Panchenko, Alexander",Making Sense of Word Embeddings,,August,2016,https://www.aclweb.org/anthology/W16-1620,,,,,
6,peters_deep_2018,"Peters, Matthew; Neumann, Mark; Iyyer, Mohit; Gardner, Matt; Clark, Christopher; Lee, Kenton; Zettlemoyer, Luke",Deep Contextualized Word Representations,,June,2018,https://www.aclweb.org/anthology/N18-1202,"We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",,,,
6,raheleh_makki_axel_soto_stephen_brooks_evangelos_milios_semantic-based_2017,"Raheleh Makki, Axel Soto, Stephen Brooks, Evangelos Milios",Semantic-Based Active Query Expansion for Personalized Microblog Recommendation,,,2017,,"Since Twitter has become one of the most important information sources, tweet  ltering and recommendation systems have gained in importance. These systems help users  find highly relevant and non-redundant information about their topics of interest. However, traditional methods used for retrieving web documents may not perform equally well when applied to tweets due to the inherent challenges associated to the short and noisy text content. We pro- pose a novel method for the re nement of the interest pro le by means of user involvement. The results show the effectiveness of the proposed selection strategies in improving the performance of the recommender system, while also outperforming one of the best supervised learning-to-rank methods when applied to TREC 2015 Microblog track dataset.",,,,
6,ratinov_local_2011,"Ratinov, Lev; Roth, Dan; Downey, Doug; Anderson, Mike",Local and Global Algorithms for Disambiguation to Wikipedia,,,2011,http://dl.acm.org/citation.cfm?id=2002472.2002642,"Disambiguating concepts and entities in a context sensitive way is a fundamental problem in natural language processing. The comprehensiveness of Wikipedia has made the online encyclopedia an increasingly popular target for disambiguation. Disambiguation to Wikipedia is similar to a traditional Word Sense Disambiguation task, but distinct in that the Wikipedia link structure provides additional information about which disambiguations are compatible. In this work we analyze approaches that utilize this information to arrive at coherent sets of disambiguations for a given document (which we call global"" approaches)", and compare them to more traditional (local) approaches. We show that previous approaches for global disambiguation can be improved," but even then the local disambiguation provides a baseline which is very hard to beat.""",,
6,reimers_sentence-bert:_2019,"Reimers, Nils; Gurevych, Iryna",Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks,,November,2019,http://arxiv.org/abs/1908.10084,"BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations ({\textasciitilde}65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.",,Computer Science - Computation and Language,,
6,ribeiro_beyond_2020,"Ribeiro, Marco Tulio; Wu, Tongshuang; Guestrin, Carlos; Singh, Sameer",Beyond Accuracy: Behavioral Testing of NLP Models with CheckList,,July,2020,https://www.aclweb.org/anthology/2020.acl-main.442,"Although measuring held-out accuracy has been the primary approach to evaluate generalization, it often overestimates the performance of NLP models, while alternative approaches for evaluating models either focus on individual tasks or on specific behaviors. Inspired by principles of behavioral testing in software engineering, we introduce CheckList, a task-agnostic methodology for testing NLP models. CheckList includes a matrix of general linguistic capabilities and test types that facilitate comprehensive test ideation, as well as a software tool to generate a large and diverse number of test cases quickly. We illustrate the utility of CheckList with tests for three tasks, identifying critical failures in both commercial and state-of-art models. In a user study, a team responsible for a commercial sentiment analysis model found new and actionable bugs in an extensively tested model. In another user study, NLP practitioners with CheckList created twice as many tests, and found almost three times as many bugs as users without it.",,,,
6,rimell_sledded:_2016,"Rimell, Laura; Vecchi, Eva Maria",SLEDDED: A Proposed Dataset of Event Descriptions for Evaluating Phrase Representations,,August,2016,https://www.aclweb.org/anthology/W16-2525,,,,,
6,rothe_autoextend:_2015,"Rothe, Sascha; SchÃ_tze, Hinrich",AutoExtend: Extending Word Embeddings to Embeddings for Synsets and Lexemes,,July,2015,https://www.aclweb.org/anthology/P15-1173,,,,,
6,rush_annotated_2018,"Rush, Alexander",The Annotated Transformer: code explained as paper pdf,,July,2018,https://www.aclweb.org/anthology/W18-2509,"(Note this is not being submitted blind. The chair of the workshop requested this submission unblinded from me on twitter, so assuming that is okay.) A major goal of open-source NLP is to quickly and accurately reproduce the results of new work, in a manner that the community can easily use and modify. While most papers publish enough detail for replication, it still may be difficult to achieve good results in practice. This paper presents a worked exercise of paper reproduction with the goal of implementing the results of the recent Transformer model. The replication exercise aims at simple code structure that follows closely with the original work, while achieving an efficient usable system.",,,,
6,sajadi_domain-specific_2015,"Sajadi, Armin; Milios, Evangelos E.; KeÅ¡elj, Vlado; Janssen, Jeannette C. M.",Domain-Specific Semantic Relatedness from Wikipedia Structure: A Case Study in Biomedical Text,,,2015,,"Wikipedia is becoming an important knowledge source in various domain specific applications based on concept representation. This introduces the need for concrete evaluation of Wikipedia as a foundation for computing semantic relatedness between concepts. While lexical resources like WordNet cover generic English well, they are weak in their coverage of domain specific terms and named entities, which is one of the strengths of Wikipedia. Furthermore, semantic relatedness methods that rely on the hierarchical structure of a lexical resource are not directly applicable to the Wikipedia link structure, which is not hierarchical and whose links do not capture well defined semantic relationships like hyponymy.In this paper we (1) Evaluate Wikipedia in a domain specific semantic relatedness task and demonstrate that Wikipedia based methods can be competitive with state of the art ontology based methods and distributional methods in the biomedical domain (2) Adapt and evaluate the effectiveness of bibliometric methods of various degrees of sophistication on Wikipedia (3) Propose a new graph-based method for calculating semantic relatedness that outperforms existing methods by considering some specific features of Wikipedia structure.",,"Computational Linguistics, Distributional Method, Neighborhood Graph, Semantic Relatedness, Semantic Similarity",,
6,sajadi_vector_2017,"Sajadi, Armin; Milios, Evangelos E.; Keselj, Vlado",Vector Space Representation of Concepts Using Wikipedia Graph Structure,,,2017,,"We introduce a vector space representation of concepts using Wikipedia graph structure to calculate semantic relatedness. The proposed method starts from the neighborhood graph of a concept as the primary form and transfers this graph into a vector space to obtain the final representation. The proposed method achieves state of the art results on various relatedness datasets.Combining the vector space representation with standard coherence model, we show that the proposed relatedness method performs successfully in Word Sense Disambiguation (WSD). We then suggest a different formulation for coherence to demonstrate that, in a short enough sentence, there is one key entity that can help disambiguate every other entity. Using this finding, we provide a vector space based method that can outperform the standard coherence model in a significantly shorter computation time.",,"Entity representation, Graph methods, Semantic relatedness, Word Sense Disambiguation",,
6,saphra_evaluating_2016,"Saphra, Naomi",Evaluating Informal-Domain Word Representations With UrbanDictionary,,August,2016,https://www.aclweb.org/anthology/W16-2517,,,,,
6,scarlini_sensembert_2020,"Scarlini, Bianca; Passini, Tommaso; Navigli, Roberto",SENSEMBERT: Context-Enhanced Sense Embeddings for Multilingual Word Sense Disambiguation,,,2020,http://www.sensembert.org/resources/scarlini_etal_aaai2020.pdf,,,,,
6,sener_active_2018,"Sener, Ozan; Savarese, Silvio",Active Learning for Convolutional Neural Networks: A Core-Set Approach,,May,2018,http://arxiv.org/abs/1708.00489,"Convolutional neural networks (CNNs) have been successfully applied to many recognition and learning tasks using a universal recipe; training a deep model on a very large dataset of supervised examples. However, this approach is rather restrictive in practice since collecting a large set of labeled images is very expensive. One way to ease this problem is coming up with smart ways for choosing images to be labelled from a very large collection (ie. active learning). Our empirical study suggests that many of the active learning heuristics in the literature are not effective when applied to CNNs in batch setting. Inspired by these limitations, we define the problem of active learning as core-set selection, ie. choosing set of points such that a model learned over the selected subset is competitive for the remaining data points. We further present a theoretical result characterizing the performance of any selected subset using the geometry of the datapoints. As an active learning algorithm, we choose the subset which is expected to yield best result according to our characterization. Our experiments show that the proposed method significantly outperforms existing approaches in image classification experiments by a large margin.",,"Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning",,
6,sevastjanova_mixed-initiative_2018,"Sevastjanova, Rita; El-Assady, Mennatallah; Hautli-Janisz, Annette; Kehlbeck, Rebecca; Deussen, Oliver; Keim, Daniel; Kalouli, Aikaterini-Lida; Butt, Miriam",Mixed-Initiative Active Learning for Generating Linguistic Insights in Question Classiï¬Åcation,,October,2018,https://www.interactive-analysis.org/year/2018/,"We propose a mixed-initiative active learning system to tackle the challenge of building descriptive models for under-studied linguistic phenomena. Our particular use case is the linguistic analysis of question types, in particular in understanding what characterizes information-seeking vs. non-information-seeking questions (i.e., whether the speaker wants to elicit an answer from the hearer or not) and how automated methods can assist with the linguistic analysis. Our approach is motivated by the need for an effective and efï¬Åcient human-in-the-loop process in natural language processing that relies on example-based learning and provides immediate feedback to the user. In addition to the concrete implementation of a question classiï¬Åcation system, we describe general paradigms of explainable mixed-initiative learning, allowing for the user to access the patterns identiï¬Åed automatically by the system, rather than being confronted by a machine learning black box. Our user study demonstrates the capability of our system in providing deep linguistic insight into this particular analysis problem. The results of our evaluation are competitive with the current state-of-the-art.",,,,
6,shen_learning_2011,"Shen, Chao; Li, Tao",Learning to Rank for Query-Focused Multi-document Summarization,,December,2011,,"In this paper, we explore how to use ranking SVM to train the feature weights for query-focused multi-document summarization. To apply a supervised learning method to sentence extraction in multi-document summarization, we need to derive the sentence labels for training corpus from the existing human labeling data in form of. However, this process is not trivial, because the human summaries are abstractive, and do not necessarily well match the sentences in the documents. In this paper, we try to address the above problem from the following two aspects. First, we make use of sentence-to-sentence relationships to better estimate the probability of a sentence in the document set to be a summary sentence. Second, to make the derived training data less sensitive, we adopt a cost sensitive loss in the ranking SVM's objective function. The experimental results demonstrate the effectiveness of our proposed method.",,"Feature extraction, Humans, Redundancy, SVM ranking, Supervised learning, Support vector machines, Training, Training data, document handling, feature weights, learning (artificial intelligence), learning to rank, probability, query processing, query-based multi-document summarization, query-focused multidocument summarization, rank learning, sentence probability estimation, sentence-to-sentence relationships, summarization, support vector machine, support vector machines",,
6,sherkat_vector_2017,"Sherkat, Ehsan; Milios, Evangelos E.",Vector Embedding of Wikipedia Concepts and Entities,,,2017,,"Using deep learning for different machine learning tasks such as word embedding has recently gained a lot of researchersâ€™ attention. Word embedding is the task of mapping words or phrases to a low dimensional numerical vector. In this paper, we use deep learning to embed Wikipedia concepts and entities. The English version of Wikipedia contains more than five million pages, which suggest its capability to cover many English entities, phrases, and concepts. Each Wikipedia page is considered as a concept. Some concepts correspond to entities, such as a personâ€™s name, an organization or a place. Contrary to word embedding, Wikipedia concepts embedding is not ambiguous, so there are different vectors for concepts with similar surface form but different mentions. We proposed several approaches and evaluated their performance based on Concept Analogy and Concept Similarity tasks. The results show that proposed approaches have the performance comparable and in some cases even higher than the state-of-the-art methods.",,"Concept embedding, Vector representation, Wikipedia",,
6,shi_short-text_2018,"Shi, Tian; Kang, Kyeongpil; Choo, Jaegul; Reddy, Chandan K.",Short-Text Topic Modeling via Non-negative Matrix Factorization Enriched with Local Word-Context Correlations,,April,2018,https://doi.org/10.1145/3178876.3186009,"Being a prevalent form of social communications on the Internet, billions of short texts are generated everyday. Discovering knowledge from them has gained a lot of interest from both industry and academia. The short texts have a limited contextual information, and they are sparse, noisy and ambiguous, and hence, automatically learning topics from them remains an important challenge. To tackle this problem, in this paper, we propose a semantics-assisted non-negative matrix factorization (SeaNMF) model to discover topics for the short texts. It effectively incorporates the word-context semantic correlations into the model, where the semantic relationships between the words and their contexts are learned from the skip-gram view of the corpus. The SeaNMF model is solved using a block coordinate descent algorithm. We also develop a sparse variant of the SeaNMF model which can achieve a better model interpretability. Extensive quantitative evaluations on various real-world short text datasets demonstrate the superior performance of the proposed models over several other state-of-the-art methods in terms of topic coherence and classification accuracy. The qualitative semantic analysis demonstrates the interpretability of our models by discovering meaningful and consistent topics. With a simple formulation and the superior performance, SeaNMF can be an effective standard topic model for short texts.",,"non-negative matrix factorization, short texts, topic modeling, word embedding",,
6,shin_learning_2016,"Shin, Hoo-Chang; Roberts, Kirk; Lu, Le; Demner-Fushman, Dina; Yao, Jianhua; Summers, Ronald M.",Learning to Read Chest X-Rays: Recurrent Neural Cascade Model for Automated Image Annotation,,June,2016,http://ieeexplore.ieee.org/document/7780643/,"Despite the recent advances in automatically describing image contents, their applications have been mostly limited to image caption datasets containing natural images (e.g., Flickr 30k, MSCOCO). In this paper, we present a deep learning model to efï¬Åciently detect a disease from an image and annotate its contexts (e.g., location, severity and the affected organs). We employ a publicly available radiology dataset of chest x-rays and their reports, and use its image annotations to mine disease names to train convolutional neural networks (CNNs). In doing so, we adopt various regularization techniques to circumvent the large normalvs-diseased cases bias. Recurrent neural networks (RNNs) are then trained to describe the contexts of a detected disease, based on the deep CNN features. Moreover, we introduce a novel approach to use the weights of the already trained pair of CNN/RNN on the domain-speciï¬Åc image/text dataset, to infer the joint image/text contexts for composite image labeling. Signiï¬Åcantly improved image annotation results are demonstrated using the recurrent neural cascade model by taking the joint image/text contexts into account.",,,,
6,silva_analyzing_2019,"Silva, Victor N.; Herman, Ashley; Mirzaei, Maryam; Du, Elisa; Hu, Bowen; Lefsrud, Lianne M.; Sander, Joerg; Stroulia, Eleni; Sawchyn, Monica",Analyzing and visualizing the canadian research landscape,,November,2019,,"Research evaluation is an important activity in the overall context of scholarly work, for researchers' career advancement, publication and proposal adjudication, universities' strategic investments, and funding agencies' planning. In this paper, we describe a system that uses state-of-the-art text-analysis methods to analyze and visualize the grant dataset, recently made available by NSERC to gain insights around the science-and-technology research in Canada, which we believe can inform the above processes.",,"document clustering, research evaluation, text analysis, text-evolution analysis, visualization",,
6,stevens_representing_2019,"Stevens, Jon; Chen, Derek; Zimmer, Jacob; Punturo, Brandon; Kim, Mike",Representing document-level semantics of biomedical literature using pre-trained embedding models,,,2019,,"We present two novel tasks aimed at capturing document-level semantics, i.e., highlevel topical or thematic content, of biomedical scientific publications. We use these tasks to evaluate whether word and sequence embedding models pre-trained on biomedical literature can be used to derive meaningful document-level semantic representations for these publications. We evaluate approaches from two broad categories: (1) lexical pooling, or vectorizing documents purely based on aggregation of lexical items, which includes the NCBIâ€™s BioWordVec model and Tf-idf-based vectorizations, both with and without word pre-filtering based on biomedical ontologies, (2) sequence embedding, which includes the NCBIâ€™s BioSentVec model and BioBERT. For both of our tasks, lexical pooling outperformed sequence embedding, and the best overall method was mean pooling of BioWordVec word embeddings. We also include baselines trained on nonbiomedical English to show that training on biomedical literature is warranted. The methods discussed here have potential applications for clustering, comparing, analyzing and recommending scientific literature in the biomedical domain.",,,,
6,su_arez-paniagua_word_nodate,"Su ÌÅarez-Paniagua, Victor",Word Embedding Clustering for Disease Named Entity Recognition,,,,,"This paper reports the use of a machine learning-based ap- proach with word embedding features for the Disease Named Entity  Recognition and Normalization subtask of the BioCreative V Chemical- Disease Relation (CDR) challenge task. Firstly, we developed a feature  extraction phase with standard features used in current Named Entity Recognition (NER) systems. Then, we compared the use of the word vectors and the word clusters generated by the Word2Vec tool to add the best of both in the feature set. For this purpose, we trained the Word2Vec models over Wikipedia and MedLine as corpora. Our results suggest that the use of word clusters improves 28\% in F-score in disease  mention recognition and increases precision almost 49\% in the normal- ization task over the baseline system provided by the organizers.",,,,
6,sun_modeling_2015,"Sun, Yaming; Lin, Lei; Tang, Duyu; Yang, Nan; Ji, Zhenzhou; Wang, Xiaolong","Modeling Mention, Context and Entity with Neural Networks for Entity Disambiguation",,,2015,http://dl.acm.org/citation.cfm?id=2832415.2832435,"Given a query consisting of a mention (name string) and a background document, entity disambiguation calls for linking the mention to an entity from reference knowledge base like Wikipedia. Existing studies typically use hand-crafted features to represent mention, context and entity, which is laborintensive and weak to discover explanatory factors of data. In this paper, we address this problem by presenting a new neural network approach. The model takes consideration of the semantic representations of mention, context and entity, encodes them in continuous vector space and effectively leverages them for entity disambiguation. Specifically, we model variable-sized contexts with convolutional neural network, and embed the positions of context words to factor in the distance between context word and mention. Furthermore, we employ neural tensor network to model the semantic interactions between context and mention. We conduct experiments for entity disambiguation on two benchmark datasets from TAC-KBP 2009 and 2010. Experimental results show that our method yields state-of-the-art performances on both datasets.",,,,
6,tan_university_2015,"Tan, Luchen; Roegiest, Adam; Clarke, Charles L. A.",University of Waterloo at TREC 2015 Microblog Track.,,,2015,,,,,,
6,tang_document_2015,"Tang, Duyu; Qin, Bing; Liu, Ting",Document Modeling with Gated Recurrent Neural Network for Sentiment Classification,,,2015,http://aclweb.org/anthology/D15-1167,,,,,
6,tang_document_2015,"Tang, Duyu; Qin, Bing; Liu, Ting",Document Modeling with Gated Recurrent Neural Network for Sentiment Classification,,September,2015,https://www.aclweb.org/anthology/D15-1167,,,,,
6,torabi_asr_querying_2018,"Torabi Asr, Fatemeh; Zinkov, Robert; Jones, Michael",Querying Word Embeddings for Similarity and Relatedness,,June,2018,https://www.aclweb.org/anthology/N18-1062,"Word embeddings obtained from neural network models such as Word2Vec Skipgram have become popular representations of word meaning and have been evaluated on a variety of word similarity and relatedness norming data. Skipgram generates a set of word and context embeddings, the latter typically discarded after training. We demonstrate the usefulness of context embeddings in predicting asymmetric association between words from a recently published dataset of production norms (Jouravlev \& McRae, 2016). Our findings suggest that humans respond with words closer to the cue within the context embedding space (rather than the word embedding space), when asked to generate thematically related words.",,,,
6,trebacz_more_2020,"Trebacz, Maja; Church, Luke",More than a label: machine-assisted data interpretation,,July,2020,https://participatoryml.github.io/,"We motivate and describe a prototype that presents an alternative view on how labelling of data can be done, with the goal of not only efficiently attaching labels to the data, but also supporting a researcher gaining an understanding of the data in the process of labelling.",,,,
6,tripathi_how_2019,"Tripathi, Dhruv; Medlar, Alan; Glowacka, Dorota","How Relevance Feedback is Framed Affects User Experience, but Not Behaviour",,,2019,http://doi.acm.org/10.1145/3295750.3298957,"Retrieval systems based on machine learning require both positive and negative examples to perform inference, which is usually obtained through relevance feedback. Unfortunately, explicit negative relevance feedback is thought to have poor user experience. Instead, systems typically rely on implicit negative feedback. In this study, we confirm that, in the case of binary relevance feedback, users prefer giving positive feedback (and implicit negative feedback) over negative feedback (and implicit positive feedback). These two feedback mechanisms are functionally equivalent, capturing the same information from the user, but differ in how they are framed. Despite users' preference for positive feedback, there were no significant differences in behaviour. As users were not shown how feedback influenced search results, we hypothesise that previously reported results could, at least in part, be due to cognitive biases related to user perception of negative feedback.",,"experimental design, negative relevance feedback, relevance feedback, scientific literature search, user studies",,
6,turner_t-shaped_2019,"Turner, David; Serrano, Diego; Stroulia, Eleni; Lyons, Kelly",A T-shaped Measure of Multidisciplinarity in Academic Research Networks: The GRAND Case Study,,,2019,,"Service-science research has long been studying T-shapedness, arguing that service scientists should be T-shaped individuals, deeply knowledgeable in one field and able to collaborate and communicate across disciplines. The value of multidisciplinarity has also been recognized in academic environments, as funding agencies are committing substantial support to large-scale research initiatives that span across disciplines, organizations, academia and industry, even across national borders, and aim to address the major challenges of our time, from climate change, to energy shortage, to pandemics. New incentives and performance indicators are needed to encourage and reward multidisciplinary collaborative work. In this paper, we introduce a metric for multidisciplinarity, based on the notion of T-shapedness and we report on the application of this measure on data collected over four years from the GRAND Network of Centres of Excellence, a large-scale, Canadian, multidisciplinary research network conducting research on digital media with numerous academic and industrial partners. We describe our findings on how the community evolved over time in terms of its T-shaped multidisciplinarity and compare the multidisciplinarity of GRAND researchers to their non-GRAND peers.",,,,
6,van_dijk_early_2015,"van Dijk, David; Tsagkias, Manos; de Rijke, Maarten",Early Detection of Topical Expertise in Community Question Answering,,,2015,http://doi.acm.org/10.1145/2766462.2767840,"We focus on detecting potential topical experts in community question answering platforms early on in their lifecycle. We use a semi-supervised machine learning approach. We extract three types of feature: (i) textual, (ii) behavioral, and (iii) time-aware, which we use to predict whether a user will become an expert in the longterm. We compare our method to a machine learning method based on a state-of-the-art method in expertise retrieval. Results on data from Stack Overflow demonstrate the utility of adding behavioral and time-aware features to the baseline method with a net improvement in accuracy of 26\% for very early detection of expertise.",,"community question answering, expertise finding, user profiling",,
6,van_gysel_unsupervised_2016,"Van Gysel, Christophe; de Rijke, Maarten; Worring, Marcel","Unsupervised, Efficient and Semantic Expertise Retrieval",,,2016,https://doi.org/10.1145/2872427.2882974,"We introduce an unsupervised discriminative model for the task of retrieving experts in online document collections. We exclusively employ textual evidence and avoid explicit feature engineering by learning distributed word representations in an unsupervised way. We compare our model to state-of-the-art unsupervised statistical vector space and probabilistic generative approaches. Our proposed log-linear model achieves the retrieval performance levels of state-of-the-art document-centric methods with the low inference cost of so-called profile-centric approaches. It yields a statistically significant improved ranking over vector space and generative models in most cases, matching the performance of supervised methods on various benchmarks. That is, by using solely text we can do as well as methods that work with external evidence and/or relevance feedback. A contrastive analysis of rankings produced by discriminative and generative approaches shows that they have complementary strengths due to the ability of the unsupervised discriminative model to perform semantic matching.",,"expertise retrieval, language models, semantic matching",,
6,vasilescu_evaluating_2004,"Vasilescu, Florentina; Langlais, Philippe; Lapalme, Guy",Evaluating Variants of the Lesk Approach for Disambiguating Words,,,2004,,"This paper presents a detailed analysis of the factors determining the performance of Lesk-based word sense disambiguation methods. We conducted a series of experiments on the original Lesk algorithm, adapted to WORDNET, and on some variants. These methods were evaluated on the test corpus from SENSEVAL2, English All Words, and on excerpts from SEMCOR. We designed a fine grain analysis of the answers provided by each variant in order to better understand the algorithms than by the mere precision and recall",,"Dictionary, Experiment, Information, Lesk algorithm, Performance, Precision and recall, Web Services for Devices, Word sense, Word-sense disambiguation, WordNet",,
6,vaswani_attention_2017,"Vaswani, Ashish; Shazeer, Noam; Parmar, Niki; Uszkoreit, Jakob; Jones, Llion; Gomez, Aidan N.; Kaiser, Lukasz; Polosukhin, Illia",Attention Is All You Need,,June,2017,http://arxiv.org/abs/1706.03762,"The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",,"Computer Science - Computation and Language, Computer Science - Machine Learning",,
6,vial_sense_2017,"Vial, LoÃ¯c; Lecouteux, Benjamin; Schwab, Didier",Sense Embeddings in Knowledge-Based Word Sense Disambiguation,,,2017,https://www.aclweb.org/anthology/W17-6940,,,,,
6,wang_chestx-ray8_2017,"Wang, Xiaosong; Peng, Yifan; Lu, Le; Lu, Zhiyong; Bagheri, Mohammadhadi; Summers, Ronald M.",ChestX-Ray8: Hospital-Scale Chest X-Ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases,,July,2017,,"The chest X-ray is one of the most commonly accessible radiological examinations for screening and diagnosis of many lung diseases. A tremendous number of X-ray imaging studies accompanied by radiological reports are accumulated and stored in many modern hospitals Picture Archiving and Communication Systems (PACS). On the other side, it is still an open question how this type of hospital-size knowledge database containing invaluable imaging informatics (i.e., loosely labeled) can be used to facilitate the data-hungry deep learning paradigms in building truly large-scale high precision computer-aided diagnosis (CAD) systems. In this paper, we present a new chest X-ray database, namely ChestX-ray8, which comprises 108,948 frontal-view X-ray images of 32,717 unique patients with the text-mined eight disease image labels (where each image can have multi-labels), from the associated radiological reports using natural language processing. Importantly, we demonstrate that these commonly occurring thoracic diseases can be detected and even spatially-located via a unified weakly-supervised multi-label image classification and disease localization framework, which is validated using our proposed dataset. Although the initial quantitative results are promising as reported, deep convolutional neural network based reading chest X-rays (i.e., recognizing and locating the common disease patterns trained with only image-level labels) remains a strenuous task for fully-automated high precision CAD systems.",,"Biomedical imaging, ChestX-Ray8, Databases, Diseases, Image, Image segmentation, Machine learning, PACS, Pathology, X-ray imaging, computerised tomography, data mining, deep learning paradigms, diagnostic radiography, disease localization framework, diseases, frontal-view X-ray images, fully-automated high precision CAD systems, hospital-scale chest X-ray database, hospital-size knowledge database, hospitals, image classification, image-level labels, large-scale high precision computer, learning (artificial intelligence), lung, medical image processing, natural language processing, neural nets, picture archiving and communication systems, radiological examination, radiology, text-mined eight disease image labels, thorax disease localization, visual databases, weakly-supervised classification",,
6,wang_combining_2017,"Wang, Jin; Wang, Zhongyuan; Zhang, Dawei; Yan, Jun",Combining Knowledge with Deep Convolutional Neural Networks for Short Text Classification,,,2017,http://dl.acm.org/citation.cfm?id=3172077.3172295,"Text classification is a fundamental task in NLP applications. Most existing work relied on either explicit or implicit text representation to address this problem. While these techniques work well for sentences, they can not easily be applied to short text because of its shortness and sparsity. In this paper, we propose a framework based on convolutional neural networks that combines explicit and implicit representations of short text for classification. We first conceptualize a short text as a set of relevant concepts using a large taxonomy knowledge base. We then obtain the embedding of short text by coalescing the words and relevant concepts on top of pre-trained word vectors. We further incorporate character level features into our model to capture fine-grained subword information. Experimental results on five commonly used datasets show that our proposed method significantly outperforms state-of-the-art methods.",,,,
6,wang_glue:_2018,"Wang, Alex; Singh, Amanpreet; Michael, Julian; Hill, Felix; Levy, Omer; Bowman, Samuel",GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding,,,2018,http://aclweb.org/anthology/W18-5446,"For natural language understanding (NLU) technology to be maximally useful, it must be able to process language in a way that is not exclusive to a single task, genre, or dataset. In pursuit of this objective, we introduce the General Language Understanding Evaluation (GLUE) benchmark, a collection of tools for evaluating the performance of models across a diverse set of existing NLU tasks. By including tasks with limited training data, GLUE is designed to favor and encourage models that share general linguistic knowledge across tasks. GLUE also includes a hand-crafted diagnostic test suite that enables detailed linguistic analysis of models. We evaluate baselines based on current methods for transfer and representation learning and ï¬Ånd that multi-task training on all tasks performs better than training a separate model per task. However, the low absolute performance of our best model indicates the need for improved general NLU systems.",,,,
6,wang_mimic-extract_2020,"Wang, Shirly; McDermott, Matthew B. A.; Chauhan, Geeticka; Ghassemi, Marzyeh; Hughes, Michael C.; Naumann, Tristan","MIMIC-Extract: A Data Extraction, Preprocessing, and Representation Pipeline for MIMIC-III",,April,2020,https://doi.org/10.1145/3368555.3384469,"Machine learning for healthcare researchers face challenges to progress and reproducibility due to a lack of standardized processing frameworks for public datasets. We present MIMIC-Extract, an open source pipeline for transforming the raw electronic health record (EHR) data of critical care patients from the publicly-available MIMIC-III database into data structures that are directly usable in common time-series prediction pipelines. MIMIC-Extract addresses three challenges in making complex EHR data accessible to the broader machine learning community. First, MIMIC-Extract transforms raw vital sign and laboratory measurements into usable hourly time series, performing essential steps such as unit conversion, outlier handling, and aggregation of semantically similar features to reduce missingness and improve robustness. Second, MIMIC-Extract extracts and makes prediction of clinically-relevant targets possible, including outcomes such as mortality and length-of-stay as well as comprehensive hourly intervention signals for ventilators, vasopressors, and fluid therapies. Finally, the pipeline emphasizes reproducibility and extensibility to future research questions. We demonstrate the pipeline's effectiveness by developing several benchmark tasks for outcome and intervention forecasting and assessing the performance of competitive models.",,,,
6,wang_tienet_2018,"Wang, Xiaosong; Peng, Yifan; Lu, Le; Lu, Zhiyong; Summers, Ronald M.",TieNet: Text-Image Embedding Network for Common Thorax Disease Classification and Reporting in Chest X-Rays,,June,2018,https://ieeexplore.ieee.org/document/8579041/,"Chest X-rays are one of the most common radiological examinations in daily clinical routines. Reporting thorax diseases using chest X-rays is often an entry-level task for radiologist trainees. Yet, reading a chest X-ray image remains a challenging job for learning-oriented machine intelligence, due to (1) shortage of large-scale machinelearnable medical image datasets, and (2) lack of techniques that can mimic the high-level reasoning of human radiologists that requires years of knowledge accumulation and professional training. In this paper, we show the clinical free-text radiological reportscan be utilized as a priori knowledge for tackling these two key problems. We propose a novel Text-Image Embedding network (TieNet) for extracting the distinctive image and text representations. Multilevel attention models are integrated into an end-to-end trainable CNN-RNN architecture for highlighting the meaningful text words and image regions. We ï¬Årst apply TieNet to classify the chest X-rays by using both image features and text embeddings extracted from associated reports. The proposed auto-annotation framework achieves high accuracy (over 0.9 on average in AUCs) in assigning disease labels for our hand-label evaluation dataset. Furthermore, we transform the TieNet into a chest X-ray reporting system. It simulates the reporting process and can output disease classiï¬Åcation and a preliminary report together. The classiï¬Åcation results are signiï¬Åcantly improved (6\% increase on average in AUCs) compared to the state-of-the-art baseline on an unseen and hand-labeled dataset (OpenI).",,,,
6,wen_enhancing_2019,"Wen, Andrew; Wang, Yanshan; Kaggal, Vinod C.; Liu, Sijia; Liu, Hongfang; Fan, Jungwei",Enhancing Clinical Information Retrieval through Context-Aware Queries and Indices,,December,2019,,"The big data revolution has created a hefty demand for searching large-scale electronic health records (EHRs) to support clinical practice, research, and administration. Despite the volume of data involved, fast and accurate identification of clinical narratives pertinent to a clinical case being seen by any given provider is crucial for decision-making at the point of care. In the general domain, this capability is accomplished through a combination of the inverted index data structure, horizontal scaling, and information retrieval (IR) scoring algorithms. These technologies are also being used in the clinical domain, but have met limited success, particularly as clinical cases become more complex. One barrier affecting clinical performance is that contextual information, such as negation, temporality, and the subject of clinical mentions, impact clinical relevance but is not considered in general IR methodologies. In this study, we implemented a solution by identifying and incorporating the aforementioned semantic contexts as part of IR indexing/scoring with Elasticsearch. Experiments were conducted in comparison to baseline approaches with respect to: 1) evaluation of the impact on the quality (relevance) of the returned results, and 2) evaluation of the impact on execution time and storage requirements. The results showed a 5.1-23.1\% improvement in retrieval quality, along with achieving 35\% faster query execution time. Cost-wise, the solution required 1.5-2 times larger space and about 3 times increase in indexing time. The higher relevance demonstrated the merit of incorporating contextual information into clinical IR, and the near-constant increase in time and space suggested promising scalability.",,"Big Data, Clinical Information Retrieval, Clinical diagnosis, Diabetes, EHR, Elasticsearch, Electronic Health Records, IR indexing, Indexes, Information Retrieval, Manganese, Payloads, Semantics, big data revolution, clinical IR, clinical case, clinical domain, clinical information retrieval, clinical narratives, clinical performance, clinical practice, clinical relevance, context-aware queries, contextual information, data structures, electronic health records, general IR methodologies, general domain, health care, horizontal scaling, indexing, indexing time, large-scale electronic health records, query execution time, query processing, retrieval quality, semantic contexts, ubiquitous computing",,
6,wendlandt_factors_2018,"Wendlandt, Laura; Kummerfeld, Jonathan K.; Mihalcea, Rada",Factors Influencing the Surprising Instability of Word Embeddings,,June,2018,https://aclweb.org/anthology/papers/N/N18/N18-1190/,,,,,
6,yang_social_2011,"Yang, Zi; Cai, Keke; Tang, Jie; Zhang, Li; Su, Zhong; Li, Juanzi",Social context summarization,,,2011,http://portal.acm.org/citation.cfm?doid=2009916.2009954,"We study a novel problem of social context summarization for Web documents. Traditional summarization research has focused on extracting informative sentences from standard documents. With the rapid growth of online social networks, abundant user generated content (e.g., comments) associated with the standard documents is available. Which parts in a document are social users really caring about? How can we generate summaries for standard documents by considering both the informativeness of sentences and interests of social users? This paper explores such an approach by modeling Web documents and social contexts into a uniï¬Åed framework. We propose a dual wing factor graph (DWFG) model, which utilizes the mutual reinforcement between Web documents and their associated social contexts to generate summaries. An efï¬Åcient algorithm is designed to learn the proposed factor graph model. Experimental results on a Twitter data set validate the effectiveness of the proposed model. By leveraging the social context information, our approach obtains signiï¬Åcant improvement (averagely +5.0\%17.3\%) over several alternative methods (CRF, SVM, LR, PR, and DocLead) on the performance of summarization.",,,,
6,yasunaga_graph-based_2017,"Yasunaga, Michihiro; Zhang, Rui; Meelu, Kshitijh; Pareek, Ayush; Srinivasan, Krishnan; Radev, Dragomir",Graph-based Neural Multi-Document Summarization,,,2017,http://aclweb.org/anthology/K17-1045,"We propose a neural multi-document summarization (MDS) system that incorporates sentence relation graphs. We employ a Graph Convolutional Network (GCN) on the relation graphs, with sentence embeddings obtained from Recurrent Neural Networks as input node features. Through multiple layer-wise propagation, the GCN generates high-level hidden sentence features for salience estimation. We then use a greedy heuristic to extract salient sentences while avoiding redundancy. In our experiments on DUC 2004, we consider three types of sentence relation graphs and demonstrate the advantage of combining sentence relations in graphs with the representation power of deep neural networks. Our model improves upon traditional graph-based extractive approaches and the vanilla GRU sequence model with no graph, and it achieves competitive results against other state-of-the-art multidocument summarization systems.",,,,
6,ye_hybrid_2012,"Ye, Zheng; Huang, Jimmy Xiangji; Miao, Jun",A hybrid model for ad-hoc information retrieval,,,2012,http://dl.acm.org/citation.cfm?doid=2348283.2348451,"Many information retrieval (IR) techniques have been proposed to improve the performance, and some combinations of these techniques has been demonstrated to be eï¬€ective. However, how to eï¬€ectively combine them is largely unexplored. It is possible that a method reduces the positive inï¬‚uence of the other one even if both of them are eï¬€ective separately. In this paper, we propose a new hybrid model which can simply and ï¬‚exibly combine components of three diï¬€erent IR techniques under a uniform framework. Extensive experiments on the TREC standard collections indicate that our proposed model can outperform the best TREC systems consistently in the ad-hoc retrieval. It shows that the combination strategy in our proposed model is very effective. Meanwhile, this method is also re-useable for other researchers to test whether their new methods are additive to the current technologies.",,,,
6,yeh_wikiwalk:_2009,"Yeh, Eric; Ramage, Daniel; Manning, Christopher D.; Agirre, Eneko; Soroa, Aitor",WikiWalk: Random Walks on Wikipedia for Semantic Relatedness,,,2009,http://dl.acm.org/citation.cfm?id=1708124.1708133,"Computing semantic relatedness of natural language texts is a key component of tasks such as information retrieval and summarization, and often depends on knowledge of a broad range of real-world concepts and relationships. We address this knowledge integration issue by computing semantic relatedness using personalized PageRank (random walks) on a graph derived from Wikipedia. This paper evaluates methods for building the graph, including link selection strategies, and two methods for representing input texts as distributions over the graph nodes: one based on a dictionary lookup, the other based on Explicit Semantic Analysis. We evaluate our techniques on standard word relatedness and text similarity datasets, finding that they capture similarity information complementary to existing Wikipedia-based relatedness measures, resulting in small improvements on a state-of-the-art measure.",,,,
6,yin_model-based_2018,"Yin, Jianhua; Chao, Daren; Liu, Zhongkun; Zhang, Wei; Yu, Xiaohui; Wang, Jianyong",Model-based Clustering of Short Text Streams,,,2018,http://doi.acm.org/10.1145/3219819.3220094,"Short text stream clustering has become an increasingly important problem due to the explosive growth of short text in diverse social medias. In this paper, we propose a model-based short text stream clustering algorithm (MStream) which can deal with the concept drift problem and sparsity problem naturally. The MStream algorithm can achieve state-of-the-art performance with only one pass of the stream, and can have even better performance when we allow multiple iterations of each batch. We further propose an improved algorithm of MStream with forgetting rules called MStreamF, which can efficiently delete outdated documents by deleting clusters of outdated batches. Our extensive experimental study shows that MStream and MStreamF can achieve better performance than three baselines on several real datasets.",,"dirichlet process, mixture model, text stream clustering",,
6,zamani_neural_2018,"Zamani, Hamed; Mitra, Bhaskar; Song, Xia; Craswell, Nick; Tiwary, Saurabh",Neural Ranking Models with Multiple Document Fields,,,2018,http://doi.acm.org/10.1145/3159652.3159730,"Deep neural networks have recently shown promise in the ad-hoc retrieval task. However, such models have often been based on one field of the document, for example considering document title only or document body only. Since in practice documents typically have multiple fields, and given that non-neural ranking models such as BM25F have been developed to take advantage of document structure, this paper investigates how neural models can deal with multiple document fields. We introduce a model that can consume short text fields such as document title and long text fields such as document body. It can also handle multi-instance fields with variable number of instances, for example where each document has zero or more instances of incoming anchor text. Since fields vary in coverage and quality, we introduce a masking method to handle missing field instances, as well as a field-level dropout method to avoid relying too much on any one field. As in the studies of non-neural field weighting, we find it is better for the ranker to score the whole document jointly, rather than generate a per-field score and aggregate. We find that different document fields may match different aspects of the query and therefore benefit from comparing with separate representations of the query text. The combination of techniques introduced here leads to a neural ranker that can take advantage of full document structure, including multiple instance and missing instance data, of variable length. The techniques significantly enhance the performance of the ranker, and outperform a learning to rank baseline with hand-crafted features.",,"deep neural networks, document representation, neural ranking models, representation learning, structured documents",,
6,zhang_active_2017,"Zhang, Ye; Lease, Matthew; Wallace, Byron C.",Active Discriminative Text Representation Learning,,February,2017,https://arxiv.org/abs/1606.04212,"We propose a new active learning (AL) method for text classiï¬Åcation with convolutional neural networks (CNNs). In AL, one selects the instances to be manually labeled with the aim of maximizing model performance with minimal effort. Neural models capitalize on word embeddings as representations (features), tuning these to the task at hand. We argue that AL strategies for multi-layered neural models should focus on selecting instances that most affect the embedding space (i.e., induce discriminative word representations). This is in contrast to traditional AL approaches (e.g., entropy-based uncertainty sampling), which specify higher level objectives. We propose a simple approach for sentence classiï¬Åcation that selects instances containing words whose embeddings are likely to be updated with the greatest magnitude, thereby rapidly learning discriminative, task-speciï¬Åc embeddings. We extend this approach to document classiï¬Åcation by jointly considering: (1) the expected changes to the constituent word representations; and (2) the modelâ€™s current overall uncertainty regarding the instance. The relative emphasis placed on these criteria is governed by a stochastic process that favors selecting instances likely to improve representations at the outset of learning, and then shifts toward general uncertainty sampling as AL progresses. Empirical results show that our method outperforms baseline AL approaches on both sentence and document classiï¬Åcation tasks. We also show that, as expected, the method quickly learns discriminative word embeddings. To the best of our knowledge, this is the ï¬Årst work on AL addressing neural models for text classiï¬Åcation.",,,,
6,zhang_effective_2018,"Zhang, Haotian; Abualsaud, Mustafa; Ghelani, Nimesh; Smucker, Mark D.; Cormack, Gordon V.; Grossman, Maura R.",Effective User Interaction for High-Recall Retrieval: Less is More,,,2018,http://doi.acm.org/10.1145/3269206.3271796,"High-recall retrieval --- finding all or nearly all relevant documents --- is critical to applications such as electronic discovery, systematic review, and the construction of test collections for information retrieval tasks. The effectiveness of current methods for high-recall information retrieval is limited by their reliance on human input, either to generate queries, or to assess the relevance of documents. Past research has shown that humans can assess the relevance of documents faster and with little loss in accuracy by judging shorter document surrogates, e.g.{\textbackslash} extractive summaries, in place of full documents. To test the hypothesis that short document surrogates can reduce assessment time and effort for high-recall retrieval, we conducted a 50-person, controlled, user study. We designed a high-recall retrieval system using continuous active learning (CAL) that could display either full documents or short document excerpts for relevance assessment. In addition, we tested the value of integrating a search engine with CAL. In the experiment, we asked participants to try to find as many relevant documents as possible within one hour. We observed that our study participants were able to find significantly more relevant documents when they used the system with document excerpts as opposed to full documents. We also found that allowing participants to compose and execute their own search queries did not improve their ability to find relevant documents and, by some measures, impaired performance. These results suggest that for high-recall systems to maximize performance, system designers should think carefully about the amount and nature of user interaction incorporated into the system.",,"electronic discovery, high-recall, systematic review",,
6,zhang_geospatial_2007,"Zhang, J.; Shi, H.",Geospatial visualization using google maps: a case study on conference presenters,,August,2007,,,,"Computer science, Data visualization, Decision making, Earth, GIS, Geographic Information Systems, Google maps, Internet, Microsoft Virtual Earth, Spatial databases, Visual databases, Weather forecasting, Web server, Web services, cartography, conference presenters, data mining, data visualisation, free Web mapping services providers, geographic information systems, geographic location mapping methodologies, geospatial location, geospatial visualization, location-based decision making, map feature generation",,
6,zhang_graph_2018,"Zhang, Zhiqiang; Wang, Linan; Xie, Xiaoqin; Pan, Haiwei",A Graph Based Document Retrieval Method,,May,2018,,A new document retrieval method based on graph was proposed in this paper. Queries and documents are represented by graphs. The paper also proposes the concept of the document semantic unit in consideration of the overhead of graph computing. The size of semantic unit is used as the granularity for graph construction. This new method puts queries and documents in an unequal level instead of regarding them as equivalent entities which conventional IR system does. The paper further proposes the similarity calculating method of graphs based on general maximum common subgraph. The result of the experiment shows this method is able to yield better document retrieval results.,,"Computational modeling, Computer science, Information retrieval, Mathematical model, Natural languages, Semantics, Tools, conventional IR system, document handling, document semantic unit, graph computing, graph construction, graph theory, graph-based document retrieval method, information retrieval, similarity calculating method",,
6,zhang_omnia_2015,"Zhang, Yating; Jatowt, Adam; Bhowmick, Sourav; Tanaka, Katsumi","Omnia Mutantur, Nihil Interit: Connecting Past with Present by Finding Corresponding Terms across Time",,July,2015,https://www.aclweb.org/anthology/P15-1063,,,,,
6,zhao_explainable_2016,"Zhao, Yukun; Liang, Shangsong; Ren, Zhaochun; Ma, Jun; Yilmaz, Emine; de Rijke, Maarten",Explainable User Clustering in Short Text Streams,,,2016,http://doi.acm.org/10.1145/2911451.2911522,"User clustering has been studied from different angles: behavior-based, to identify similar browsing or search patterns, and content-based, to identify shared interests. Once user clusters have been found, they can be used for recommendation and personalization. So far, content-based user clustering has mostly focused on static sets of relatively long documents. Given the dynamic nature of social media, there is a need to dynamically cluster users in the context of short text streams. User clustering in this setting is more challenging than in the case of long documents as it is difficult to capture the users' dynamic topic distributions in sparse data settings. To address this problem, we propose a dynamic user clustering topic model (or UCT for short). UCT adaptively tracks changes of each user's time-varying topic distribution based both on the short texts the user posts during a given time period and on the previously estimated distribution. To infer changes, we propose a Gibbs sampling algorithm where a set of word-pairs from each user is constructed for sampling. The clustering results are explainable and human-understandable, in contrast to many other clustering algorithms. For evaluation purposes, we work with a dataset consisting of users and tweets from each user. Experimental results demonstrate the effectiveness of our proposed clustering model compared to state-of-the-art baselines.",,"short text processing, user clustering, user topic modeling",,
6,zheng_learning_2017,"Zheng, Xiaoqing; Feng, Jiangtao; Chen, Yi; Peng, Haoyuan; Zhangs, Wenqing",Learning Context-Specific Word/Character Embeddings,,February,2017,,"Unsupervised word representations have demonstrated improvements in predictive generalization on various NLP tasks. Most of the existing models are in fact good at capturing the relatedness among words rather than their â€œgenuineâ€ù similarity because the context representations are often represented by a sum (or an average) of the neighborâ€™s embeddings, which simpliï¬Åes the computation but ignores an important fact that the meaning of a word is determined by its context, reï¬‚ecting not only the surrounding words but also the rules used to combine them (i.e. compositionality). On the other hand, much effort has been devoted to learning a singleprototype representation per word, which is problematic because many words are polysemous, and a single-prototype model is incapable of capturing phenomena of homonymy and polysemy. We present a neural network architecture to jointly learn word embeddings and context representations from large data sets. The explicitly produced context representations are further used to learn context-speciï¬Åc and multiprototype word embeddings. Our embeddings were evaluated on several NLP tasks, and the experimental results demonstrated the proposed model outperformed other competitors and is applicable to intrinsically â€œcharacter-basedâ€ù languages.",,,,
6,zhu_deep_2017,"Zhu, Xiaodan; Grefenstette, Edward",Deep Learning for Semantic Composition,,July,2017,https://www.aclweb.org/anthology/P17-5003/,,,,,
9,tilbury_word_2018,"Tilbury, Kyle",Word Embeddings for Domain Specific Semantic Relatedness,,November,2018,https://DalSpace.library.dal.ca//handle/10222/74927,"Word embeddings are becoming pervasive in natural language processing (NLP), with one of their main strengths being their ability to capture semantic relationships between words. Rather than training their own embeddings many NLP practitioners elect to use pre-trained word embeddings. These pre-trained embeddings are typically created and evaluated using general corpora. Thus, there is a deficiency in the understanding of their performance within a technical domain. In this thesis, we explore how the nature of the data used to train embeddings can affect their performance when computing semantic relatedness within different domains. The three main contributions are as follows. Firstly, we find that the performance of general pre-trained embeddings is lacking in the biomedical domain. Secondly, we provide key insights that should be considered when working with word embeddings for any semantic task. Finally, we develop new biomedical word embeddings and provide them as publicly available for use by others.",,,,
10,ahuja_why_2020,"Ahuja, Harshit",Why Is Data Science Losing Its Charm?,Medium,June,2020,https://towardsdatascience.com/why-is-data-science-losing-its-charm-3f7780b443f5,Data science was once the most loved career option but the trends are changing.,,,,
10,alammar_illustrated_nodate,"Alammar, Jay","The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)",,,,http://jalammar.github.io/illustrated-bert/,"Discussions: Hacker News (98 points, 19 comments), Reddit r/MachineLearning (164 points, 20 comments)   Translations: Chinese (Simplified), Persian  The year 2018 has been an inflection point for machine learning models handling text (or more accurately, Natural Language Processing or NLP for short). Our conceptual understanding of how best to represent words and sentences in a way that best captures underlying meanings and relationships is rapidly evolving. Moreover, the NLP community has been putting forward incredibly powerful components that you can freely download and use in your own models and pipelines (Itâ€™s been referred to as NLPâ€™s ImageNet moment, referencing how years ago similar developments accelerated the development of machine learning in Computer Vision tasks).",,,,
10,andrea_moro_and_roberto_navigli_babelfy:_nodate,Andrea Moro; Roberto Navigli,Babelfyâ„¢: How to query Babelfy programmatically,,,,http://babelfy.org/guide,"This page describes how you can query Babelfy through an HTTP interface that returns JSON. Before you can use the HTTP API you must obtain an API KEY by  (note that keys are shared across BabelNet and Babelfy, so if you already registered to the BabelNet website, you can just retrieve your Babelfy key by ). You can then append the key parameter to the HTTP requests as shown in the examples below. All requests must be executed using the GET method and they should include the Accept-Encoding: gzip header in order to obtain compressed content.",,,,
10,armin_sajadi_wikisim_2018,"Sajadi, Armin",wikisim on github,,April,2018,https://github.com/asajadi/wikisim/tree/master/datasets/ner,,,,,
10,bensen_press_elmo_2019,bensen\_press,ELMo Meet BERT: Recent Advances in Natural Language Embeddings,Bensen AI,February,2019,https://bensen.ai/elmo-meet-bert-recent-advances-in-natural-language-embeddings/,"Embeddings are a key tool in transfer learning in NLP. Earlier this year, the paperÂ â€œDeep contextualized word representationsâ€ùÂ introduced ELMo (2018), a new technique for embedding words into real vector space using bidirectional LSTMs trained on aÂ language modelingÂ objective. In addition to beating previous performance benchmarks, using ELMo as a pre-trained embedding for other NLP",,,,
10,boulton_integer_2018,"Boulton, Freddy",Integer Programming in Python,Towards Data Science,March,2018,https://towardsdatascience.com/integer-programming-in-python-1cbdfa240df2,Integer Programming (IP) problems are optimization problems where all of the variables are constrained to be integers. IP problems areâ€_,,,,
10,brightmart_albert_2019,brightmart,ALBERT code brightmart/albert\_zh,,October,2019,https://github.com/brightmart/albert_zh,"An Implementation of A Lite Bert For Self-Supervised Learning Language Representations with TensorFlow  ALBert is based on Bert, but with some improvements. It achieves state of the art performance on main benchmarks with 30\% parameters less.  For albert\_base\_zh it only has ten percentage parameters compare of original bert model, and main accuracy is retained.",,"albert, bert, chinese-corpus, pre-trained, pre-trained-model, pytorch, roberta, tensorflow, xlnet",,
10,brownlee_gentle_2016,"Brownlee, Jason",A Gentle Introduction to XGBoost for Applied Machine Learning,Machine Learning Mastery,August,2016,https://machinelearningmastery.com/gentle-introduction-xgboost-applied-machine-learning/,"XGBoost is an algorithm that has recently been dominating applied machine learning and Kaggle competitions for structured or tabular data. XGBoost is an implementation of gradient boosted decision trees designed for speed and performance. In this post you will discover XGBoost and get a gentle introduction to what is, where it came from and how â€_",,,,
10,camacho-collados_word_nodate,"Camacho-Collados, Jose","Word, Sense and Contextualized Embeddings: Vector Representations of Meaning in NLP",,,,,,,,,
10,cskopecce_ultimate_2017,cskopecce,The Ultimate List of Medical Specialties - Explore Options \textbar SGU,,December,2017,https://www.sgu.edu/blog/medical/ultimate-list-of-medical-specialties/,"See The Ultimate List of Medical Specialties \& Explore The Type of Medicine Options You Have: Internal Medicine, Surgery Specialties \& more.",,,,
10,dharun_playing_2019,Dharun,Playing with XLNet,Medium,July,2019,https://medium.com/@dharun199531/playing-with-xlnet-79efc8360590,XLNet is the most recent model released by Google and CMU which outperformed BERT on several downstream tasks.,,,,
10,dietz_utilizing_2019,"Dietz, Laura","Utilizing Knowledge Graphs in Text-centric Information Retrieval: laura-dietz/tutorial-utilizing-kg - RESOURCES""",,August,2019,https://github.com/laura-dietz/tutorial-utilizing-kg,,,,,
10,ehsan_vector_2019,Ehsan,Vector Embedding of Wikipedia Concepts and Entities - Code,,April,2019,https://github.com/ehsansherkat/ConVec,"In this project, we use skip-gram model to embed Wikipedia Concepts and Entities. The English version of Wikipedia contains more than five million pages, which suggest its capability to cover many ..",,,,
10,farahmand_pre-trained_2019,"Farahmand, Meghdad",Pre-trained Word Embeddings or Embedding Layer: A Dilemma,Medium,June,2019,https://towardsdatascience.com/pre-trained-word-embeddings-or-embedding-layer-a-dilemma-8406959fd76c,A comparison between pre-trained word embeddings and embedding layers on the performance of semantic NLP tasks,,,,
10,ghaddarabs_ghaddarabswiner_2020,ghaddarAbs,ghaddarAbs/WiNER,,February,2020,https://github.com/ghaddarAbs/WiNER,Contribute to ghaddarAbs/WiNER development by creating an account on GitHub.,,,,
10,gildenblat_overview_nodate,"Gildenblat, Jacob",Overview of Active Learning for Deep Learning,,,,https://jacobgil.github.io/deeplearning/activelearning,,,,,
10,government_of_canada_nserc_2016,"Government of Canada, Natural Sciences; Engineering Research Council of Canada",NSERC â€“ List of Evaluation Groups and Research Topics,Natural Sciences and Engineering Research Council of Canada (NSERC),June,2016,http://www.nserc-crsng.gc.ca/Professors-Professeurs/Grants-Subs/DGPList-PSDListe_eng.asp,,,,,
10,hardt_simple_nodate,"Hardt, Moritz","Simple and efficient semantic embeddings for rare words, n-grams, and language features",Off the convex path,,,http://offconvex.github.io/2018/09/18/alacarte/,Algorithms off the convex path.,,,,
10,hazoom_word2vec_2018,"Hazoom, Moshe",Word2Vec For Phrases â€” Learning Embeddings For More Than One Word,Medium,December,2018,https://towardsdatascience.com/word2vec-for-phrases-learning-embeddings-for-more-than-one-word-727b6cf723cf,How to learn similar terms in a given unsupervised corpus using Word2Vec,,,,
10,horev_bert_2018,"Horev, Rani",BERT Explained: State of the art language model for NLP,Towards Data Science,November,2018,https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270,"An approachable and understandable explanation of BERT, a recent paper by Google that achieved SOTA results in wide variety of NLP tasks.",,,,
10,howard_fastai_nodate,"Howard, Jeremy; Thomas, Rachel; Gugger, Sylvain",Fastai,,,,https://www.fast.ai/,"The fastai library simplifies training fast and accurate neural nets using modern best practices. It's based on research in to deep learning best practices undertaken at fast.ai, including out of the box"" support for vision", text, tabular, and collab (collaborative filtering) models. If you're looking for the source code, head over to the fastai repo on GitHub. For brief examples
10,jacob_devlin_bert:_nodate,Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,,,,https://arxiv.org/abs/1810.04805,,,,,
10,kandi_handling_2018,"Kandi, Shabeel",Handling Out-of-Vocabulary Words in Natural Language Processing based on Context,Medium,October,2018,https://medium.com/@shabeelkandi/handling-out-of-vocabulary-words-in-natural-language-processing-based-on-context-4bbba16214d5,Word Embeddings encode the relationships between words through vector representations of the words. These word vectors are analogous toâ€_,,,,
10,kandi_handling-out--vocabulary-words--natural-language-processing-using-language-modelling_2019,"Kandi, Shabeel",Handling-Out-of-Vocabulary-Words-in-Natural-Language-Processing-using-Language-Modelling development by creating an account on GitHub,,July,2019,https://github.com/shabeelkandi/Handling-Out-of-Vocabulary-Words-in-Natural-Language-Processing-using-Language-Modelling,,,,,
10,kolloju_santhoshkolloju/abstractive-summarization--transfer-learning_2019,"Kolloju, Santhosh",santhoshkolloju/Abstractive-Summarization-With-Transfer-Learning,,December,2019,https://github.com/santhoshkolloju/Abstractive-Summarization-With-Transfer-Learning,Abstractive summarisation using Bert as encoder and Transformer Decoder,,"abstractive-summarization, abstractive-text-summarization, bert, bert-model, nlg, nlp, summarization, transfer-learning, transformer",,
10,lai_bert-embedding:_nodate,"Lai, Gary",bert-embedding: BERT token level embedding with MxNet,,,,https://github.com/imgarylai/bert_embedding,,,"Scientific/Engineering - Artificial Intelligence, Scientific/Engineering - Information Analysis, bert,, deep,, embedding, encoding,, gluonnlp,, learning,, machine,, mxnet,, nlp,, sentence,",,
10,lecun_deep_nodate,"LeCun, Yann","Deep learning, neural networks and the future of AI",,,,https://www.ted.com/talks/yann_lecun_deep_learning_neural_networks_and_the_future_of_ai,"Yann LeCun, the chief AI scientist at Facebook, helped develop the deep learning algorithms that power many artificial intelligence systems today. In conversation with head of TED Chris Anderson, LeCun discusses his current research into self-supervised machine learning, how he's trying to build machines that learn with common sense (like humans) and his hopes for the next conceptual breakthrough in AI.",,,,
10,liu_nlpyang/presumm_2019,"Liu, Yang",nlpyang/PreSumm,,December,2019,https://github.com/nlpyang/PreSumm,code for EMNLP 2019 paper Text Summarization with Pretrained Encoders,,,,
10,ma_3_2019,"Ma, Edward",3 silver bullets of word embeddings in NLP,Medium,February,2019,https://towardsdatascience.com/3-silver-bullets-of-word-embedding-in-nlp-10fa8f50cc5a,Word Embedding is silver bullet to resolve many NLP problem. Most of modern NLP architecture adopted word embedding and giving upâ€_,,,,
10,ma_examples_2019,"Ma, Edward",Some examples of applying BERT in specific domain,Medium,May,2019,https://towardsdatascience.com/how-to-apply-bert-in-scientific-domain-2d9db0480bd9,Applying BERT in specific domain,,,,
10,mandot_how_2019,"Mandot, Pushkar",How exactly XGBoost Works?,Medium,February,2019,https://medium.com/@pushkarmandot/how-exactly-xgboost-works-a320d9b8aeef,The Story of the fitting model on gradients,,,,
10,mcmahan_ru_2016,"McMahan, Brian",RU Twitter Analysis (ML Spring 2012 Project). Contribute to braingineer/RUTA development by creating an account on GitHub,,June,2016,https://github.com/braingineer/RUTA,,,,,
10,messly__5_2018,"\textbar, Messly",5 Reasons Why Doctors Choose Their Medical Specialties,The Messly Blog,March,2018,https://blog.messly.co.uk/reasons-why-doctors-choose-specialties/,"Why do doctors tend to choose one medical specialty over another and how should you approach the subject? In 2017, the BMA conducted some qualitative research into the drivers of career choices and here are the top 5 factors driving specialty selection.",,,,
10,miller_bert-extractive-summarizer_nodate,"Miller, Derek",bert-extractive-summarizer: Extractive Text Summarization with BERT,,,,https://github.com/dmmiller612/bert-extractive-summarizer,,,"bert,, deep learning,, extractive summarization,, machine learning,, pytorch,, summarization",,
10,miller_bert-extractive-summarizer:_nodate,"Miller, Derek",bert-extractive-summarizer: Extractive Text Summarization with BERT,,,,https://github.com/dmmiller612/bert-extractive-summarizer,,,"bert,, deep learning,, extractive summarization,, machine learning,, pytorch,, summary",,
10,mitra_learning_2019,"Mitra, Bhaskar",Learning to Match Using Local and Distributed Representations of Text for Web Search: A Duet Neural Document Ranking Model,,May,2019,https://github.com/bmitra-msft/NDRM,,,,,
10,morde_xgboost_2019,"Morde, Vishal",XGBoost Algorithm: Long May She Reign!,Medium,April,2019,https://towardsdatascience.com/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d,The new queen of Machine Learning algorithms taking over the worldâ€_,,,,
10,noauthor_10_2018,,10 Exciting Ideas of 2018 in NLP,Sebastian Ruder,December,2018,http://ruder.io/10-exciting-ideas-of-2018-in-nlp/,"This post gathers 10 ideas that I found exciting and impactful this yearâ€”and that we'll likely see more of in the future. For each idea, it highlights 1-2 papers that execute them well.",,,,
10,noauthor_190911202_nodate,,[1909.11202] A Visual Analytics Framework for Adversarial Text Generation,,,,https://arxiv.org/abs/1909.11202,,,text generation,,
10,noauthor_200514165_nodate,,[2005.14165] Language Models are Few-Shot Learners,,,,https://arxiv.org/abs/2005.14165,,,,,
10,noauthor_8_nodate,,(8) (PDF) A Survey of Topic Modeling in Text Mining,ResearchGate,,,https://www.researchgate.net/publication/276327703_A_Survey_of_Topic_Modeling_in_Text_Mining,"ResearchGate is a network dedicated to science and research. Connect, collaborate and discover scientific publications, jobs and conferences. All for free.",,,,
10,noauthor_accepted_nodate,,Accepted Papers \textbackslashtextbar SIGIR 2018,,,,https://sigir.org/sigir2018/accepted-papers/,,,,,
10,noauthor_allennlp_nodate,,AllenNLP,,,,%PUBLIC_URL%,"AllenNLP is a free, open-source natural language processing platform for building state of the art models.",,,,
10,noauthor_allennlp_nodate,,AllenNLP Interpret,,,,https://allennlp.org/interpret,,,,,
10,noauthor_analyzing_2019,,Analyzing the Design Space for Visualizing Neural Attention in Text Classification,,August,2019,https://observablehq.com/@clpuc/analyzing-the-design-space-for-visualizing-neural-attenti,An Observable notebook by PUC Chile.,,,,
10,noauthor_annotated_nodate,,The Annotated Transformer: code explained,,,,http://nlp.seas.harvard.edu/2018/04/03/attention.html,,,,,
10,noauthor_augmenting_nodate,,Augmenting pre-trained word embeddings with domain specific data Â· Issue \#622 Â· zalandoresearch/flair,GitHub,,,https://github.com/zalandoresearch/flair/issues/622,I have a small corpus (about 25 million words) from a very specific domain that is not well represented by any of the large corpora used to train the word embeddings available in Flair. My idea is ...,,,,
10,noauthor_bert_nodate,,BERT Word Embeddings Tutorial Â· Chris McCormick,,,,https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/,,,,,
10,noauthor_bert_nodate,,BERT code on GIthub,,,,https://github.com/google-research/bert,"BERT, or Bidirectional Encoder Representations from Transformers, is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.  Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: https://arxiv.org/abs/1810.04805.",,,,
10,noauthor_bert_nodate,,BERT pre-training using only domain specific text Â· Issue \#615 Â· google-research/bert,GitHub,,,https://github.com/google-research/bert/issues/615,"BERT is pre-trained using Wikipedia and other sources of normal text, but my problem domain has a very specific vocabulary \&amp; grammar. Is there an easy way to train BERT completely from domain s...",,,,
10,noauthor_best_nodate,,Best Practices for Text Classification with Deep Learning,,,,https://machinelearningmastery.com/best-practices-document-classification-deep-learning/,,,,,
10,noauthor_book_nodate,,Book an ACM Distinguished Speaker and Make Your Next Event a Hit,,,,https://speakers.acm.org/,,,,,
10,noauthor_challenge_nodate,,The Challenge \textbar bioasq.org,,,,http://bioasq.org/,,,,,
10,noauthor_challenges_nodate,,Challenges of Development \& Validation of Deep Learning for Radiology,,,,http://blog.qure.ai/notes/deep-learning-radiology-challenges,,,,,
10,noauthor_chexpert_nodate,,CheXpert: A Large Dataset of Chest X-Rays and Competition for Automated Chest X-Ray Interpretation.,,,,https://stanfordmlgroup.github.io/competitions/chexpert/,"CheXpert is a large dataset of chest x-rays and competition for automated chest x-ray interpretation, which features uncertainty labels and radiologist-labeled reference standard evaluation sets.",,,,
10,noauthor_cluster-based_nodate,,Cluster-Based Active Learning [MATLAB],,,,,,,,,
10,noauthor_cognate_2020,,Cognate,Wikipedia,February,2020,https://en.wikipedia.org/w/index.php?title=Cognate&oldid=939825640,"In linguistics, cognates are words that have a common etymological origin. Cognates are often inherited from a shared parent language, but they may also involve borrowings from some other language. For example, the English words dish and desk and the German word Tisch (table"") are cognates because they all come from Latin discus", which relates to their flat surfaces. Cognates may have evolved similar, different or even opposite meanings, but in most cases there are some similar sounds or letters in the words, in some cases appearing to be dissimilar. Some words sound similar
10,noauthor_cognitive_nodate,,Cognitive Computation Group,,,,https://cogcomp.org/page/software_view/Wikifier,,,,,
10,noauthor_common_nodate,,Common Crawl,,,,https://commoncrawl.org/,,,,,
10,noauthor_data_nodate,,Data Science for Good: City of Los Angeles,,,,https://kaggle.com/c/data-science-for-good-city-of-los-angeles,Help the City of Los Angeles to structure and analyze its job descriptions,,,,
10,noauthor_dmis-lab/biobert_2019,,dmis-lab/biobert,,December,2019,https://github.com/dmis-lab/biobert,BioBERT: a pre-trained biomedical language representation model,,,,
10,noauthor_elementaibaal_2020,,ElementAI/baal,,July,2020,https://github.com/ElementAI/baal,Using approximate bayesian posteriors in deep nets for active learning,,,,
10,noauthor_elmo:_nodate,,ELMo: Deep contextualized word representations,,,,https://allennlp.org/elmo,,,,,
10,noauthor_elsevier_nodate,,Elsevier Enhanced Reader,,,,https://reader.elsevier.com/reader/sd/pii/S2468502X17300074?token=81475D74AC3084E36E50432D8C288672E6F1C91D2C8E3A637A8357776E9DF5D3CE518BF60E18AA76CD0F08C09846BDEB,,,,,
10,noauthor_english_nodate,,English word vectors Â· fastText,,,,https://fasttext.cc/index.html,This page gathers several pre-trained word vectors trained using fastText.,,,,
10,noauthor_evidence-aware_nodate,,Evidence-Aware Hierarchical Interactive Attention Networks for Explainable Claim Verification,,,,,,,,,
10,noauthor_examples_nodate,,Examples using Common Crawl Data â€“ Common Crawl,,,,https://commoncrawl.org/the-data/examples/,,,,,
10,noauthor_experimental_nodate,,(experimental) Dynamic Quantization on BERT â€” PyTorch Tutorials 1.5.0 documentation,,,,https://pytorch.org/tutorials/intermediate/dynamic_quantization_bert_tutorial.html,,,,,
10,noauthor_explainable_2019,,Explainable artificial intelligence,Wikipedia,July,2019,https://en.wikipedia.org/w/index.php?title=Explainable_artificial_intelligence&oldid=906684460,"Explainable AI (XAI), Interpretable AI, or Transparent AI refer to techniques in artificial intelligence (AI) which can be trusted and easily understood by humans. It contrasts with the concept of the black box"" in machine learning where even their designers cannot explain why the AI arrived at a specific decision. XAI can be used to implement a social right to explanation. Some claim that transparency rarely comes for free and that there are often tradeoffs between how ""smart"" an AI is and how transparent it is; these tradeoffs are expected to grow larger as AI systems increase in internal complexity. The technical challenge of explaining AI decisions is sometimes known as the interpretability problem. Another consideration is info-besity (overload of information)", thus, full transparency may not be always possible or even required. The amount of information presented should vary based on the stakeholder interacting with the intelligent system AI systems optimize behavior to satisfy a mathematically-specified goal system chosen by the system designers," such as the command ""maximize accuracy of assessing how positive film reviews are in the test dataset"". The AI may learn useful general rules from the test-set"," such as ""reviews containing the word 'horrible'"" are likely to be negative"". However"
10,noauthor_exploring_nodate,,Exploring Unsupervised Graph Learning in Literature-Based Discovery,,,,,,,,,
10,noauthor_generalized_2019,,Generalized Language Models: BERT \& OpenAI GPT-2,TOPBOTS,April,2019,https://www.topbots.com/generalized-language-models-bert-openai-gpt2/,"EDITORâ€™S NOTE: Generalized Language Models is an extensive four-part series by Lillian Weng of OpenAI.Â  Part 1: CoVe, ELMo \& Cross-View Training Part 2: ULMFiT \& OpenAI GPT Part 3: BERT \& OpenAI GPT-2 Part 4: Common Tasks \& Datasets Do you find this in-depth technical education about language models and NLP applications to be â€_",,,,
10,noauthor_github_nodate,,"GitHub - cbaziotis/neat-vision: Neat (Neural Attention) Vision, is a visualization tool for the attention mechanisms of deep-learning models for Natural Language Processing (NLP) tasks. (framework-agnostic)",,,,https://github.com/cbaziotis/neat-vision,,,,,
10,noauthor_github_nodate,,GitHub - ElementAI/baal: Using approximate bayesian posteriors in deep nets for active learning,,,,https://github.com/ElementAI/baal,,,,,
10,noauthor_gpt-3_2020,,GPT-3: Language Models are Few-Shot Learners (Paper Explained),,May,2020,https://www.youtube.com/watch?v=SY5PvZrJhLE,"How far can you go with ONLY language modeling? Can a large enough language model perform NLP task out of the box? OpenAI take on these and other questions by training a transformer that is an order of magnitude larger than anything that has ever been built before and the results are astounding.  OUTLINE: 0:00 - Intro \&amp; Overview 1:20 - Language Models 2:45 - Language Modeling Datasets 3:20 - Model Size 5:35 - Transformer Models 7:25 - Fine Tuning 10:15 - In-Context Learning 17:15 - Start of Experimental Results 19:10 - Question Answering 23:10 - What I think is happening 28:50 - Translation 31:30 - Winograd Schemes 33:00 - Commonsense Reasoning 37:00 - Reading Comprehension 37:30 - SuperGLUE 40:40 - NLI 41:40 - Arithmetic Expressions 48:30 - Word Unscrambling 50:30 - SAT Analogies 52:10 - News Article Generation 58:10 - Made-up Words 1:01:10 - Training Set Contamination 1:03:10 - Task Examples  https://arxiv.org/abs/2005.14165 https://github.com/openai/gpt-3  Abstract: Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.  Authors: Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei  Links: YouTube: https://www.youtube.com/c/yannickilcher Twitter: https://twitter.com/ykilcher BitChute: https://www.bitchute.com/channel/yann... Minds: https://www.minds.com/ykilcher",,,,
10,noauthor_gpt-3_2020,,GPT-3 Wikipedia,Wikipedia,August,2020,https://en.wikipedia.org/w/index.php?title=GPT-3&oldid=973517827,"Generative Pre-trained Transformer 3 (GPT-3) is an autoregressive language model that uses deep learning to produce human-like text.  It is the third-generation language prediction model in the GPT-n series created by OpenAI, a for-profit San Francisco-based artificial intelligence research laboratory. GPT-3's full version has a capacity of  175 billion machine learning parameters, which is over two orders of magnitude greater than that of its predecessor, GPT-2. GPT-3, which was introduced in May 2020, and is in beta testing as of July 2020,  is part of a trend in natural language processing (NLP) systems of pre-trained language representations. Prior to the release of GPT-3, the largest language model was Microsoft's Turing NLG, introduced in February 2020, with a capacity of 17 billion parameters or less than 10 percent compared to GPT-3.The quality of the text generated by GPT-3 is so high that it is difficult to distinguish from that written by a human, which has both benefits and risks. Thirty-one OpenAI researchers and engineers presented the original May 28, 2020 paper introducing GPT-3. In their paper, they warned of GPT-3's potential dangers and called for research to mitigate risk. David Chalmers, an Australian philosopher, described GPT-3 as one of the most interesting and important AI systems ever produced.""""",,,,
10,noauthor_gpt-3_nodate,,GPT-3 [2005.14165] Language Models are Few-Shot Learners,,,,https://arxiv.org/abs/2005.14165,,,,,
10,noauthor_guidance_nodate,,Guidance on loading embeddings with multi-word phrases Â· Issue \#2192 Â· explosion/spaCy,GitHub,,,https://github.com/explosion/spaCy/issues/2192,"Many pre-trained embeddings have multi-word phrases included in the vocabulary. In particular, I\&\#39;m interested in using Mikolov\&\#39;s latest work: https://fasttext.cc/docs/en/english-vectors.htm...",,,,
10,noauthor_how_nodate,,How to use BERT for finding similar sentences or similar news? Â· Issue \#876 Â· huggingface/transformers,GitHub,,,https://github.com/huggingface/transformers/issues/876,"I have used BERT NextSentencePredictor to find similar sentences or similar news, However, It\&\#39;s super slow. Even on Tesla V100 which is the fastest GPU till now. It takes around 10secs for a qu...",,,,
10,noauthor_huaweis_nodate,,Huaweiâ€™s TinyBERT Is 7X Smaller and 9X Faster Than BERT,,,,https://medium.com/syncedreview/huaweis-tinybert-is-7x-smaller-and-9x-faster-than-bert-2fbe76f03974,,,,,
10,noauthor_human---loop_nodate,,Human-in-the-Loop Machine Learning,Manning Publications,,,https://www.manning.com/books/human-in-the-loop-machine-learning,"I've learned a lot of new things about Machine Learning I would never even have considered before.  How humans and machines should work together to solve problems is one of the most important questions in technology. However, in machine learning, the accuracy of innovative algorithms often end up with the most attention. But to build the most accurate model quickly you also need clean, relevant, correctly-labeled data for your system to train on. Human-in-the-Loop Machine Learning is a practical guide to optimizing the entire machine learning process, including techniques for annotation, sampling, and even using ML systems to help automate the process.",,,,
10,noauthor_implicit_nodate,,Implicit Biases Against People with Disabilities,,,,https://www.americanbar.org/groups/diversity/disabilityrights/resources/implicit_bias/,A guide discussing implicit bias and how to address it.,,,,
10,noauthor_interactive_nodate,,Interactive Analysis of Word Vector Embeddings,,,,http://graphics.cs.wisc.edu/Vis/EmbVis/,,,,,
10,noauthor_inter-american_2020,,Inter-American Development Bank,Wikipedia,July,2020,https://en.wikipedia.org/w/index.php?title=Inter-American_Development_Bank&oldid=967729344,"The Inter-American Development Bank (IADB or IDB or BID) is the largest source of development financing for Latin America and the Caribbean. Established in 1959, the IDB supports Latin American and Caribbean economic development, social development and regional integration by lending to governments and government agencies, including State corporations. The IDB has four official languages: English, French, Portuguese, and Spanish. Its official names in the other three languages are as follows:  In all three other languages the Bank uses the acronym BID"".""",,,,
10,noauthor_introduction_2019,,Introduction to PyTorch-Transformers with Python Implementation,Analytics Vidhya,July,2019,https://www.analyticsvidhya.com/blog/2019/07/pytorch-transformers-nlp-python/,PyTorch Transformers is the latest state-of-the-art NLP library for performing human-level tasks. Learn how to use PyTorch Transfomers in Python.,,,,
10,noauthor_introduction_nodate,,Introduction to Common Crawl - [PDF Document],,,,https://vdocuments.mx/introduction-to-common-crawl.html,,,,,
10,noauthor_invision_nodate,,InVision \textbar User interface Mockups,InVision,,,https://www.invisionapp.com/,InVision is the digital product design platform used to make the worldâ€™s best customer experiences.,,,,
10,noauthor_jackknife_2019,,Jackknife resampling,Wikipedia,November,2019,https://en.wikipedia.org/w/index.php?title=Jackknife_resampling&oldid=928068336,"In statistics, the jackknife is a resampling technique especially useful for variance and bias estimation. The jackknife pre-dates other common resampling methods such as the bootstrap. The jackknife estimator of a parameter is found by systematically leaving out each observation from a dataset and calculating the estimate and then finding the average of these calculations. Given a sample of size                         n                 \{{\textbackslash}displaystyle n\}   , the jackknife estimate is found by aggregating the estimates of each                         (         n         âˆ’         1         )                 \{{\textbackslash}displaystyle (n-1)\}   -sized sub-sample. The jackknife technique was developed by Maurice Quenouille (1924â€“1973) from 1949 and refined in 1956. John Tukey expanded on the technique in 1958 and proposed the name jackknife"" because", like a physical jack-knife (a compact folding knife)," it is a rough-and-ready tool that can improvise a solution for a variety of problems even though specific problems may be more efficiently solved with a purpose-designed tool.The jackknife is a linear approximation of the bootstrap.""",,
10,noauthor_language_2019,,"Language Models are Unsupervised Multitask Learners, Code from openai/gpt-2",,July,2019,https://github.com/openai/gpt-2,,,,,
10,noauthor_language_nodate,,Language Models and Contextualised Word Embeddings,,,,http://www.davidsbatista.net/blog/2018/12/06/Word_Embeddings/,,,,,
10,noauthor_large_nodate,,A Large Corpus for Supervised Word-Sense Disambiguation,Google AI Blog,,,http://ai.googleblog.com/2017/01/a-large-corpus-for-supervised-word.html,"Posted by Colin Evans and Dayu Yuan, Software Engineers Understanding the various meanings of a particular word in text is key to understand...",,,,
10,noauthor_leveraging_2019,,Leveraging Language Models for Commonsense Reasoning in Neural Networks,Salesforce Research,June,2019,https://blog.einstein.ai/leveraging-language-models-for-commonsense/,"Commonsense reasoning that draws upon world knowledge derived from spatial and temporal relations, laws of physics, causes and effects, and social conventions is a feature of human intelligence.  However, it is difficult to instill such commonsense reasoning abilities into artificial intelligence implemented by deep neural networks. While neural networks effectively learn from a large number of examples, commonsense reasoning for humans precisely hits upon the kind of reasoning that is in less need of exemplification. Rather, humans pick up the kind of knowledge required to do commonsense reasoning simply by living in the world and doing everyday things.  AI models have limited access to the kind of world knowledge that is necessary for commonsense reasoning. Neural networks designed for tasks that use natural language often only see text -- no visual data, sounds, tactile sensations, or scents are known to these networks. Since these natural language processing networks are limited to text alone, as a poor substitute for living in the world, we have them read a human-mind-boggling amount of text, including all of Wikipedia and thousands of books.  We then probe the commonsense reasoning capacity of the neural network by using a multiple-choice test called Commonsense Question Answering (CQA) [Talmor et al., 2019]. The neural network trains on a few examples from CQA that require commonsense reasoning to answer. Then, we administer the real test with questions the network has never seen. Compared to humans, these well-read neural networks have known to perform quite poorly on this task [Talmor et al., 2019].  We instill commonsense reasoning in neural networks by way of explanations and show that it improves performance on the CQA as shown below.",,,,
10,noauthor_library_2019,,Library of deep learning models and datasets designed to make deep learning more accessible and accelerate ML research.: tensorflow/tensor2tensor,,August,2019,https://github.com/tensorflow/tensor2tensor,,,Transformer,,
10,noauthor_lingvis.io_nodate,,LingVis.io,,,,https://lingvis.io/,,,,,
10,noauthor_lstmvis:_nodate,,LSTMVis: A Tool for Visual Analysis of Hidden State Dynamics in Recurrent Neural Networks - IEEE Journals \& Magazine,,,,https://ieeexplore.ieee.org/document/8017583,,,,,
10,noauthor_ltma_nodate,,"LTMA: Layered Topic Matching for the Comparative Exploration, Evaluation, and Refinement of Topic Modeling Results - IEEE Conference Publication",,,,https://ieeexplore.ieee.org/abstract/document/8534018,,,,,
10,noauthor_machine_nodate,,Machine Learning Explainability \textbar Kaggle,,,,https://www.kaggle.com/learn/machine-learning-explainability,,,,,
10,noauthor_malnis662_nodate,,[MALNIS:662] FW: [DNLP-W-275] Some links from today's meeting - evangelos.milios@gmail.com - Gmail,,,,https://mail.google.com/mail/u/0/?tab=rm#inbox/FMfcgxwJXBzLBZRvdNmzrPjJfTWjCFtB,,,,,
10,noauthor_mds20_2020,,MDS20 Minitutorial: A Mathematical Perspective of Machine Learning by Weinan E,,May,2020,https://www.youtube.com/watch?v=4PKudVGz4GM,"Abstract: The heart of modern machine learning is the approximation of high dimensional functions. Traditional approaches, such as approximation by piecewise polynomials, wavelets, or other linear combinations of fixed basis functions, suffer from the curse of dimensionality. We will discuss representations and approximations that overcome this difficulty, as well as gradient flows that can be used to find the optimal approximation. We will see that at the continuous level, machine learning consists of a series of reasonably nice variational and PDE-like problems. Modern machine learning models/algorithms, such as the random feature and shallow/deep neural network models, are all special discretizations of these continuous problems. We will also discuss how to construct new models/algorithms using the same philosophy. At the theoretical level, we will present a framework that is suited for analyzing machine learning models and algorithms in high dimension, and present results that are free of the curse of dimensionality. Finally, we will discuss the fundamental reasons that are responsible for the success of modern machine learning, as well as the subtleties and mysteries that still remain to be understood.   Weinan E, Princeton University, U.S., weinan@math.princeton.edu   This is one of six minitutorial talks organized by Carola-Bibiane SchÃ¶nlieb (University of Cambridge, United Kingdom) and Ozan Ã–ktem (KTH Stockholm, Sweden) under the title Deep Learning for Inverse Problems and Partial Differential Equations"" as part of the 2020 SIAM Conference on Mathematics of Data Science. For more information"," visit https://meetings.siam.org/sess/dsp\_pr....""",,,
10,noauthor_mispelling_2019,,Mispelling oblivious word Embeddings: Dataset,,September,2019,https://github.com/facebookresearch/moe,Misspelling Oblivious Word Embeddings. Contribute to facebookresearch/moe development by creating an account on GitHub.,,,,
10,noauthor_mispelling_nodate,,Mispelling oblivious word Embeddings: A new model for word embeddings that are resilient to misspellings,,,,https://ai.facebook.com/blog/-a-new-model-for-word-embeddings-that-are-resilient-to-misspellings-/,"Misspelling Oblivious Embeddings (MOE) is a new model for word embeddings that are resilient to misspellings, improving the ability to apply word embeddings to real-world situations, where misspellings are common.",,,,
10,noauthor_mixed-integer_nodate,,Mixed-Integer Programming \textbar OR-Tools,Google Developers,,,https://developers.google.com/optimization/mip/integer_opt,,,,,
10,noauthor_mlflow_nodate,,MLflow,,,,https://mlflow.org/docs/latest/concepts.html,,,,,
10,noauthor_models_2019,,Models and examples built with TensorFlow. Contribute to tensorflow/models development by creating an account on GitHub,,May,2019,https://github.com/tensorflow/models,,,,,
10,noauthor_multi-label_2019,,Multi-label classification,Wikipedia,July,2019,https://en.wikipedia.org/w/index.php?title=Multi-label_classification&oldid=907682368,"In machine learning, multi-label classification and the strongly related problem of multi-output classification are variants of the classification problem where multiple labels may be assigned to each instance. Multi-label classification is a generalization of multiclass classification, which is the single-label problem of categorizing instances into precisely one of more than two classes; in the multi-label problem there is no constraint on how many of the classes the instance can be assigned to. Formally, multi-label classification is the problem of finding a model that maps inputs x to binary vectors y (assigning a value of 0 or 1 for each element (label) in y).",,,,
10,noauthor_multilabel_nodate,,Multilabel Classification,,,,https://mlr.mlr-org.com/articles/tutorial/multilabel.html,,,,,
10,noauthor_multi-label_nodate,,Multi-Label Classification: Papers With Code,,,,http://paperswithcode.com/task/multi-label-classification,See leaderboards and papers with code for Multi-Label Classification,,,,
10,noauthor_multilabel_nodate-1,,Multilabel Classification,,,,https://mlr.mlr-org.com/articles/tutorial/multilabel.html,,,,,
10,noauthor_neural_nodate,,Neural Vector Representations beyond Words: Sentence and Document Embeddings,,,,http://gerard.demelo.org/teaching/embedding-tutorial/,,,,,
10,noauthor_nyu-dl/dl4marco-bert_2019,,nyu-dl/dl4marco-bert,,December,2019,https://github.com/nyu-dl/dl4marco-bert,Contribute to nyu-dl/dl4marco-bert development by creating an account on GitHub.,,,,
10,noauthor_online_nodate,,An Online Semantic-enhanced Dirichlet Model for Short Text Stream Clustering - ACL Anthology,,,,https://www.aclweb.org/anthology/2020.acl-main.70/,,,,,
10,noauthor_open_nodate,,Open Sourcing BERT: State-of-the-Art Pre-training for Natural Language Processing,Google AI Blog,,,http://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html,"Posted by Jacob Devlin and Ming-Wei Chang, Research Scientists, Google AI Language   One of the biggest challenges in natural language proce...",,,,
10,noauthor_openai_nodate,,"OpenAI Presents GPT-3, a 175 Billion Parameters Language Model â€“ NVIDIA Developer News Center",,,,https://news.developer.nvidia.com/openai-presents-gpt-3-a-175-billion-parameters-language-model/,,,,,
10,noauthor_openai/gpt-2_2019,,openai/gpt-2,,November,2019,https://github.com/openai/gpt-2,"Code for the paper Language Models are Unsupervised Multitask Learners""""",,,,
10,noauthor_openaigpt-3_2020,,openai/gpt-3,,July,2020,https://github.com/openai/gpt-3,GPT-3: Language Models are Few-Shot Learners. Contribute to openai/gpt-3 development by creating an account on GitHub.,,,,
10,noauthor_openaigpt-3_nodate,,openai/gpt-3,GitHub,,,https://github.com/openai/gpt-3,GPT-3: Language Models are Few-Shot Learners. Contribute to openai/gpt-3 development by creating an account on GitHub.,,,,
10,noauthor_openaigpt-3_nodate-1,,openai/gpt-3,GitHub,,,https://github.com/openai/gpt-3,GPT-3: Language Models are Few-Shot Learners. Contribute to openai/gpt-3 development by creating an account on GitHub.,,,,
10,noauthor_papers_nodate,,Papers With Code : Document Embedding,,,,http://paperswithcode.com/task/document-embedding,See leaderboards and papers with code for Document Embedding,,,,
10,noauthor_parameter_nodate,,Parameter space analysis,,,,https://vda.cs.univie.ac.at/research/focus/parameter-space-analysis/,Parameter space analysis,,,,
10,noauthor_precision_2020,,Precision and recall,Wikipedia,February,2020,https://en.wikipedia.org/w/index.php?title=Precision_and_recall&oldid=940115319,"In pattern recognition, information retrieval and classification (machine learning), precision (also called positive predictive value) is the fraction of relevant instances among the retrieved instances, while recall (also known as sensitivity) is the fraction of the total amount of relevant instances that were actually retrieved. Both precision and recall are therefore based on an understanding and measure of relevance. Suppose a computer program for recognizing dogs in photographs identifies 8 dogs in a picture containing 12 dogs and some cats. Of the 8 identified as dogs, 5 actually are dogs (true positives), while the rest are cats (false positives). The program's precision is 5/8 while its recall is 5/12.  When a search engine returns 30 pages only 20 of which were relevant while failing to return 40 additional relevant pages, its precision is 20/30 = 2/3 while its recall is 20/60 = 1/3.  So, in this case, precision is how useful the search results are"""," and recall is ""how complete the results are"". In statistics", if the null hypothesis is that all items  are irrelevant (where the hypothesis is accepted or rejected based on the number selected compared with the sample size), absence of type I and type II errors (i.e. perfect sensitivity and specificity of 100\% each) corresponds respectively to perfect precision (no false positive) and perfect recall (no false negative).  The above pattern recognition example contained 8 âˆ’ 5 = 3 type I errors and 12 âˆ’ 5 = 7 type II errors.  Precision can be seen as a measure of exactness or quality, whereas recall is a measure of completeness or quantity. The exact relationship between sensitivity and specificity to precision depends on the percent of positive cases in the population. In simple terms
10,noauthor_probabilistic_nodate,,Probabilistic Topic Modeling for Comparative Analysis of Document Collections \textbar ACM Transactions on Knowledge Discovery from Data,,,,https://dl.acm.org/doi/abs/10.1145/3369873,,,,,
10,noauthor_project_nodate,,Project Implicit,,,,https://implicit.harvard.edu/implicit/,,,,,
10,noauthor_pyldavis_nodate,,Pyldavis :: Anaconda Cloud,,,,https://anaconda.org/conda-forge/pyldavis,,,,,
10,noauthor_pytorch-transformers/readme.md_nodate,,pytorch-transformers/README.md at master Â· huggingface/pytorch-transformers Â· GitHub,,,,https://webcache.googleusercontent.com/search?q=cache:0z6lgj2TtzkJ:https://github.com/huggingface/pytorch-transformers/blob/master/examples/lm_finetuning/README.md+&cd=1&hl=en&ct=clnk&gl=ca&client=firefox-b-d,,,,,
10,noauthor_q8bert_nodate,,"Q8BERT, a Quantized 8bit Version of BERT-Base",Intel,,,https://www.intel.com/content/www/us/en/artificial-intelligence/posts/q8bert.html,"Q8BERT, a Quantized 8bit Version of BERT-Base",,,,
10,noauthor_r/mlquestions_nodate,,r/MLQuestions - BERT: How can I generate word embeddings from BERT similar to word2vec or GloVe?,reddit,,,https://www.reddit.com/r/MLQuestions/comments/atfr4u/bert_how_can_i_generate_word_embeddings_from_bert/,16 votes and 7 comments so far on Reddit,,,,
10,noauthor_recipe_nodate,,A Recipe for Training Neural Networks,,,,http://karpathy.github.io/2019/04/25/recipe/,,,,,
10,noauthor_regression_nodate,,regression - What is an ablation study? And is there a systematic way to perform it?,Cross Validated,,,https://stats.stackexchange.com/questions/380040/what-is-an-ablation-study-and-is-there-a-systematic-way-to-perform-it,,,,,
10,noauthor_saif_nodate,,Saif \textbar Bias EEC,,,,https://saifmohammad.com/WebPages/Biases-SA.html,,,,,
10,noauthor_sentence-bert:_2019,,Sentence-BERT: UKPLab/sentence-transformers,,November,2019,https://github.com/UKPLab/sentence-transformers,Sentence Embeddings with BERT \& XLNet. Contribute to UKPLab/sentence-transformers development by creating an account on GitHub.,,,,
10,noauthor_sentencepiece:_2019,,SentencePiece: Unsupervised text tokenizer for Neural Network-based text generation.: google/sentencepiece,,July,2019,https://github.com/google/sentencepiece,,,,,
10,noauthor_seq2seq:_nodate,,seq2seq: code,,,,https://google.github.io/seq2seq/,,,,,
10,noauthor_siam_2020,,SIAM Mathematics of Data Science (MDS20) Distinguished Lecture Series: Yann LeCun,,July,2020,https://www.youtube.com/watch?v=9sqoe_krQ1E,"The Deep Learning - Applied Math Connection    Abstract: Deep learning (DL) is causing revolutions in computer perception, signal restoration/reconstruction, signal synthesis, natural language understanding, and process control. DL is increasingly used to provide approximate solutions to PDE and non-linear optimization problems, with many applications in cosmology, material science, high-energy physics, and various applications of fluid dynamics. But one of the most direct impacts of DL on the scientific computing community has been to provide flexible software platforms for numerical problems, such as PyTorch and TensorFlow, with built-in support for multi-dimensional arrays, GPU, parallelism, and automatic differentiation.   While DL has become a new tool in the toolbox of the applied mathematician, DL heavily relies on the tools of applied mathematics, such as large-scale non-linear, non-convex optimization. But our understanding of the landscape of the objective function and the convergence properties in DL systems is still very superficial.   One of the key questions in Machine Learning today is how to learn predictive models of the world in a self-supervised manner, a bit like humans and animals. A class of methods that can predict high-dimensional signals, such as video, under uncertainty will be presented. It is based on capturing dependencies between variable by shaping an energy function.   Yann LeCun, Facebook, U.S.   This is one of seven virtual plenary talks originally scheduled for the 2020 SIAM Conference on Mathematics of Data Science. For more information on this session, visit https://meetings.siam.org/sess/dsp\_pr.... To view the virtual program and register for other invited plenary talks, minitutorial talks, and minisymposia, please visit the MDS20 website at https://www.siam.org/conferences/cm/c....",,,,
10,noauthor_squad:_nodate,,SQuAD: The Stanford Question Answering Dataset,,,,https://rajpurkar.github.io/SQuAD-explorer/,,,,,
10,noauthor_stsbenchmark_nodate,,STSbenchmark - stswiki,,,,http://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark,,,,,
10,noauthor_subword-level_nodate,,Subword-level Composition Functions for Learning Word Embeddings \textbar vecto,,,,http://vecto.space/projects/subword/,,,,,
10,noauthor_sustainlp2020_nodate,,SustaiNLP2020,,,,https://sites.google.com/view/sustainlp2020,,,,,
10,noauthor_teaching_nodate,,Teaching Machines to Read Radiology Reports,,,,http://blog.qure.ai/notes/teaching-machines-read-radiology-reports,,,,,
10,noauthor_tensorflow_2019,,TensorFlow Neural Machine Translation Tutorial. Contribute to tensorflow/nmt development by creating an account on GitHub,,August,2019,https://github.com/tensorflow/nmt,,,seq2seq,,
10,noauthor_third_nodate,,The Third Workshop on Evaluating Vector Space Representations for NLP,RepEval2019,,,http://repeval2019.github.io/program/,The Third Workshop on Evaluating Vector Space Representations for NLP,,,,
10,noauthor_transfer_nodate,,Transfer Learning using ELMO Embedding â€“ Towards Data Science,,,,https://towardsdatascience.com/transfer-learning-using-elmo-embedding-c4a7e415103c,,,,,
10,noauthor_trec_nodate,,TREC CAR: A dataset for Complex Answer Retrieval,,,,http://trec-car.cs.unh.edu/,,,,,
10,noauthor_tutorials_nodate,,Tutorials and Presentations on using Common Crawl Data â€“ Common Crawl,,,,https://commoncrawl.org/the-data/tutorials/,,,,,
10,noauthor_ukplabsentence-transformers_nodate,,UKPLab/sentence-transformers,GitHub,,,https://github.com/UKPLab/sentence-transformers,Sentence Embeddings with BERT \& XLNet. Contribute to UKPLab/sentence-transformers development by creating an account on GitHub.,,,,
10,noauthor_understanding_nodate,,Understanding the Math behind the XGBoost Algorithm,,,,https://www.analyticsvidhya.com/blog/2018/09/an-end-to-end-guide-to-understand-the-math-behind-xgboost/,,,,,
10,noauthor_using_nodate,,Using FastText models for robust embeddings,,,,https://kaggle.com/mschumacher/using-fasttext-models-for-robust-embeddings,Using data from Toxic Comment Classification Challenge,,,,
10,noauthor_very_2019,,A very simple framework for state-of-the-art Natural Language Processing (NLP): zalandoresearch/flair,,July,2019,https://github.com/zalandoresearch/flair,,,,,
10,noauthor_visirr_nodate,,"VisIRR: A Visual Analytics System for Information Retrieval and Recommendation for Large-Scale Document Data: ACM Transactions on Knowledge Discovery from Data: Vol 12, No 1",,,,https://dl.acm.org/doi/abs/10.1145/3070616,,,,,
10,noauthor_want_nodate,,Want to use our data? â€“ Common Crawl,,,,https://commoncrawl.org/the-data/,,,,,
10,noauthor_ways_nodate,,Ways to Solve a MultiLabel Classification Problem,,,,https://kaggle.com/jyothish/ways-to-solve-a-multilabel-classification-problem,Using data from Toxic Comment Classification Challenge,,,,
10,noauthor_webvectors:_nodate,,WebVectors: distributional semantic models online,,,,http://vectors.nlpl.eu/explore/embeddings/en/#,,,,,
10,noauthor_welcome_nodate,,Welcome \textbar Flask (A Python Microframework),,,,http://flask.pocoo.org/,,,,,
10,noauthor_welcome_nodate,,Welcome to bert-embeddingâ€™s documentation! â€” bert-embedding documentation,,,,https://bert-embedding.readthedocs.io/en/latest/,,,,,
10,noauthor_word_nodate,,Word embedding demo,,,,http://bionlp-www.utu.fi/wv_demo/,,,,,
10,noauthor_word_nodate,,Word Sense Disambiguation,NLP-progress,,,http://nlpprogress.com/english/word_sense_disambiguation.html,"Repository to track the progress in Natural Language Processing (NLP), including the datasets and the current state-of-the-art for the most common NLP tasks.",,,,
10,noauthor_write_nodate,,Write With Transformer,,,,https://transformer.huggingface.co,See how a modern neural network auto-completes your text,,,,
10,noauthor_xgboost_2019,,XGBoost,Wikipedia,August,2019,https://en.wikipedia.org/wiki/XGBoost,"XGBoost is an open-source software library which provides a gradient boosting framework for C++, Java, Python,R, and Julia. It works on Linux, Windows, and macOS. From the project description, it aims to provide a Scalable", Portable and Distributed Gradient Boosting (GBM, GBRT," GBDT) Library"". It runs on a single machine", as well as the distributed processing frameworks Apache Hadoop
10,nooney_deep_2019,"Nooney, Kartik",Deep dive into multi-label classification..! (With detailed Case Study),Medium,February,2019,https://towardsdatascience.com/journey-to-the-center-of-multi-label-classification-384c40229bff,Toxic-comments classification.,,,,
10,palachy_document_2019,"Palachy, Shay",Document Embedding Techniques,Medium,October,2019,https://towardsdatascience.com/document-embedding-techniques-fed3e7a6a25d,A review of notable literature on the topic,,,,
10,ph.d_bert_2019,"Ph.D, Michel Kana",BERT for dummies â€” Step by Step Tutorial,Medium,October,2019,https://towardsdatascience.com/bert-for-dummies-step-by-step-tutorial-fb90890ffe03,DIY Practical guide on Transformer. Hands-on proven PyTorch code for Intent Classification with BERT fine-tuned.,,,,
10,picture_embedding_nodate,"Picture, Nikhil Thorat, Charles Nicholson Big Daniel Smilkov",Embedding projector - visualization of high-dimensional data,,,,http://projector.tensorflow.org,Visualize high dimensional data.,,,,
10,plotz_paper_2018,"Plotz, Mike",Paper Summary: Improving Language Understanding by Generative Pre-Training,Medium,November,2018,https://medium.com/@hyponymous/paper-summary-improving-language-understanding-by-generative-pre-training-7b77babd7086,Part of the series A Month of Machine Learning Paper Summaries.,,,,
10,poyiatzis_nlp:_2019,"Poyiatzis, Andreas",NLP: Contextualized word embeddings from BERT,Medium,March,2019,https://towardsdatascience.com/nlp-extract-contextualized-word-embeddings-from-bert-keras-tf-67ef29f60a7b,Extract contextualized word embeddings from BERT using Keras and TF,,,,
10,pubmeddev_home_nodate,pubmeddev,Home - PubMed - NCBI,,,,https://www.ncbi.nlm.nih.gov/pubmed/,"PubMed comprises more than 30 million citations for biomedical literature from MEDLINE, life science journals, and online books. Citations may include links to full-text content from PubMed Central and publisher web sites.",,,,
10,raghudeep_review:_2019,Raghudeep,Review: BioBERT paper,Medium,February,2019,https://medium.com/@raghudeep/biobert-insights-b4c66fde8fa7,The objective of this article is to understand the application of BERT pre-trained model for biomedical field and then try to figure outâ€_,,,,
10,rong_xin_wevi:_nodate,Rong Xin,wevi: word embedding visual inspector,,,,https://ronxin.github.io/wevi/,,,,,
10,sener_active_2019,"Sener, Ozan","Active Learning for Convolutional Neural Networks: A Core-Set Approach: ozansener/active\_learning\_coreset, Source code for ICLR 2018 Paper:",,August,2019,https://github.com/ozansener/active_learning_coreset,,,,,
10,sieg_pre-trained_2019,"Sieg, Adrien",FROM Pre-trained Word Embeddings TO Pre-trained Language Models â€” Focus on BERT,Medium,November,2019,https://medium.com/@adriensieg/from-pre-trained-word-embeddings-to-pre-trained-language-models-focus-on-bert-343815627598,FROM Static Word Embedding TO Dynamic (Contextualized) Word Embedding,,,,
10,silva_flexible_2019,"Silva, Victor",A Flexible Framework to Monitor Evolution of Clusters,ERA,,2019,https://era.library.ualberta.ca/items/96c04198-b0c6-4c95-a878-c08b617079a5,"Nowadays, the volume of collected data and the size of datasets raise various challenges in the field of data mining. One of such...",,,,
10,spencer_punctuator_nodate,"Spencer, Chris",punctuator: Adds punctuation to a block of text.,,,,https://github.com/chrisspen/punctuator2,,,,,
10,taylor_elmo:_2019,"Taylor, Josh",ELMo: Contextual language embedding,Towards Data Science,January,2019,https://towardsdatascience.com/elmo-contextual-language-embedding-335de2268604,Using deep contextualised language representations from ELMo to create a semantic search engine and why context is everything in NLP,,,,
10,team_subword-level_2018,"Team, The Vecto",Subword-level Composition Functions for Learning Word Embeddings \textbar vecto,vecto,June,2018,http://vecto.space/,Explore dozens of published embeddings!,,,,
10,toews_gpt-3_nodate,"Toews, Rob",GPT-3 Is Amazingâ€”And Overhyped,Forbes,,,https://www.forbes.com/sites/robtoews/2020/07/19/gpt-3-is-amazingand-overhyped/,It is important for the technology community to have a more clear-eyed understanding of what GPT-3 can and cannot do.,,,,
10,tom_kenter_neural_nodate,Tom Kenter; Alexey Borisov; Christophe Van Gysel; Mostafa Delghani; Maarten de Rijke; Bhaskar Mitra,Neural Networks for Information Retrieval (Tutorial NN4IR@SIGIR2017),,,,http://nn4ir.com/,"Morning 1. Preliminaries 2. Semantic matching 1 3. Semantic matching 2 Afternoon 4. Learning to rank 5. Modeling user behavior 6. Generating responses 7. Outlook 8. Wrap up Slides available at http://nn4ir.com  Bibliography available at http://nn4ir.com  Lecture notes: B. Mitra and N. Craswell. An introduction to neural information retrieval. Foundations and Trends in Information Retrieval, 2017, under review. http://bit.ly/neuralir-intro  Survey: K.D. Onal et al. Neural information retrieval: At the end of the early years. Information Retrieval Journal, 2017, under review.",,,,
10,trec-web_trec-web-2013:_2017,trec-web,TREC-web-2013: detailed explanation of the tasks and process using ClueWeb09,,May,2017,https://github.com/trec-web/trec-web-2013,,,,,
10,vig_bertviz_2020,"Vig, Jesse",Bertviz: Tool for visualizing attention in the Transformer model,,February,2020,https://github.com/jessevig/bertviz,"Tool for visualizing attention in the Transformer model (BERT, GPT-2, Albert, XLNet, RoBERTa, CTRL, etc.)",,"bert, machine-learning, natural-language-processing, neural-network, nlp, pytorch, visualization",,
10,vig_jessevig/bertviz_2019,"Vig, Jesse",jessevig/bertviz,,December,2019,https://github.com/jessevig/bertviz,"Tool for visualizing attention in the Transformer model (BERT, GPT-2, Albert, XLNet, RoBERTa, CTRL, etc.)",,"bert, machine-learning, natural-language-processing, neural-network, nlp, pytorch, visualization",,
10,vig_openai_2019,"Vig, Jesse",OpenAI GPT-2: Understanding Language Generation through Visualization,Medium,November,2019,https://towardsdatascience.com/openai-gpt-2-understanding-language-generation-through-visualization-8252f683b2f8,How the super-sized language model is able to finish your thoughts.,,,,
10,weinan_zhang_generative_nodate,Weinan Zhang,Generative Adversarial Nets for Information Retrieval,,,,http://wnzhang.net/tutorials/sigir2018/docs/sigir18-irgan-full-tutorial.pdf,,,,,
10,wright_meet_2019,"Wright, Less",Meet ALBERT: a new â€˜Lite BERTâ€™ from Google \& Toyota with State of the Art NLP performance and 18x fewer parameters.,Medium,October,2019,https://medium.com/lessw/meet-albert-a-new-lite-bert-from-google-toyota-with-state-of-the-art-nlp-performance-and-18x-df8f7b58fa28,"Your previous NLP models are parameter inefficient and kind of obsolete.  Google Research and Toyota Technological Institute jointly released a new paper that introduces the world to what is arguably BERTâ€™s successor, a much smaller/smarter Lite Bert called ALBERT. (â€œALBERT: A Lite BERT for Self-supervised Learning of Language Representationsâ€ù).",,,,
10,xiaodan_zhu_deep_2017,Xiaodan Zhu; Edward Grefenstette,Deep Learning for Semantic Composition,,,2017,,,,,,
10,yao_pkuicst_2016,"Yao, Lili; Lv, Chao; Fan, Feifan; Yang, Jianwu; Zhao, Dongyan",PKUICST at TREC 2017 Real-Time Summarization Track: Push Notifications and Email Digest,undefined,,2016,/paper/PKUICST-at-TREC-2017-Real-Time-Summarization-Track%3A-Yao-Lv/75feb3db609a8a7d2e561627e4868f302a07896c,"In this paper, we describe our approaches and corresponding results in the Real-Time Summarization(RTS) track at the 2017 Text Retrieval Conference(TREC). The main idea is to build a two-stage filter system for both scenario A and B. In the first stage, tweets are filtered according to its relevance score to a particular topic, while in the second stage, they are filtered according to its novelty score to previous submitted tweets. We tried several approaches to model the text similarity, such as negative KL-divergence and cosine distance, as well as blending models. Especially, in scenario A, the push notification scenario, we designed a decoupled system that can maintain high availability in order to meet the real-time requirements. The experiment results show that our methods reach good performance with respect to several metrics such as EG-p and nDCG-p. Introduction Microblog, such as Twitter and Weibo, has become one of the most important accesses for people to get information. However, finding out helpful information from massive microblogs by hand can be very difficult and exhausting. Building an automatic system that helps to pick out specific microblog is a good solution. The TREC 2017 Real-Time Summarization (RTS) Track aims to explore techniques that helps build such systems. There are two scenarios contained in the RTS Track: â€¢ Scenario A (push notification): Content that is identified as relevant and novel by a system based on the userâ€™s interest profile should be sent to the user in a timely fashion. â€¢ Scenario B (email digest): Participating systems should identify tweets and aggregate them into an email digest. The email should be periodically sent to a user. Under that circumstances, users can read a longer story about the contents. For both scenario A and B, we perform a two-stage filter system separately. In push notification scenario, our system contains three function modules, Filter Module, Judge Module and Submit Module, and two tables, i.e. Pre-process Table and Submit Table, that transfer data between modules. The Filter *Corresponding author. Module listens to the tweet sample stream, roughly filter out tweets that obviously irrelevant to the interest profiles, and insert the remain tweets into the Pre-process Table. The Judge Module continuously detects new tweets from the Preprocess Table, and compute the relevance score between those tweets and the interest profiles. A tuned relevance threshold Î± is utilized to judge whether a specific tweet and an interest profile are relevant. Then, for every relevant profile, we compute the novelty score between current tweet and previous submitted tweets. Similarly, a novelty threshold Î_ is used to determine whether a tweet is novel. Those tweets passed the two threshold will be inserted to the Submit Table. Finally, the Submit Module submit tweets in the Submit Table to the Evaluation Broker. The independence of the three module guarantees that the system can recover quickly and safely from system crash. In email digest scenario, we directly precess the tweets from Pre-process Table. The whole procedure is basically the same as Judge Model in scenario A. Two threshold are performed to filter out those â€™relevant but novelâ€™ tweets. the only difference is that after the filtering, we sort the tweets by the relevance score for every interest profile, and select the top 100 tweets per interest profile per day.",,,,
10,zhang_hilrecognizer_nodate,"Zhang, Shanshan",HILRecognizer,,,,https://github.com/nymph332088/HILRecognizer,,,,,
10,zhi-hui_luozhhubpymeshsim_2020,"Zhi-Hui, Luo",luozhhub/pyMeSHSim,,July,2020,https://github.com/luozhhub/pyMeSHSim,a module to calculate the MeSH similarity. Contribute to luozhhub/pyMeSHSim development by creating an account on GitHub.,,,,
10,ruotsalo_low-dimensional_2017,"Ruotsalo, Tuukka Juhani; Peltonen, Jaakko Tapani; Eugster, Manuel J. A.; MyllymÃ_ki, Petri Jukka; Jacucci, Giulio; Kaski, Samuel; Glowacka, Dorota; Ruotsalo, Tuukka Juhani; Peltonen, Jaakko Tapani; Eugster, Manuel J. A.; MyllymÃ_ki, Petri Jukka; Jacucci, Giulio; Kaski, Samuel; Glowacka, Dorota","Low-dimensional information discovery and presentation system, apparatus and method",,October,2017,https://patents.google.com/patent/US9798780B2/en,,,"features, intent, representation, search, user",,
9,emanuela_boros_neural_2018,Emanuela BoroÅŸ,Neural Methods for Event Extraction,,,2018,https://tel.archives-ouvertes.fr/tel-01943841/document,,,,,
9,sajadi_semantic_2018,"Sajadi, Armin",Semantic Analysis using Wikipedia Graph Structure,,April,2018,https://DalSpace.library.dal.ca//handle/10222/73806,"Wikipedia is becoming an important knowledge source in various domain specific applications based on concept representation. While lexical resources like WordNet cover generic English well, they are weak in their coverage of domain-specific terms and named entities, which is one of the strengths of Wikipedia. Furthermore, semantic relatedness methods that rely on the hierarchical structure of a lexical resource are not directly applicable to the Wikipedia link structure, which is not hierarchical and whose links do not capture well defined semantic relationships like hyponymy. We introduce a vector space representation of concepts using Wikipedia graph structure to calculate semantic relatedness. The proposed method starts from the neighbourhood graph of a concept as the primary form and transfers this graph into a vector space to obtain the final representation. The proposed method achieves state-of-the-art results on various relatedness datasets. We evaluate Wikipedia in a domain-specific semantic relatedness task and are able to demonstrate that Wikipedia-based methods can be competitive with state of the art ontology-based methods and distributional methods in the biomedical domain. The comparison includes a wide range of structure and corpus-based methods, such as our proposed word2vec-based embeddings: a hybrid distributional/knowledge-based word2vec and node-embedding, a word2vec application on graph structure. Our representations have also been reported to achieve the highest results in a query expansion task. We also use a standard coherence model to show that the proposed relatedness method performs successfully in Word Sense Disambiguation (WSD). We then suggest a different formulation for coherence to demonstrate that, in a short enough sentence, there is one key entity that can help disambiguate every other entity. Using this finding, we provide a vector space based method that can outperform the standard coherence model in a significantly shorter computation time. We use our findings in WSD to create a complete wikifier, a supervised approach based on learning to rank that combines our new coherence measure with other sources of information, such as textual context. The final product is an open source project that is available through direct API or web service.",,,,
9,wang_event_2017,"Wang, Wei",Event Detection and Encoding from News Articles,,September,2017,https://vtechworks.lib.vt.edu/bitstream/handle/10919/82238/Wang_W_D_2018.pdf?sequence=1&isAllowed=y,"Event extraction is a type of information extraction(IE) that works on extracting the speci c knowledge of certain incidents from texts. Nowadays the amount of available information (such as news, blogs, and social media) grows in exponential order. Therefore, it becomes imperative to develop algorithms that automatically extract the machine-readable information from large volumes of text data. In this dissertation, we focus on three problems in obtaining event-related information from news articles. (1) The  rst e ort is to comprehensively analyze the performance and challenges in current large-scale event encoding systems. (2) The second problem involves event detection and critical information extractions from news articles. (3) Third, the e orts concentrate on event-encoding which aims to extract event extent and arguments from texts. We start by investigating the two large-scale event extraction systems (ICEWS and GDELT) in the political science domain. We design a set of experiments to evaluate the quality of the extracted events from the two target systems, in terms of reliability and correctness. The results show that there exist signi cant discrepancies between the outputs of automated systems and hand-coded system and the accuracy of both systems are far away from satisfying. These  ndings provide preliminary background and set the foundation for using advanced machine learning algorithms for event related information extraction. Inspired by the successful application of deep learning in Natural Language Processing (NLP), we propose a Multi-Instance Convolutional Neural Network (MI-CNN) model for event detection and critical sentences extraction without sentence level labels. To evaluate the model, we run a set of experiments on a real-world protest event dataset. The result shows that our model could be able to outperform the strong baseline models and extract the meaningful key sentences without domain knowledge and manually designed features. We also extend the MI-CNN model and propose an MIMTRNN model for event extraction with distant supervision to overcome the problem of lacking  ne level labels and small size training data. The proposed MIMTRNN model systematically integrates the RNN, MultiInstance Learning, and Multi-Task Learning into a uni ed framework. The RNN module aims to encode into the representation of entity mentions the sequential information as well as the dependencies between event arguments, which are very useful in the event extraction task. The Multi-Instance Learning paradigm makes the system does not require the precise labels in entity mention level and make it perfect to work together with distant supervision for event extraction. And the Multi-Task Learning module in our approach is designed to alleviate the potential over tting problem caused by the relatively small size of training data. The results of the experiments on two real-world datasets(Cyber-Attack and Civil Unrest) show that our model could be able to bene t from the advantage of each component and outperform other baseline methods signi cantly.",,,,
13,benito_cross-domain_2016,"Benito, Alejandro; Theron, Roberto",Cross-domain Visual Exploration of Academic Corpora via the Latent Meaning of User-authored Keywords,,,2016,https://osf.io/h29qv,"Nowadays, scholars dedicate a substantial amount of their work to the querying and browsing of increasingly large collections of research papers on the Internet. In parallel, the recent surge of novel interdisciplinary approaches in science requires scholars to acquire competencies in new ï¬Åelds for which they may lack the necessary vocabulary to formulate adequate queries. This problem, together with the issue of information overload, poses new challenges in the ï¬Åelds of natural language processing (NLP) and visualization design that call for a rapid response from the scientiï¬Åc community. In this respect, we report on a novel visualization scheme that enables the exploration of research paper collections via the analysis of semantic proximity relationships found in author-assigned keywords. Our proposal replaces traditional string queries with a bag-of-words (BoW) extracted from a user-generated auxiliary corpus that captures the intentionality of the research. Continuing along the lines established by other authors in the ï¬Åelds of literature-based discovery (LBD), NLP and visual analytics (VA), we combine novel advances in the ï¬Åelds of NLP with visual network analysis techniques to offer scholars a perspective of the target corpus that better ï¬Åts their research interests. To highlight the advantages of our proposal, we conduct two experiments employing a collection of visualization research papers and an auxiliary cross-domain BoW. Here, we showcase how our visualization can be used to maximize the effectiveness of a browsing session by enhancing the language acquisition task, which allows for effectively extracting knowledge that is in line with the usersâ€™ previous expectations.",,,,
13,nigam_applying_2016,"Nigam, Priyanka",Applying Deep Learning to ICD-9 Multi-label Classification from Medical Records,,,2016,http://cs224d.stanford.edu/reports/priyanka.pdf,"Medical records contain detailed notes written by medical care providers about a patientâ€™s physical and mental health, analysis of lab tests and radiology results, treatment courses, and more. This information may be valuable in improving medical care. In this project, we apply deep learning models to the multi-label classification task of assigning ICD-9 labels from these medical notes. Previous works have applied machine learning methods, like logistic regression and hierarchical SVM, using bag-of-words features to this task. On a dataset of around 40,000 critical care unit patients with 10 labels and with 100 labels, we find that a Recurrent Neural Network (RNN) and a RNN with Long Short-term Memory (LSTM) units show an improvement over the Binary Relevance Logistic Regression model.",,,,
13,radford_improving_nodate,"Radford, Alec; Narasimhan, Karthik; Salimans, Tim; Sutskever, Ilya",Improving Language Understanding by Generative Pre-Training,,,,https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf,"Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classiï¬Åcation. Although large unlabeled text corpora are abundant, labeled data for learning these speciï¬Åc tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative ï¬Åne-tuning on each speciï¬Åc task. In contrast to previous approaches, we make use of task-aware input transformations during ï¬Åne-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures speciï¬Åcally crafted for each task, signiï¬Åcantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on question answering (RACE), and 1.5\% on textual entailment (MultiNLI).",,,,
13,towfighi_labelling_2019,"Towfighi, Sohrab; Agarwal, Arnav; Mak, Denise Y. F.; Verma, Amol",Labelling chest x-ray reports using an open-source NLP and ML tool for text data binary classification,,November,2019,http://medrxiv.org/lookup/doi/10.1101/19012518,"The chest x-ray is a commonly requested diagnostic test on internal medicine wards which can diagnose many acute pathologies needing intervention. We developed a natural language processing (NLP) and machine learning (ML) model to identify the presence of opacities or endotracheal intubation on chest x-rays using only the radiology report. This a preliminary report of our work and findings. Using the General Medicine Inpatient Initiative (GEMINI) dataset, housing inpatient clinical and administrative data from 7 major hospitals, we retrieved 1000 plain film radiology reports which were classified according to 4 labels by an internal medicine resident. NLP/ML models were developed to identify the following on the radiograph reports: the report is that of a chest x-ray, there is definite absence of an opacity, there is definite presence of an opacity, the report is a follow-up report with minimal details in its text, and there is an endotracheal tube in place. Our NLP/ML model development methodology included a random search of either TF-IDF or bag-of-words for vectorization along with random search of various ML models. Our Python programming scripts were made publicly available on GitHub to allow other parties to train models using their own text data. 100 randomly generated ML pipelines were compared using 10-fold cross validation on 75\% of the data, while 25\% of the data was left out for generalizability testing. With respect to the question of whether a chest xray definitely lacks an opacity, the modelâ€™s performance metrics were accuracy of 0.84, precision of 0.94, recall of 0.81, and receiver operating characteristic area under curve of 0.86. Model performance was worse when trained against a highly imbalanced dataset despite the use of an advanced oversampling technique.",,,,
